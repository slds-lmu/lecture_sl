\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-ensembles.tex}
\input{../../latex-math/ml-trees.tex}

\newcommand{\titlefigure}{figure/compboost-illustration-2.png}
\newcommand{\learninggoals}{
  \item Details of nonlinear BLs and splines
  \item Understand the built-in feature selection process
  \item Fair base learner selection
  \item Feature importance and PDPs
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Gradient Boosting: Advanced CWB Topics}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Nonlinear base learners}

As an alternative we can use nonlinear base learners, such as $P$- or
$B$-splines, which make the model equivalent to a
\textbf{generalized additive model (GAM)} (as long as the base learners keep
their additive structure, which is the case for splines).
\vspace{0.5cm}

\vfill

\begin{center}
\includegraphics[width=0.9\textwidth]{figure/bspline-basis.png}
\end{center}

% \vfill
%
% \begin{center}
% \includegraphics[width=1\textwidth]{figure_man/NBL02.png}
% \end{center}

\end{vbframe}

% ------------------------------------------------------------------------------

% tex file and figures are created automatically by: rsrc/fig-cwb-anim-nl.R
\input{tex/fig-cwb-anim-nl}

%\begin{vbframe}{boston housing: continued}
%
%\begin{itemize}
%  \small
%  \item This time we offer our model both
%  linear and nonlinear ($P$-spline) base learners (again, with intercept) for
%  the 13 features.
%  \item By iteration 100, the model has selected 8 out of the resulting 26 base
%  learners and consistently picked the nonlinear option.
%  We can trace how these spline predictors evolve over the iterations:
%\end{itemize}
%
%\vfill
%
%\begin{center}
%% \includegraphics[width=0.6\textwidth]{figure_man/spam-example.png}
%\includegraphics[width = \textwidth]{figure/compboost-illustration-3.png}
%\end{center}

% Unfortunately it's rather ugly to write down the decomposition

% <<spam-formula, eval = FALSE, echo = TRUE>>=
% formula <-
%   spam ~ bols(word_freq_make) + bbs(word_freq_make, df = 2, center = TRUE) +
%     bols(word_freq_address) + bbs(word_freq_address, df = 2, center = TRUE) +
%     bols(word_freq_all) + bbs(word_freq_all, df = 2, center = TRUE) +
% #  ...
% mboost(formula, data = spam, family = Binomial(), control = boost_control(mstop = 100))
% @
%
%\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Nonlinear effect decomposition}

Even when allowing for more complexity we typically want to keep solutions as
simple as possible.

\lz

\citebutton{Kneib~et~al., 2009}{https://epub.ub.uni-muenchen.de/2063/1/tr003.pdf} %Kneib~et~al. (2009) 
proposed a decomposition of each base learner into a
constant, a linear and a nonlinear part.
The boosting algorithm will automatically decide which feature to include --
linear, nonlinear, or none at all:

\vspace{-0.7cm}

\begin{align*}
b_j(x_j, \thetam) & = b_{j, \text{const}}(x_j, \thetam) + b_{j, \text{lin}}
(x_j, \thetam) + b_{j, \text{nonlin}}(x_j, \thetam)\\
 & = \theta^{[m]}_\text{j,const} + x_j \cdot \theta^{[m]}_{j, \text{lin}} +
 s_j(x_j, \thetam_{j,\text{nonlin}}),
\end{align*}

\small
where
\begin{itemize}
  \small
  \item $\theta_\text{j,const}$ is the intercept of feature $j$,
  \item $x_j \cdot \theta^{[m]}_{j, \text{lin}}$ is a feature-specific linear
  base learner, and
  \item $s_j(x_j, \thetam_{j,\text{nonlin}})$ is a (centered) nonlinear base
  learner capturing deviation from the linear effect (see next slide).
\end{itemize}

\framebreak

\begin{itemize}
    \item 
        Suppose a base learner $b_{j,\text{nonlin}}\in\mathcal{B}_{j,\text{nonlin}}$ (a spline base learner) with design matrix $\bm{Z}_{j,\text{nonlin}}\in\R^{n\times d_{j,\text{nonlin}}}$ and a linear base learner $b_{j,\text{lin}}\in\mathcal{B}_{j,\text{lin}}$ with design matrix $\bm{Z}_{j,\text{lin}}\in\R^{n\times d_{j,\text{nonlin}}}$.

    \item
        Often, $\mathcal{B}_{j,\text{lin}} \subseteq \mathcal{B}_{j,\text{nonlin}}$ and hence $b_{j,\text{lin}}$ is overlayed by $b_{j,\text{nonlin}}$ and not selected during the fitting.

     \item
        Subtracting $b_{j,\text{lin}}$ from $b_{j,\text{nnnlin}}$ to obtain a centered nonlinear base learner $b_{j,\text{nnonlin}^c}$ means to transform $\bm{Z}_{j,nonlin}$ so that the $span(\bm{Z}_{j,lin})$ is not included in $span(\bm{Z}_{j,nonlin})$. 

    \item 
        Practically, this means that $b_{j,\text{nonlin}^c}\notin \mathcal{B}_{j,\text{lin}}$ and hence cannot model any linear effect.

    \item
        The centering is done by computing the QR decomposition of $\bm{A} = \bm{Z}_{j,nonlin}^T \bm{Z}_{j,lin}$.

    \item 
        The first $k$ columns of the $\bm{Q}$ matrix from the QR decomposition form an orthonormal basis for the span of the first $k$ columns of $\bm{A}$.

    \item 
        Hence $\bm{Q}^\ast$, which is $\bm{Q}$ without the first $d_{j,\text{lin}}$ columns, rotates $\bm{Z}_{j,nonlin}$ to $\bm{Z}_{j,nonlin^c} = \bm{Q}^\ast\bm{Z}_{j,nonlin}\in\R^{n\times d_{j,nonlin} - d_{j,lin}}$ which defines $b_{j,\text{nnonlin}^c}$
\end{itemize}

\end{vbframe}

\begin{vbframe}{Example: Nonlinear effect decomposition}

\begin{itemize}
    \item 
        Suppose $n = 100$ uniformly distributed $x$ values between $0$ and $10$.

    \item 
        The response predictor $y = 2\sin(x) + 2x$ has a nonlinear and linear component.

    \item
        We apply CWB with $M = 500$ to $\{(\xi, \yi) | i = 1, \dots, n\}$ with:
        \begin{itemize}
            \item One model with $\mathcal{B} = \{b_{j,\text{lin}}, b_{j,\text{nonlin}}\}$
            \item One model with $\mathcal{B} = \{b_{j,\text{lin}}, b_{j,\text{nonlin}^c}\}$
        \end{itemize}
\end{itemize}

% Create figures with: rsrc/fig-centered-bl.R
\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figure/fig-decomp1.png}
    \hfill\includegraphics[width=0.45\textwidth]{figure/fig-decomp2.png}
\end{figure}

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{Fair Base Learner Selection}

\begin{itemize}

  \item
    Using splines and linear base learners in CWB will favor
    the more complex spline base learner over the linear base learner.

  \item
    This makes it harder to achieve the desired behavior of the base learner
    decomposition as explained previously.

  \item
    To conduct a fair base learner selection we set the degrees of freedom of all base learners equal.

  \item
    The idea is to set the regularization/penalty term of a single learner in a manner that their complexity is treated equally.

\end{itemize}

\framebreak

% ------------------------------------------------------------------------------

Especially for linear models and GAMs it is possible to transform the degrees of freedom into a corresponding penalty term.

\textcolor{red}{@BB nochmal nachlesen}

\begin{itemize}

  \item
    Parameters of the base learners are estimated via:
    $$
    \thetam_j = \left(\mathbf{Z}_j^T \mathbf{Z}_j + \lambda_j \mathbf{K}_j
    \right)^{-1}\mathbf{Z}_j^T \rmm\,,
    $$
    with $\mathbf{Z}_j$ the design matrix of the $j$-th base learner,
    $\lambda_j$ the penalty term, and $\mathbf{K}_j$ the penalty matrix.

  \item
    Having that kind of model, we use the hat matrix
    $\mathbf{S}_j = \mathbf{Z}_j\left(\mathbf{Z}_j^T \mathbf{Z}_j +
    \lambda_j \mathbf{K}_j\right)^{-1}\mathbf{Z}_j^T$ to define the degrees of
    freedom:
    $$
    \operatorname{df}(\lambda_j) = \trace\left(2\mathbf{S}_j - \mathbf{S}_j^T
    \mathbf{S}_j\right).
    $$
    \textbf{Note:} With $\lambda_j = 0$, $\mathbf{S}_j$ is the projection matrix
    into the target space with
    $\trace(\mathbf{S}_j) = \operatorname{rank}(\Xmat)$, which corresponds to
    the number of parameters in the model.

\end{itemize}

\framebreak

% ------------------------------------------------------------------------------

It is possible to calculate $\lambda_j$ by applying the Demmler-Reinsch
orthogonalization (see
\href{https://www.tandfonline.com/doi/abs/10.1198/jcgs.2011.09220}
{Hofer et al. (2011)}).

% (see Hofer et. al. (2011).\textit{\enquote{A framework for unbiased model selection based on boosting.}}).

Consider the following example of a GAM using splines with 24 parameters:

\begin{itemize}

  \item
    Setting $\operatorname{df} = 24$ gives $B$-splines with $\lambda_j = 0$.

  \item
    Setting $\operatorname{df} = 4$ gives $P$-splines with $\lambda_j = 418$.

  \item
    Setting $\operatorname{df} = 2$ gives $P$-splines with $\lambda_j = 42751174892$.

\end{itemize}

\begin{center}
\includegraphics[width=0.7\textwidth]{figure_man/df_to_lambda.pdf}
\end{center}

\end{vbframe}

\begin{vbframe}{Available base learners}

There is a large amount of possible base learners, e.g.:

\begin{itemize}
  \item Linear effects and interactions (with or without intercept)
  \item Uni- or multivariate splines and tensor product splines
  \item Trees
  \item Random effects and Markov random fields
  \item Effects of functional covariates
  \item ...
\end{itemize}

\lz

In combination with the flexible choice of loss functions, this allows boosting to fit  a huge number of different statistical models with the same algorithm. Recent extensions include GAMLSS-models, where multiple additive predictors are boosted to model different distribution parameters (e.g., conditional mean and variance for a Gaussian model).

\end{vbframe}
% ------------------------------------------------------------------------------
\begin{vbframe}{Partial Dependence Plots (PDP)}

If we use single features in base learners, we think of each base learner as a wrapper around a feature which represents the effect of that feature on the target variable. Base learners can be selected more than once (with varying parameter estimates), signaling that this feature is more important.\\
E.g. let $j \in \{ 1,2,3 \}$, the first three iterations might look as follows
\begin{align*}
m = 1: \quad & \hat{f}^{[1]}(\xv) = \hat{f}^{[0]} + \alpha \textcolor{blue}{\hat{b}_2(x_2, \hat{\theta}^{[1]})}\\
m = 2: \quad & \hat{f}^{[2]}(\xv) = \hat{f}^{[1]} + \alpha  \textcolor{orange}{\hat{b}_3(x_3, \hat{\theta}^{[2]})}\\
m = 3: \quad & \hat{f}^{[3]}(\xv) = \hat{f}^{[2]} + \alpha \textcolor{blue}{\hat{b}_2(x_2, \hat{\theta}^{[3]})}
\end{align*}

Due to linearity, $\hat{b}_2$ base learners can be aggregated:
$$
\hat{f}^{[3]}(\xv) = \hat{f}^{[0]} + \alpha (\textcolor{blue}{\hat{b}_2(x_2, \hat{\theta}^{[1]} + \hat{\theta}^{[3]})} + \textcolor{orange}{\hat{b}_3(x_3, \hat{\theta}^{[2]})})
$$

Which is equivalent to:
$\hat{f}^{[3]}(\xv) = \hat{f}_0 + \textcolor{blue}{\hat{f}_2(x_2)} + \textcolor{orange}{\hat{f}_3(x_3)}$.\\
Hence, $\hat{f}$ can be decomposed into the marginal feature effects (PDPs).



\end{vbframe}




\begin{vbframe}{Feature importance}
\begin{itemize}
  \item We can further exploit the additive structure of the boosted ensemble to
  compute measures of \textbf{variable importance}.
  \item To this end, we simply sum for each feature $x_j$ the improvements in
  empirical risk achieved over all iterations until
  $1 < m_{\text{stop}} \leq M$:
  % \begin{align*}
    $$VI_j = \sum_{m = 1}^{m_{\text{stop}}} \left( \riske \left(
    \fmd(\xv) \right) - \riske \left(\fm(\xv)
    \right) \right) \cdot \I_{[j \in j^{[m]})]},$$
  % \end{align*}
\end{itemize}

\end{vbframe}

% \begin{vbframe}{Available base learners}
%
% There is a large amount of possible base learners, e.g.:
%
% \begin{itemize}
%   \item Linear effects and interactions (with or without intercept)
%   \item Uni- or multivariate splines and tensor product splines
%   \item Trees
%   \item Random effects and Markov random fields
%   \item Effects of functional covariates
%   \item ...
% \end{itemize}
%
% \lz
%
% In combination with the flexible choice of loss functions, this allows boosting to fit  a huge number of different statistical models with the same algorithm. Recent extensions include GAMLSS-models, where multiple additive predictors are boosted to model different distribution parameters (e.g., conditional mean and variance for a Gaussian model).
%
% \end{vbframe}

% ------------------------------------------------------------------------------

% \begin{vbframe}{RF vs AdaBoost vs GBM vs Blackboost}

% Again the Spirals data from mlbench. Blackboost: mboost with regression trees as base learners
% \end{vbframe}

\begin{vbframe}{Take-home message}

\begin{itemize}
  \item Componentwise gradient boosting is the statistical re-interpretation of
  gradient boosting.
  \item We can fit a large number of statistical models, even in high dimensions
  ($p \gg n$).
  \item A drawback compared to statistical models is that we do not get valid
  inference for coefficients $\rightarrow$ post-selection inference.
  % This can be (partially) solved by bootstrap inference or related methods.
  \item In most cases, gradient boosting with trees will dominate componentwise
  boosting in terms of performance due to its inherent ability to include
  higher-order interaction terms.
  % In most cases, componentwise gradient boosting will have worse
  % predictive performance than gradient boosting with trees. This is often
  % because additive base learner motivated by regression models (LM, GLM, GAM)
  % usually do not include higher-order interaction terms.
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
