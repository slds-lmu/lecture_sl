\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-ensembles.tex}
\input{../../latex-math/ml-trees.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Boosting
  }{% Lecture title  
    Gradient Boosting: Advanced CWB
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/compboost-illustration-2.png
  }{
  \item Details of nonlinear BLs and splines
  \item Decomposition for splines
  \item Fair base learner selection
  \item Feature importance and PDPs
}

% ------------------------------------------------------------------------------

\begin{vbframe}{Nonlinear base learners}

As an alternative we can use nonlinear base learners, such as $P$- or
$B$-splines, which make the model equivalent to a
\textbf{generalized additive model (GAM)} (as long as the base learners keep
their additive structure, which is the case for splines).
\vspace{0.5cm}

\vfill

\begin{center}
\includegraphics[width=0.9\textwidth]{figure/bspline-basis.png}
\end{center}

% \vfill
%
% \begin{center}
% \includegraphics[width=1\textwidth]{figure_man/NBL02.png}
% \end{center}

\end{vbframe}

% ------------------------------------------------------------------------------

% tex file and figures are created automatically by: rsrc/fig-cwb-anim-nl.R
\input{tex/fig-cwb-anim-nl}

%\begin{vbframe}{boston housing: continued}
%
%\begin{itemize}
%  \small
%  \item This time we offer our model both
%  linear and nonlinear ($P$-spline) base learners (again, with intercept) for
%  the 13 features.
%  \item By iteration 100, the model has selected 8 out of the resulting 26 base
%  learners and consistently picked the nonlinear option.
%  We can trace how these spline predictors evolve over the iterations:
%\end{itemize}
%
%\vfill
%
%\begin{center}
%% \includegraphics[width=0.6\textwidth]{figure_man/spam-example.png}
%\includegraphics[width = \textwidth]{figure/compboost-illustration-3.png}
%\end{center}

% Unfortunately it's rather ugly to write down the decomposition

% <<spam-formula, eval = FALSE, echo = TRUE>>=
% formula <-
%   spam ~ bols(word_freq_make) + bbs(word_freq_make, df = 2, center = TRUE) +
%     bols(word_freq_address) + bbs(word_freq_address, df = 2, center = TRUE) +
%     bols(word_freq_all) + bbs(word_freq_all, df = 2, center = TRUE) +
% #  ...
% mboost(formula, data = spam, family = Binomial(), control = boost_control(mstop = 100))
% @
%
%\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Nonlinear effect decomposition}

%Even when allowing for more complexity we typically want to keep solutions as
%simple as possible.

%\lz

\citelink{KNEIB2009VARIABLE} %Kneib~et~al. (2009) 
proposed a decomposition of each base learner into a
constant, a linear and a nonlinear part.
The boosting algorithm will automatically decide which feature to include --
linear, nonlinear, or none at all:

\vspace{-0.7cm}

\begin{align*}
b_j(x_j, \thetam) & = b_{j, \text{const}}(x_j, \thetam) + b_{j, \text{lin}}
(x_j, \thetam) + b_{j, \text{nonlin}}(x_j, \thetam)\\
 & = \theta^{[m]}_\text{j,const} + x_j \cdot \theta^{[m]}_{j, \text{lin}} +
 s_j(x_j, \thetam_{j,\text{nonlin}}),
\end{align*}

\small
where
\begin{itemize}
  \small
  \item $\theta_\text{j,const}$ is the intercept of feature $j$,
  \item $x_j \cdot \theta^{[m]}_{j, \text{lin}}$ is a feature-specific linear
  base learner, and
  \item $s_j(x_j, \thetam_{j,\text{nonlin}})$ is a (centered) nonlinear base
  learner capturing deviation from the linear effect 
\end{itemize}

Careful: We usually also apply an orthogonalization procedure on top of this but skip technical details here. 

\framebreak

% \begin{itemize}
%     \item 
%         Suppose a BL $b_{j,\text{nonlin}}\in\mathcal{B}_{j,\text{nonlin}}$ (a spline BL) with design matrix $\bm{Z}_{j,\text{nonlin}}\in\R^{n\times d_{j,\text{nonlin}}}$ and a linear BL $b_{j,\text{lin}}\in\mathcal{B}_{j,\text{lin}}$ with design matrix $\bm{Z}_{j,\text{lin}}\in\R^{n\times d_{j,\text{lin}}}$.

%     \item
%         Often, $\mathcal{B}_{j,\text{lin}} \subseteq \mathcal{B}_{j,\text{nonlin}}$ and hence $b_{j,\text{lin}}$ is overlayed by $b_{j,\text{nonlin}}$ and not selected during training.

%      \item
%         Subtracting $b_{j,\text{lin}}$ from $b_{j,\text{nnnlin}}$ to obtain a centered nonlinear BL $b_{j,\text{nnonlin}^c}$ means to transform $\bm{Z}_{j,nonlin}$ so that the $span(\bm{Z}_{j,lin})$ is not included in $span(\bm{Z}_{j,nonlin})$.  
%         It follows that $b_{j,\text{nonlin}^c}\notin \mathcal{B}_{j,\text{lin}}$ and hence the BL cannot model any linear effect.

%     \item
%         The centering is done by computing the QR decomposition of $\bm{A} := \bm{Z}_{j,nonlin}^T \bm{Z}_{j,lin}$.

%     \item 
%         The first $k$ columns of the $\bm{Q}$ matrix from the QR decomposition form an orthonormal basis for the span of the first $k$ columns of $\bm{A}$.

%     \item 
%         Hence $\bm{Q}^\ast$, which is $\bm{Q}$ without the first $d_{j,\text{lin}}$ columns, rotates $\bm{Z}_{j,nonlin}$ to $\bm{Z}_{j,nonlin^c} = \bm{Q}^\ast\bm{Z}_{j,nonlin}\in\R^{n\times (d_{j,nonlin} - d_{j,lin})}$ which defines $b_{j,\text{nnonlin}^c}$
% \end{itemize}

\end{vbframe}

\begin{vbframe}{Nonlinear effect decomposition}

\begin{itemize}
    \item 
        Suppose $n = 100$ uniformly distributed $x$ values between $0$ and $10$.

    \item 
        The response $y = 2\sin(x) + x + 2 + \varepsilon$ has a nonlinear and linear component ($\varepsilon \sim \mathcal{N}(0,\frac{1}{2})$).

    \item
        We apply CWB with $M = 500$ to $\{(\xi, \yi)\}_{i=1}^{n}$ with:
        \begin{itemize}
            \item One model with $\mathcal{B} = \{b_{j,\text{lin}}, b_{j,\text{nonlin}}\}$
            \item One model with $\mathcal{B} = \{b_{j,\text{lin}}, b_{j,\text{nonlin}^c}\}$
        \end{itemize}
\end{itemize}

% Create figures with: rsrc/fig-centered-bl.R
\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figure/fig-decomp1.png}
    \hfill\includegraphics[width=0.45\textwidth]{figure/fig-decomp2.png}
\end{figure}

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{Fair Base Learner Selection}

\begin{itemize} \setlength{\itemsep}{1.6em}

  \item
    Using splines and linear base learners in CWB will favor
    the more complex spline BLs over the linear BLs

  \item
    This makes it harder to achieve the desired behavior of the base learner
    decomposition as explained previously

  \item
    To conduct a fair base learner selection, we set the degrees of freedom of all base learners equal

  \item
    The idea is to set a single learner's regularization/penalty term so that their complexity is treated equally

\item We also skip some technical details here

\end{itemize}

\framebreak

% ------------------------------------------------------------------------------

% Especially for linear models and GAMs it is possible to transform the degrees of freedom into a corresponding penalty term.

% %\textcolor{red}{@BB nochmal nachlesen}

% \begin{itemize}

%   \item
%     Parameters of the base learners are estimated via:
%     $$
%     \thetam_j = \left(\mathbf{Z}_j^T \mathbf{Z}_j + \lambda_j \mathbf{K}_j
%     \right)^{-1}\mathbf{Z}_j^T \rmm\,,
%     $$
%     with $\mathbf{Z}_j$ the design matrix of the $j$-th base learner,
%     $\lambda_j$ the penalty term, and $\mathbf{K}_j$ the penalty matrix.

%   \item
%     Having that kind of model, we use the hat matrix
%     $\mathbf{S}_j = \mathbf{Z}_j\left(\mathbf{Z}_j^T \mathbf{Z}_j +
%     \lambda_j \mathbf{K}_j\right)^{-1}\mathbf{Z}_j^T$ to define the (\textit{effective}) degrees of
%     freedom:
%     $$
%     \operatorname{df}(\lambda_j) = \trace\left(2\mathbf{S}_j - \mathbf{S}_j^T
%     \mathbf{S}_j\right).
%     $$
%     \textbf{Note:} With $\lambda_j = 0$, $\mathbf{S}_j$ is the projection matrix
%     into the target space with
%     $\trace(\mathbf{S}_j) = \operatorname{rank}(\Xmat)$, which corresponds to
%     the number of parameters in the model.

% \end{itemize}

\framebreak

% ------------------------------------------------------------------------------

% It is possible to calculate $\lambda_j$ by applying the Demmler-Reinsch
% orthogonalization (see
% \citebutton{Hofer et al. (2011)}{https://www.tandfonline.com/doi/abs/10.1198/jcgs.2011.0922}).
% % link invalid
% % (see Hofer et. al. (2011).\textit{\enquote{A framework for unbiased model selection based on boosting.}}).

% Consider the following example of a GAM using splines with 24 B-Spline basis functions:

% \begin{itemize}

%   \item
%     Setting $\operatorname{df} = 24$ gives $B$-splines with $\lambda_j = 0$.

%   \item
%     Setting $\operatorname{df} = 4$ gives $P$-splines with $\lambda_j = 418$.

%   \item
%     Setting $\operatorname{df} = 2$ gives $P$-splines with $\lambda_j = 42751174892$.

% \end{itemize}

% \begin{center}
% \includegraphics[width=0.7\textwidth]{figure_man/df_to_lambda.pdf}
% \end{center}

\end{vbframe}

\begin{vbframe}{Available base learners}

There is a large number of possible base learners, e.g.:

\begin{itemize}
  \item Linear effects and interactions (with or without intercept)
  \item Uni- or multivariate splines and tensor product splines
  \item Trees
  \item Random effects and Markov random fields
  \item Effects of functional covariates
  \item ...
\end{itemize}

\lz

In combination with the flexible choice of loss functions, boosting can be applied to fit a huge class of models.\\ 
\vspace{0.1cm}
Recent extensions include distributional regression (GAMLSS), where multiple additive predictors are boosted to model all distributional parameters (e.g., cond. mean and variance for a Gaussian model).

\end{vbframe}
% ------------------------------------------------------------------------------
\begin{vbframe}{Partial Dependence Plots }

If we use single features in base learners, we consider each BL as a wrapper around a feature representing the feature's effect on the target. BLs can be selected more than once (with varying parameter estimates), signaling that this feature is more important.\\
E.g. let $j \in \{ 1,2,3 \}$, the first three iterations might look as follows
\begin{align*}
m = 1: \quad & \hat{f}^{[1]}(\xv) = \hat{f}^{[0]} + \alpha \textcolor{blue}{\hat{b}_2(x_2, \hat{\theta}^{[1]})}\\
m = 2: \quad & \hat{f}^{[2]}(\xv) = \hat{f}^{[1]} + \alpha  \textcolor{orange}{\hat{b}_3(x_3, \hat{\theta}^{[2]})}\\
m = 3: \quad & \hat{f}^{[3]}(\xv) = \hat{f}^{[2]} + \alpha \textcolor{blue}{\hat{b}_2(x_2, \hat{\theta}^{[3]})}
\end{align*}

Due to linearity, $\hat{b}_2$ base learners can be aggregated:
$$
\hat{f}^{[3]}(\xv) = \hat{f}^{[0]} + \alpha (\textcolor{blue}{\hat{b}_2(x_2, \hat{\theta}^{[1]} + \hat{\theta}^{[3]})} + \textcolor{orange}{\hat{b}_3(x_3, \hat{\theta}^{[2]})})
$$

Which is equivalent to:
$\hat{f}^{[3]}(\xv) = \hat{f}_0 + \textcolor{blue}{\hat{f}_2(x_2)} + \textcolor{orange}{\hat{f}_3(x_3)}$.\\
Hence, $\hat{f}$ can be decomposed into the marginal feature effects (PDPs).



\end{vbframe}




\begin{vbframe}{Feature importance}
\begin{itemize}
  \item We can further exploit the additive structure of the boosted ensemble to
  compute measures of \textbf{variable importance}.
  \item To this end, we simply sum for each feature $x_j$ the improvements in
  empirical risk achieved over all iterations until
  $1 < m_{\text{stop}} \leq M$:
  % \begin{align*}
    $$VI_j = \sum_{m = 1}^{m_{\text{stop}}} \left( \riske \left(
    \fmd(\xv) \right) - \riske \left(\fm(\xv)
    \right) \right) \cdot \I_{[j \in j^{[m]})]},$$
  % \end{align*}
\end{itemize}

\end{vbframe}

% \begin{vbframe}{Available base learners}
%
% There is a large amount of possible base learners, e.g.:
%
% \begin{itemize}
%   \item Linear effects and interactions (with or without intercept)
%   \item Uni- or multivariate splines and tensor product splines
%   \item Trees
%   \item Random effects and Markov random fields
%   \item Effects of functional covariates
%   \item ...
% \end{itemize}
%
% \lz
%
% In combination with the flexible choice of loss functions, this allows boosting to fit  a huge number of different statistical models with the same algorithm. Recent extensions include GAMLSS-models, where multiple additive predictors are boosted to model different distribution parameters (e.g., conditional mean and variance for a Gaussian model).
%
% \end{vbframe}

% ------------------------------------------------------------------------------

% \begin{vbframe}{RF vs AdaBoost vs GBM vs Blackboost}

% Again the Spirals data from mlbench. Blackboost: mboost with regression trees as base learners
% \end{vbframe}

\begin{vbframe}{Take-home message}

\begin{itemize}
  \item Componentwise gradient boosting is the statistical re-interpretation of
  gradient boosting
  \item We can fit a large number of statistical models, even in high dimensions
  ($p \gg n$)
  \item A drawback compared to statistical models is that we do not get valid
  inference for coefficients $\rightarrow$ post-selection inference
  % This can be (partially) solved by bootstrap inference or related methods.
  \item In most cases, gradient boosting with trees will dominate componentwise
  boosting in terms of performance due to its inherent ability to include
  higher-order interaction terms
  % In most cases, componentwise gradient boosting will have worse
  % predictive performance than gradient boosting with trees. This is often
  % because additive base learner motivated by regression models (LM, GLM, GAM)
  % usually do not include higher-order interaction terms.
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
