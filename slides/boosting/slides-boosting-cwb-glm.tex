\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-ensembles.tex}
\input{../../latex-math/ml-trees.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Boosting
  }{% Lecture title  
    Gradient Boosting: CWB and GLMs
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/compboost-to-glm-iter10000.png
  }{
  \item Understand relationship of CWB and GLM
}
% ------------------------------------------------------------------------------

\begin{vbframe}{Relation to GLM}

In the simplest case we use linear models (without intercept) on single features
as base learners:

$$
  b_j(x_j,\theta) = \theta x_j  \quad \text{for } j = 1, 2, \dots, p \quad
  \text{and with } b_j \in \mathcal{B}_j = \{\theta x_j  ~\rvert~ \theta \in
  \mathbb{R} \}.
$$


This definition will result in an ordinary \textbf{linear regression} model.

\lz
% .\footnote{Note: a linear model base learner without intercept only makes sense if the covariates are centered (see \texttt{mboost} tutorial, page7)}


\begin{itemize} 
\setlength{\itemsep}{1.6em}
  %\item Note that linear base learners without intercept only make sense for
  %covariates that have been centered before.
  \item In the limit, boosting algorithm will converge to the maximum likelihood solution.
  %\item If we let the boosting algorithm converge, i.e., let it run for a really
  %long time, the parameters will converge to the \textbf{same solution} as the
  %maximum likelihood estimate.
  \item By specifying loss as NLL of exponential family distribution with an appropriate link function, CWB is equivalent to (regularized) \textbf{GLM}.
  %\item This means that, by specifying a loss function according to the negative
  %likelihood of a distribution from an exponential family and defining a link
  %function accordingly, this kind of boosting is equivalent to a (regularized)
  %\textbf{generalized linear model (GLM)}.
\end{itemize}

\framebreak

% ------------------------------------------------------------------------------

But: We do not \emph{require} an exponential family distribution and we can - in principle - apply it to any differentiable loss %and thus are able to fit models
%to all kinds of other distributions and losses, as long as we can calculate (or
%approximate) a derivative of the loss.
% Note, however, that this does not imply that the algorithm does something
% meaningful (e.g., non-convex loss functions would still require some
% additional effort).

\lz

Usually we do not let the boosting model converge fully, but use \textbf{early stopping} for the sake of regularization and feature selection.

\lz

Even though resulting model looks like a GLM, we do not have valid standard
errors for our coefficients,
so cannot provide confidence or prediction intervals or perform tests etc.
$\rightarrow$ post-selection inference.

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Example: Logistic regression with CWB}

Fitting a logistic regression (GLM with a Bernoulli distributed response) requires the specification of the loss as function as

$$
  L(y, \fx) = -y \cdot \fx + \log (1 + \exp(\fx)),\ y\in\{0, 1\}
$$

Note that CWB (as gradient boosting in general) predicts a score $\fx \in \R$. Squashing the score $\fx$ to $\pi(\xv) = s(\fx) \in [0,1]$ corresponds to transforming the linear predictor of a GLM to the response domain with a link function $s$:

\begin{itemize}
  \item $s(\fx) = (1 + \exp(-\fx))^{-1}$ for logistic regression.
  \item $s(\fx) = \Phi(\fx)$ for probit regression with $\Phi$ the CDF of the standard normal distribution.
\end{itemize}

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{Example: CWB parameter convergence}

The following figure shows the parameter values for $m \leq 10000$ iterations as well as the estimates from a linear model as crosses (GLM with normally distributed errors):

\begin{center}
\includegraphics[width=0.9\textwidth]{figure/compboost-to-glm-iter10000.png}
\end{center}

Throughout the fitting of CWB, the parameters estimated converge to the GLM solution. The used data set is \href{https://github.com/topepo/AmesHousing}{Ames Housing}.

\end{vbframe}


\endlecture
\end{document}
