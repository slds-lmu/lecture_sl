\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble} 
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-ensembles.tex}
\input{../../latex-math/ml-trees.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Boosting
  }{% Lecture title  
  	Gradient Boosting: Classification
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/boosting_classif_title.png
  }{
  \item GB for binary classification simply uses
	  Bernoulli or exponential loss
  \item For multiclass we fit $g$ discriminant functions in parallel
}

\begin{vbframe}{Binary classification}


For $\Yspace = \{0, 1\}$, we simply have to select an appropriate loss function, so let us
use Bernoulli loss as in logistic regression:

$$ \Lxy = - y \cdot \fx + \log(1 + \exp(\fx)).$$

Then,

\vspace{-0.5cm}

\begin{align*}
\tilde{r}(f) &=-\pd{\Lxy}{\fx} \\
&= y - \frac{\exp(\fx)}{1 + \exp(\fx)} \\
&= y - \frac{1}{1 + \exp(-\fx)} = y - s(\fx).
\end{align*}

Here, $s(\fx)$ is the logistic function, applied to a scoring model.
Hence, effectively, the pseudo-residuals are $y - \pix$.

Through $\pix = s(\fx)$ we can also estimate posterior probabilities.

\framebreak
%

\begin{itemize}
\item Rest works as in regression.
\item NB: We fit regression BLs against the PRs with $L2$ loss. 
%\item The tree extension for boosting for the binary case works exactly as in regression.
\item  Exponential loss works too. 
    In practice there is no big difference, although Bernoulli loss 
    makes a bit more sense from a theoretical (maximum likelihood) perspective.
\item It can be shown GB with exp loss is basically equivalent to and generalizes AdaBoost. 
% \item It follows that GB is a generalization of AdaBoost which can also use other loss functions and be used for different ML scenarios.
\end{itemize}


\end{vbframe}

\begin{vbframe}{Example: 2D Circle data}

% We now illustrate the boosting iterations for a classification example in a similar manner as we did for regression.
% However, we will now look at a simulation example with 2 instead of 1 influential features and one binary target variable.

\begin{columns}
\column{5.5cm}
\begin{itemize}
\item \texttt{mlbench} \texttt{circle} data with $n = 100$ 
\item Bernoulli loss 
\item BL = shallow tree with max. depth of 3
\item We initialized with $f^{[0]} = 0.$
\end{itemize}
\column{4.5cm}
\begin{center}
\includegraphics[width=\textwidth]{figure/boosting_classif_example.png}
\end{center}

\end{columns}



\end{vbframe}

\begin{frame}{Example: 2D Circle data}
BG color is predicted probs on LHS on RHS we show and preds of BL.

\only<1>{ 
\includegraphics[width=\textwidth]{figure/boosting_classif_1.png}
\includegraphics[width=\textwidth]{figure/boosting_classif_error_1.png}
}
\only<2>{ 
	\includegraphics[width=\textwidth]{figure/boosting_classif_2.png}
	\includegraphics[width=\textwidth]{figure/boosting_classif_error_2.png}
}
\only<3>{ 
	\includegraphics[width=\textwidth]{figure/boosting_classif_5.png}
	\includegraphics[width=\textwidth]{figure/boosting_classif_error_5.png}
}
\only<4>{ 
	\includegraphics[width=\textwidth]{figure/boosting_classif_10.png}
	\includegraphics[width=\textwidth]{figure/boosting_classif_error_10.png}
}
\only<5>{ 
	\includegraphics[width=\textwidth]{figure/boosting_classif_100.png}
	\includegraphics[width=\textwidth]{figure/boosting_classif_error_100.png}
}


\end{frame}

%\section{Interactive Playgrounds}

% \begin{vbframe}{Gradient Boosting Playground}
% \begin{center}
% 
% \includegraphics[width=0.7\textwidth]{figure_man/gbm_playground.png}
% 
% \href{http://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html}{\beamergotobutton{Open in browser.}}
% 
% \end{center}
% \end{vbframe}

% \begin{vbframe}{mlrPlayground}
% \begin{center}
% 
% \includegraphics[width=\linewidth]{figure_man/mlrplayground_welcome.png}
% 
% \href{https://compstat-lmu.shinyapps.io/mlrPlayground/}{\beamergotobutton{Open in browser.}}
% 
% \end{center}
% \end{vbframe}

% \section{Gradient Boosting for Multiclass Problems}

\begin{vbframe}{Multiclass problems}

We proceed as in softmax regression and model a categorical distribution with multinomial / log loss.
For $\Yspace = \{1, \ldots, g\}$, we create $g$ discriminant functions $\fkx$, one for each class and each one being an \textbf{additive} model of base learners.

We define the $\pi_k(\xv)$ through the softmax function:
$$ \pikx = s_k(f_1(\xv), \ldots, f_g(\xv)) = \exp(\fkx) / \sum_{j=1}^g \exp(f_j(\xv)). $$

Multinomial loss $L$:
$$ L(y, f_1(\xv), \ldots f_g(\xv)) = - \sumkg \mathds{1}_{\{y = k\}} \ln \pikx. $$

Pseudo-residuals:
$$-\pd{L(y, f_1(\xv), \ldots, f_g(\xv))}{\fkx} =  \mathds{1}_{\{y = k\}} - \pikx. $$


\framebreak

\begin{algorithm}[H]
  \begin{footnotesize}
  \begin{center}
  \caption{GB for Multiclass}
    \begin{algorithmic}[1]
      \State Initialize $f_{k}^{[0]}(\xv) = 0,\ k = 1,\ldots,g$
      \For{$m = 1 \to M$}
      \State Set $\pik^{[m]}(\xv) = \frac{\exp(f_k^{[m]}(\xv))}{\sum_j \exp(f_j^{[m]}(\xv))}, k = 1,\ldots,g$
            \For{$k = 1 \to g$}
            \State For all $i$: Compute $\rmi_k = \mathds{1}_{\{\yi = k\}} - \pik^{[m]}(\xi)$
              \State Fit a regression base learner $\hat{b}^{[m]}_k$ to the pseudo-residuals $\rmi_k$.
              %\State Obtain $\betamh_k$ by constant learning rate or line-search.
              \State Update $\hat{f}_k^{[m]} = \hat{f}_k^{[m-1]} + \alpha \hat{b}^{[m]}_k$
            \EndFor
      \EndFor
    \State Output $\hat{f}_1^{[M]}, \ldots, \hat{f}_g^{[M]}$
    \end{algorithmic}
    \end{center}
    \end{footnotesize}
\end{algorithm}

\end{vbframe}

% \begin{vbframe}{Example: 2D Iris}
	
	% We now look at an example with 2 influential features and one multiclass target variable.
	
	% \begin{columns}
		% \column{5.5cm}
		% \begin{itemize}
			% \item We used the popular dataset \texttt{iris} containing 3 classes of 50 observations each, where a class refers to a type of iris plant. 
			% \item Among multiple features in the dataset, we chose $Sepal.Length$ and $Sepal.Width$ for visualization purpose.
		% \end{itemize}
		% \column{4.5cm}
		% \begin{center}
			% \includegraphics[width=\textwidth]{figure/iris_ds_plot.png}
		% \end{center}
		
	% \end{columns}
	
	

% \end{vbframe}

\begin{frame}{Example: 2D Iris}
	LHS: BG color is predicted probs and point col is true label;  
	RHS: Contour lines of discriminant functions.
    % Background color and contours refer to the predicted class's score $f_k^{[m]}$. 
	
	
	\only<1>{ 
		\includegraphics[width=\textwidth]{figure/boosting_multiclass_1.png}
		\vfill
		Iteration=1
	}
	\only<2>{ 
		\includegraphics[width=\textwidth]{figure/boosting_multiclass_2.png}
		\vfill
		Iteration=2
	}
	\only<3>{ 
		\includegraphics[width=\textwidth]{figure/boosting_multiclass_5.png}
		\vfill
		Iteration=5
	}
	\only<4>{ 
		\includegraphics[width=\textwidth]{figure/boosting_multiclass_10.png}
		\vfill
		Iteration=10
	}
	\only<5>{ 
		\includegraphics[width=\textwidth]{figure/boosting_multiclass_100.png}
		\vfill
		Iteration=100
	}
	
	
\end{frame}



\endlecture
\end{document}

