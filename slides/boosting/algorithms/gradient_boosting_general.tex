\begin{algorithm}[H]
  \begin{footnotesize}
  \begin{center}
  \caption{Gradient Boosting Algorithm.}
    \begin{algorithmic}[1]
      \State Initialize $\hat{f}^{[0]}(\xv) = \argmin_{\theta_0\in\R} \sumin L(\yi, \theta_0)$
      %\State Set the learning rate $\beta$ to a small constant value
      \For{$m = 1 \to M$}
          \State For all $i$: $\rmi = -\left[\pd{L(y,f)}{f}\right]_{f=\fmdh(\xi),y=\yi}$
        \State Fit a regression base learner to the vector of pseudo-residuals $\rmm$:
        \State $\thetamh = \argmin \limits_{\bm{\theta}} \sumin (\rmi - b(\xi, \bm{\theta}))^2$
        %\State Line search: $\betamh = \argmin_{\beta} \sumin L(\yi, \fmd(\xv) + \beta b(\xv, \thetamh))$
        \State Set $\alpha^{[m]}$ to $\alpha$ being a small constant value or via line search
        \State Update $\fmh(\xv) = \fmdh(\xv) + \alpha^{[m]} b(\xv, \thetamh)$
      \EndFor
      \State Output $\fh(\xv) = \hat{f}^{[M]}(\xv)$
    \end{algorithmic}
    \end{center}
    \end{footnotesize}
\end{algorithm}
