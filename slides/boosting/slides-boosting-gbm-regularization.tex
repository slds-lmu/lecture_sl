\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble} 
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-ensembles.tex}
\input{../../latex-math/ml-trees.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Boosting
  }{% Lecture title  
    Gradient Boosting: Regularization
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/gbm_sine_title.png
  }{
  \item Learn about three main regularization options: number of iterations, 
    tree depth and shrinkage
  \item Understand how regularization influences model fit
}

\begin{vbframe}{Iters, Tree Depth, Learn Rate}

GB can overfit easily, due to its aggressive loss minimization.

\begin{blocki}{Options for regularization:}
\item Limit nr of iters $M$, i.e., additive components (\enquote{early stopping}),
    % i.e., limit nr of additive components.
\item Limit depth of trees.
    Can also be interpreted as choosing the order of interaction (see later).
\item Use a small learn rate $\alpha$ for only mild model updates. \\
    $\alpha$ a.k.a. shrinkage.
\end{blocki}

% The latter is achieved by multiplying $\betam$ with a small $\nu \in (0,1]$:
% $$ 
% \fm(\xv) = \fmd(\xv) + \betam b(\xv, \thetam) \,.
% $$

% Note: If $\alpha^{[m]}$ is found via line search, we multiply the next base learner by a small constant factor to shorten the step length.
% In the case of a (commonly used) constant learning rate $\alpha$, this additional parameter 
% can be factored in by choosing a smaller value for $\alpha$.
% Hence, in the following, we call $\alpha$ the shrinkage parameter or learning rate.


% \framebreak



\begin{blocki}{Practical hints:}
\item Optimal values for $M$ and $\alpha$ strongly depend on each other:
by increasing $M$ one can use a smaller value for $\alpha$ and vice versa.

\item Fast option = Make $\alpha$ small and choose $M$ by CV. 

\item Probably best to tune all 3 hyperpars jointly via, e.g., CV.
\end{blocki}

\end{vbframe}

\begin{vbframe}{Stochastic gradient boosting}

\begin{itemize}
\item Minor modification to incorporate the advantages of bagging
\item In each iter, we only fit on a random subsample of the train data
\item Especially for small train sets, this often leads helps
\item Size of random sets = new hyperpar
% substantial empirical improvements.
% How large the improvements are depends on data structure, size of the dataset,
% base learner and size of the subsamples (so this is another tuning parameter).

\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Example: Sinusoidal with Tree stumps}

% BL = tree stumps. 

% \vfill

\begin{center}
  \includegraphics[width=\textwidth]{figure/gbm_sine.png}
\end{center}

% \vfill

Works quite nicely without noise, but overfits on the RHS.

% \small
% \begin{itemize}
  % \item Iterating this very simple base learner achieves a rather nice
  % approximation of a smooth model in the end.
  % \item Again, the model overfits the noisy case with less 
  % regularization.
% \end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

%\begin{vbframe}{Variable importance}

%As for random forests, we can construct a variable importance measure to help
%interpret the model.

%\lz
%For a regression tree $b$, we can define such a measure $I^2_j(b)$ as the sum over the reduction of
%impurity ($\rightarrow$ reduction of variance) at all inner knots where the tree splits with respect to feature $x_j$.

%\framebreak

%For an additive boosting model one just takes

%$$ I^2_j = \frac{1}{M} \sum_{m=1}^M I^2_j(\bmm) $$

%%From those importance values we take the squared root and scale them so that the most important feature gets the value 100.
%To get the relative influence measures, the resulting $I^2_j, j = 1, \dots, p$ are scaled so that they sum to 1.

%\lz
%For a $g$-class problem one has $g$

%$$ \fxk = \sum_{m=1}^M b_k^{[m]}(x), \quad
%  I^2_{jk} = \frac{1}{M} \sum_{m=1}^M I^2_j(b_k^{[m]}) $$

%$$ I^2_j = \frac{1}{g} \sum_{k=1}^g I^2_{jk} $$

%\end{vbframe}

% \begin{vbframe}{Visualization 2}
% 
% \begin{center}
%   \includegraphics[width=\textwidth]{figure/gbm_sine.png}
% \end{center}
% 
% \vfill
% 
% \footnotesize
% \begin{itemize}
%   \item Iterating this very simple tree-stump base learner yields a rather nice
%   approximation of a smooth model in the end.
%   \item Severe overfitting apparent in the noisy case. We will discuss and solve 
%   this problem later.
% \end{itemize}
% 
% \end{vbframe}

\begin{vbframe}{example: spirals data}

% Consider the \texttt{spirals} data set with $\mathit{sd} = 0.1$ and $n = 300$.
We examine effect of learn rate, with fixed nr of trees and fixed depth.
% 0k and choosing a tree depth of 10:

\vfill

\includegraphics[width = \textwidth]{figure/gbm_regu_oversmoothing_overfitting}

\vfill

We observe an oversmoothing effect in the left scenario with strong 
regularization (i.e., very small learning rate) and overfitting when 
regularization is too weak (right). $\alpha = 0.001$  yields a pretty good fit.

\end{vbframe}

\begin{vbframe}{Example: Spam detection with Trees} 

% The data set we will examine briefly in the following was collected at the Hewlett Packard laboratories
% to train a personalized spam mail detector.

% \lz

% It contains data of 4601 emails. 2788 mails were regular mails and 1813 were spam.
% There are 57 numerical predictors available measuring e.g. the frequency of the most frequent words
% and special characters as well as runlengths of words in all capitals.

% \lz

% We use the R package \pkg{gbm}, which implements the introduced version of gradient boosting.

% \framebreak

% <<gbm-spam-example, eval = FALSE, echo = TRUE>>=
% library(gbm)
% data(spam, package = "ElemStatLearn")
% spam$spam = as.numeric(spam$spam) - 1 # gbm requires target to be 0/1
% gbm(spam ~ ., data = spam,
%   distribution = "bernoulli", # classification
%   n.trees = 100, # M = 100
%   interaction.depth = 2, # max. tree depth = 2
%   shrinkage = 0.001, # nu = 0.001
% )
% @
%
% \lz

% We fit a gradient boosting model for different parameter values:

\begin{table}[]
\footnotesize
\centering
\begin{tabular}{l|l}
Hyperpar     & Range                         \\
\hline
Loss        & Bernoulli (for classification) \\
Number of trees $M$ & $\{0, 1,\dots,10000\}$              \\
Shrinkage $\alpha$     & $\{0.001, 0.01, 0.1\}$           \\
Max. tree depth     & $\{1, 3, 5, 20\}$
\end{tabular}
\end{table}

\vfill
Use 3-CV in grid search; optimal config in red:


% \framebreak

% Misclassification rates for different hyperparameter settings (shrinkage and maximum tree depth) of gradient boosting:

\begin{center}
\includegraphics[width=\textwidth]{figure/gbm_spam.png}
\end{center}

% \begin{figure}
%   \includegraphics[width=10cm, height=6cm]{figure_man/gbm_spam_effects.pdf}
% \end{figure}

% \framebreak

% \begin{figure}
  % \includegraphics[width=8cm]{figure_man/gbm_spam_imp_ggplot.pdf}
  % \caption{\footnotesize Variable Importance for model with $\nu = 0.1, M = 1380$ and tree depth of $4$.}
% \end{figure}

% \framebreak

% \begin{figure}
%  \includegraphics[width=6cm]{figure_man/gbm_spam_gbmperf.pdf}
%  \caption{\footnotesize Deviance}
% \end{figure}

%
% \begin{figure}
%   \includegraphics[width=8cm, height=5cm]{figure_man/gbm_spam_partdep.pdf}
%   \caption{\footnotesize Partial Dependency Plot for 2 important features.
%   Plotted is f in dependency of one feature, if all other features are integrated over.}
% \end{figure}

\end{vbframe}


\endlecture
\end{document}
