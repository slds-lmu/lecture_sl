\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Curse of Dimensionality
  }{% Lecture title  
    Curse of Dimensionality - Examples Learning Algorithms
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/knn_density_plot.png
  }{
  \item See how the performance of k-NN and the linear model deteriorates in high-dimensional spaces 
}

\begin{vbframe}{Example: k-NN}

Let us look at the performance of algorithms for increasing dimensionality. First, we consider the k-NN algorithm:

\begin{itemize}
\item In a high dimensional space, data points are spread across a huge space.
\item The distance to the \textbf{next neighbor} $d_{NN1}(\xv)$ becomes extremely large.
\item The distance might even get so large that all points are \textbf{equally far} away - we cannot really determine the nearest neighbor anymore.
\end{itemize}

\framebreak 

Minimal, mean and maximal (NN)-distances of $10^{4}$ points uniformly distributed in the hypercube $[0,1]^p$:

\begin{center}
\includegraphics[width = 11cm ]{figure/knn_table.png}
\end{center}

\framebreak

We see a decrease of relative contrast\footnote{[\href{https://bib.dbvis.de/uploadedFiles/155.pdf}{Aggarwal et al., 2001}]} $c := \frac{\max(d(\xv,\tilde{\xv})) - \min(d(\xv,\tilde{\xv}))}{\max(d(\xv,\tilde{\xv}))}$ and \enquote{locality}\footnote{our non-standard definition} $l := \frac{\overline{d(\xv,\tilde{\xv})} -  \overline{d_{NN1}(\xv)}}{\overline{d(\xv,\tilde{\xv})}}$ with increasing number of dimensions $p$:


\begin{center}
\includegraphics[width = 11cm ]{figure/knn_contrast_locality_plot.png}
\end{center}

\end{vbframe}
\begin{vbframe}{Example: k-NN}


The consequences for the k-nearest neighbors approach can be summarized as follows:

\begin{itemize}
 \item At constant sample size $n$ and growing $p$, the distance between the observations increases\\
 $\rightarrow$ the coverage of the $p$-dimensional space decreases,\\
 $\rightarrow$ every point becomes isolated / far way from all other points.
 \item The size of the neighborhood $N_k(x)$ also \enquote{increases}\\
       (at constant $k$) \\
       $\rightarrow$ it is no longer a \enquote{local} method.
 \item Reducing $k$ dramatically does not help much either,
since the fewer observations we average, the higher the variance of our fit.
 \item[$\rightarrow$] k-NN estimates get more inaccurate with increasing dimensionality of the data.
\end{itemize}

\framebreak

To demonstrate this, we generate an artificial data set of dimension $p$ as follows: We define $a = \frac{2}{\sqrt{p}}$ and 

\begin{itemize}
\item with probability $\frac{1}{2}$ we generate a sample from class $1$ by sampling from a Gaussian with mean $\pmb\mu = (a, a, ..., a)$ and unit covariance matrix
\item with probability $\frac{1}{2}$ we generate a sample from class $2$ by sampling from a  Gaussian with mean $- \pmb\mu = (-a, -a, ..., -a)$ and unit covariance matrix
\end{itemize}


\framebreak 

\begin{center}
\includegraphics[width = 11cm]{figure/knn_density_plot.png}
\end{center}

\framebreak 
This example is constructed such that the Bayes error is always constant and does not depend on the dimension $p$. 

\lz 

The Bayes optimal classifiers predicts $\hat y = 1$ iff

\begin{footnotesize}
  \begin{eqnarray*}
  \P\left(y = 1 ~|~ \xv\right) &=& \frac{p(\xv ~|~y = 1)\P(y = 1)}{p(\xv)} = \frac{1}{2} \cdot \frac{p(\xv ~|~y = 1)}{p(\xv)}\\
  &\ge& \frac{1}{2} \cdot \frac{p(\xv ~|~y = 2)}{p(\xv)} \\ &=& \frac{p(\xv ~|~y = 2)\P(y = 2)}{p(\xv)} = \P\left(y = 2 ~|~ \xv\right). 
  \end{eqnarray*}
\end{footnotesize}

This is equivalent to 

\vspace*{-0.5cm}

\begin{footnotesize}
  \begin{eqnarray*}
  \hat y = 1 &\Leftrightarrow& \exp\left(-\frac{1}{2} \left(\xv - \pmb\mu\right)^\top \left(\xv - \pmb\mu\right)\right) \ge \exp\left(-\frac{1}{2} \left(\xv + \pmb\mu\right)^\top \left(\xv + \pmb\mu\right)\right) \\
  &\Leftrightarrow& \xv^\top \mu \ge 0. 
  \end{eqnarray*}
\end{footnotesize}

\framebreak

Optimal Bayes classifier and Bayes error (shaded area):

\begin{center}
\includegraphics[width = 8cm]{figure/knn_error_plot.png}
\end{center}


\framebreak 

We can calculate the corresponding expected misclassification error (Bayes error)

\begin{footnotesize}
\begin{eqnarray*}
&&  p(\hat y = 1 ~|~y = 2)\P(y = 2) + p(\hat y = 2 ~|~y = 1)\P(y = 1)\\
&=& \frac{1}{2} \cdot p(\xv^\top \pmb\mu \ge 0 ~|~ y = 2) + \frac{1}{2} \cdot p(\xv^\top \pmb\mu \le 0 ~|~ y = 1) \\
&\overset{\text{symm.}}{=}& p(\xv^\top \pmb\mu \le 0 ~|~ y = 1) = p\left(\sum_{i = 1}^p a \xv_i \le 0 ~|~ y = 1\right)\\
  &=& p\left(\sum_{i = 1}^p \xv_i \le 0 ~|~ y = 1\right). 
\end{eqnarray*}
\end{footnotesize}

$\sum_{i = 1}^p \xv_i ~|~ y = 1\sim \mathcal{N}(p \cdot a,~ p)$\,, because it is the sum of independent normal random variables $\xv_i ~|~ y = 1 \sim \mathcal{N}\left(a, 1\right)$ (the vector $\xv ~|~ y = 1$ follows a $\mathcal{N}\left(\pmb\mu, \id\right)$ distribution with $\pmb\mu = \left(a, ..., a\right)$). 

\framebreak 

We get for the Bayes error: 

\begin{eqnarray*}
 &=& p\left(\frac{\sum_{i = 1}^p \xv_i - p \cdot a}{\sqrt{p}} \le \frac{- p\cdot a}{\sqrt{p}} ~|~ y = 1\right) \\ &=& \Phi(- \sqrt{p} a) \overset{a = \frac{2}{\sqrt{p}}}{=} \Phi(- 2) \approx 0.0228,
\end{eqnarray*}

where $\Phi$ is the distribution function of a standard normal random variable. 

\lz 

We see that the Bayes error is independent of $p$. 


\framebreak

We also train a k-NN classifier for $k = 3, 7, 15$ for increasing dimensions and monitor its performance (evaluated by $10$ times repeated $10$-fold CV).
\medskip

\begin{center}
\includegraphics[width = 11cm ]{figure/knn_misclassification_plot.png}
\end{center}

$\to$ k-NN deteriorates quickly with increasing dimension


\end{vbframe}


\begin{vbframe}{Example: Linear Model}
We also investigate how the linear model behaves in high dimensional spaces.

\begin{itemize}
\item We take the Boston Housing data set, where the value of houses in the area around Boston is predicted based on $13$ features describing the region (e.g., crime rate, status of the population, etc. ).
\item We train a linear model on the data consisting of 506 observations. 
\item We artificially create a high-dimensional dataset by adding $100, 200, 300, ...$ noise variables (containing no information at all) and look at the performance of a linear model trained on this modified data ($10$ times repeated $10$-fold CV).
\end{itemize}
\framebreak

We compare the performance of an LM to that of a regression tree.

\vspace*{0.1cm}
\begin{center}
\includegraphics[width = 11cm ]{figure/lm_mse_plot.png}
\end{center}


$\rightarrow$ The unregularized LM struggles with the added noise features, while our tree seems to nicely filter them out.

\vfill

\begin{footnotesize}
\textbf{Note}: Trees automatically perform feature selection as only one feature at a time is considered for splitting (the smaller the depth of the tree, the less features are selected). Thus, they often perform well in high-dimensional settings. 
\end{footnotesize}

\framebreak
\begin{itemize}
\item The regression coefficients of the noise features can not be estimated precisely as zero in the unregularized LM due to small random correlations. 
\item With an increasing number of these noise features, the prediction error rises.
\item To see this, we can quantify the influence of the noise features on the prediction of each observation. \\ 
Therefore we decompose the response $\hat y^{(i)}$ of each iterations' test set into $\hat y^{(i)}_{\text{true}}$ (predicted with noise features set to 0) and $\hat y^{(i)}_{\text{noise}}$ (predicted with true features set to 0), s.t. $\hat y^{(i)} =  \hat y^{(i)}_{\text{true}} + \hat y^{(i)}_{\text{noise}} + \text{intercept}$. \\
With this, we can define the \enquote{average proportional influence of the noise features} $\kappa := \overline{\left( \frac{|\hat y^{(i)}_{\text{noise}}|}{|\hat y^{(i)}_{\text{true}}| + |\hat y^{(i)}_{\text{noise}}|} \right)}$.

\end{itemize}
\framebreak

\begin{center}
\includegraphics[width = 11cm ]{figure/lm_noise_plot.png}
\end{center}

When we add 400 noise features to the model, most of the time, on average, over $50\%$ of the flexible part of the prediction $(\hat y^{(i)} - \text{intercept})$ is determined by the noise features.

\end{vbframe} 

\begin{vbframe}{COD: Ways out}

Many methods besides k-NN struggle with the curse of dimensionality. A large part of ML is concerned with dealing with this problem and finding ways around it.

\medskip

Possible approaches are:
\begin{itemize}
\item Increasing the space coverage by gathering more observations (not always viable in practice!)
\item Reducing the number of dimensions before training (e.g. by using domain knowledge, PCA or feature selection)
\item Regularization
\end{itemize}


\end{vbframe}


\endlecture
\end{document}