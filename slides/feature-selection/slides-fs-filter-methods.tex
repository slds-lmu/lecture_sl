
\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-feature-sel}

\newcommand{\titlefigure}{figure_man/correlation_example.png}
\newcommand{\learninggoals}{
  \item Understand how filter methods work and how to apply them for feature selection.
  \item Know filter methods based on correlation, test statistics, and mutual information.
}
\title{Supervised Learning}
\date{}
\begin{document}

  \lecturechapter{Filter Methods}
  \lecture{Supervised Learning}

  \begin{vbframe}{Introduction}
  \vspace{0.4cm}
  \begin{itemize}
  \setlength{\itemsep}{1.5em}
    \item \textbf{Filter methods} construct a measure that quantifies the dependency between all features and the target variable.
    \item They yield a numerical score for each feature $x_j$, according to which we rank the features.
    \item They are model-agnostic and can be applied generically.
    \item Filter methods are strongly related to methods for determining variable importance.
  \end{itemize}
  \end{vbframe}



  \begin{vbframe}{$\chi^2$-statistic}
  \begin{itemize}
    \item Test for independence between categorical $x_j$ and cat. target $y$. Numeric features or targets can be discretized.
    \item Hypotheses: \\
    $H_0: p(x_j = m, y = k) = p(x_j = m)\, p(y = k) \,\forall m, k$\\
    %$\forall m = 1, \dots, M, \forall k = 1, \dots, K$\\%, $\forall m = 1, \dots, M$

    %\noindent\hspace*{6.55cm} $\forall k = 1, \dots, K$

    $H_1: \exists \; m, k: p(x_j = m, y = k) \neq p(x_j = m)\, p(y = k)$
    \item Calculate the $\chi^2$-statistic for each feature-target combination:
      $$ \chi_j^2 = \sum_{m = 1}^{M} \sum_{k=1}^{K} (\frac{e_{mk} - \tilde{e}_{mk}}{\tilde{e}_{mk}}) \;\;\;   \stackrel{H_0}{\underset{approx.}{\sim}} \; \chi^2 ((M-1)(K-1))\,,$$
    where $e_{mk}$ is the observed relative frequency of pair $(m,k)$, $\tilde{e}_{mk} = \frac{e_{m \cdot} e_{\cdot k}}{n}$ is the expected relative frequency, and $M,K$ are the number of values $x_j$ and $y$ can take.
    \item The greater $\chi_j^2$, the more dependent is the feature-target combination $\rightarrow$ higher relevancy.
  \end{itemize}
  \end{vbframe}


  \begin{vbframe}{Pearson \& Spearman correlation}
  \textbf{Pearson correlation $r(x_j, y)$: }
  \begin{itemize}
    \item For numeric features and targets only.
    \item Most sensitive for linear or monotonic relationships.
    \item $ r(x_j, y) = \frac{\sum_{i=1}^n (x^{(i)}_j - \bar{x}_j) (\yi - \bar{y})}{\sqrt{\sum_{i=1}^n (x^{(i)}_j - \bar{x}_j)} \sqrt{(\sum_{i=1}^n \yi - \bar{y})}},\qquad -1 \le r \le 1$
  \end{itemize}
  \vspace{0.4cm}
  \textbf{Spearman correlation $r_{SP}(x_j, y)$:}
  \begin{itemize}
    \item For features and targets at least on an ordinal scale.
    \item Equivalent to Pearson correlation computed on the ranks.
    \item Assesses monotonicity of the dependency relationship.
    % \item Calculate the Spearman correlation coefficient between each feature-target combination:
    % $$ r_{SP} = \frac{\sum (rg(\xi) - \bar{rg}_x) (rg(\yi) - \bar{rg}_y)}{\sqrt{\sum (rg(\xi) - \bar{rg}_x)^2 \sum (rg(\yi) - \bar{rg}_y)^2}}$$
    % $-1 \le r_{SP} \le 1$
    % 
    % $r_{SP} > 0$: positive correlation.
    % 
    % $r_{SP} < 0$: negative correlation.
    % 
    % $r_{SP} = 0$: no correlation.
    % \item A higher score indicates a higher relevance of the feature.
  \end{itemize}
  \lz
  Use absolute values $|r(x_j, y)|$ for feature ranking: higher score indicates a higher relevance.

  \framebreak

  Only \textbf{linear} dependency structure, non-linear (non-monotonic) aspects are not captured:

  \lz

  % in rsrc/chunk2_filter_correlation.R created
  \begin{center}
\includegraphics[width=0.75\textwidth]{figure_man/correlation_example.png}\\
\footnotesize{Comparison of Pearson correlation for different dependency structures.}
  \end{center}
  %Comparison of Pearson correlation for different dependency structures.\\
  \vspace{0.1cm}
  To assess strength of non-linear/non-monotonic dependencies, generalizations such as \textbf{distance correlation} can be used.

  % \begin{center}
  %   \includegraphics[width=0.9\textwidth]{figure_man/correlation_example.png}
  % 
  %   \scriptsize{\url{https://en.wikipedia.org/wiki/Pearson\_correlation\_coefficient\#/media/File:Correlation\_examples2.svg}}
  % \end{center}
  % 
  % \begin{vbframe}{Filter: Rank correlation}
  % \begin{itemize}
  %   \item For features and targets at least on ordinal scale.
  %   \item Equivalent to Pearson correlation computed on the ranks.
  %   \item Assesses monotonicity of the dependency relationship.
    % \item Calculate the Spearman correlation coefficient between each feature-target combination:
    % $$ r_{SP} = \frac{\sum (rg(\xi) - \bar{rg}_x) (rg(\yi) - \bar{rg}_y)}{\sqrt{\sum (rg(\xi) - \bar{rg}_x)^2 \sum (rg(\yi) - \bar{rg}_y)^2}}$$
    % $-1 \le r_{SP} \le 1$
    % 
    % $r_{SP} > 0$: positive correlation.
    % 
    % $r_{SP} < 0$: negative correlation.
    % 
    % $r_{SP} = 0$: no correlation.
    % \item A higher score indicates higher relevance of the feature.
  %   \end{itemize}
  % \end{vbframe}
  \end{vbframe}

\begin{comment}
  \begin{vbframe}{Distance correlation}
  % sources: 
  %Szekely, G.J., Rizzo, M.L., and Bakirov, N.K. (2007), Measuring and Testing Dependence by Correlation of Distances, Annals of Statistics, Vol. 35 No. 6, pp. 2769-2794. 
  % http://dx.doi.org/10.1214/009053607000000505
  %Szekely, G.J. and Rizzo, M.L. (2009), Brownian Distance Covariance, Annals of Applied Statistics, Vol. 3, No. 4, 1236-1265. 
  % http://dx.doi.org/10.1214/09-AOAS312 


  $$
    r_D(x_j, y) = \sqrt{\frac{c_{D}^2(x_j, y)}{\sqrt{c_{D}^2(x_j, x_j) c_{D}^2(y, y)}}}
  $$
  Normed version of \textbf{distance covariance}: 
  $$c_{D}(x_j, y) = \frac{1}{n^2}\sum^n_{i=1}\sum^n_{k=1} D^{(ik)}_{x_j} D^{(ik)}_{y}$$
  $$ D^{(ik)}_{x_j} = d\left(x^{(i)}_j, x^{(k)}_j\right) - (\bar{d}^{(i\cdot)}_{x_j} + \bar{d}^{(\cdot k)}_{x_j} - \bar{d}^{(\cdot \cdot)}_{x_j}) $$
  \begin{itemize}
  \item $D^{(ik)}_{x_j}$ are the centered pairwise distances.
  \item $d\left(x^{(i)}_j, x^{(k)}_j\right)$ represents the distances of observations.
  \item $\bar{d}^{(i\cdot)}_{x_j} = \tfrac{1}{n} \sum^n_{k=1} d\left(x^{(i)}_j, x^{(k)}_j\right)$ represent the mean distances.


  \end{itemize}

  \framebreak

  \begin{itemize}
    \setlength{\itemsep}{1.6em}
  \item $0 \leq r_D(x_j, y) \leq 1 \quad \forall  j \in \{1, …, p\}$
  \item $r_D(x_j, y) = 0$ only if $\xv$ and $y$ are empirically independent (!)
  \item $r_D(x_j, y) = 1$ for exact linear dependencies
  \item Assesses strength of \textbf{non-monotonic}, \textbf{non-linear}  dependencies
  \item Generally applicable, even for ranking multivariate features or non-tabular inputs (text, images, audio, etc.)
  \item Expensive to compute for large data.
  \end{itemize}

  \begin{figure}
  \includegraphics[width = 0.85\textwidth]{figure_man/distance-corre.png}
  \end{figure}
  {Comparison of Pearson, Spearman and distance correlation for different dependency structures.}


  \end{vbframe}
\end{comment}



  \begin{vbframe}{Welch's \MakeLowercase{t}-test}
  \begin{itemize}
    \item For binary classification with $\Yspace = \{ 0, 1\}$ and numeric features
    \item Two-sample t-test for samples with unequal variances.
    %The subscript $j_0$ refers to the $j$-th feature where $y = 0$ and $j_1$ where $y = 1$.
   \item Hypotheses:
    $H_0$: $\;\;\mu_{j_0} = \mu_{j_1} $ \qquad vs. \qquad $H_1$: $\;\;\mu_{j_0} \neq \mu_{j_1}$

    \item Calculate Welch's t-statistic for every feature $x_j$
    $$ t_j = \frac{\bar{x}_{j_0} - \bar{x}_{j_1}}{\sqrt{(\frac{S^2_{x_{j_0}}}{n_0} + \frac{S^2_{x_{j_1}}}{n_1})}}$$
    ($\bar{x}_{j_y}$, $S^2_{x_{j_{y}}}$ and $n_y$ are the sample mean, %pop 
    variance and sample size)
    \item Higher t-score indicates higher relevance 
  \end{itemize}
  \end{vbframe}

  \begin{vbframe}{AUC/ROC}
  \begin{itemize}
    \item For binary classification with $\Yspace = \{ 0, 1\}$ and numeric features
    \item Classify samples using each feature's values (for different thresholds), computing AUC per feature as proxy for its ability to separate classes. 
    %\item Uses each feature's values to classify samples for different thresholds, computing AUC per feature as proxy for its ability to separate classes.
    \item Features are then ranked; higher AUC scores $\to$ higher relevance.
    \end{itemize}
  %Text explaining the AUC feature selection method here (calculate ROC curve thresholded on each individual feature, larger the better,...)
   \begin{center}
   \begin{figure}
     \includegraphics[width=0.7\textwidth]{figure/fs-roc-curve.png}
   \end{figure}

   %\footnotesize{Isabelle Guyon, André Elisseeff (2003). An Introduction to Variable and Feature Selection.  Journal of Machine Learning Research (3) p. 1157-1182.}
   \end{center}
   \end{vbframe}


  \begin{vbframe}{F-Test}
  \begin{itemize}
    \item For multiclass classification ($g \ge 2$) and numeric features.
    \item Assesses whether the expected values of a feature $x_j$ within the classes of the target differ from each other.
    \item Hypotheses:

    $H_0: \mu_{j_0} = \mu_{j_1} = \dots = \mu_{j_g} \;\;\;\;$ vs. $\;\;\;\;H_1 : \exists \; k,l: \mu_{j_k} \neq \mu_{j_l}$
    \item Calculate the F-statistic for each feature-target combination:
    \begin{align*}
    F &= \frac{\text{between-group variability}}{\text{within-group variability}}\\
    F &= \frac{\sum_{k = 1}^g n_k (\bar{x}_{j_k} - \bar{x_j})^2/(g-1)}{\sum_{k = 1}^g \sum_{i = 1}^{n_k} (x_{j_k}^{(i)} - \bar{x}_{j_k})^2/(n-g)}
    \end{align*}
    where $\bar{x}_{j_k}$ is the sample mean of feature $x_j$ where $y = k$ and $\bar{x_{j}}$ is the overall sample mean of feature $x_j$.
  \item A higher F-score indicates higher relevance of the feature.
  \end{itemize}
  \end{vbframe}

  \begin{vbframe}{Mutual Information (MI)}
  $$I(X ; Y) = \E_{p(x, y)} \left[ \log \frac{p(X, Y)}{p(X) p(Y)} \right]$$

  \begin{itemize}
  \setlength{\itemsep}{1.2em}
    \item Each feature $x_j$ is rated according to $I(x_j;y)$; this is sometimes called information gain (IG).
    \item MI measures the amount of "dependence" between random variables by looking how
    different their joint distribution is from strict independence $p(X)p(Y)$.
    \item MI is zero, iff $X \perp \!\!\! \perp Y$. On the other hand, if $X$ is a deterministic function of $Y$ or vice versa, MI becomes maximal.
  \item Unlike correlation, MI is defined for both numeric and categorical variables and provides a more general measure of dependence.
  \end{itemize}
  \end{vbframe}

  \endlecture
\end{document}

