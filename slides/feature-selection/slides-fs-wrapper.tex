
\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Feature Selection
  }{% Lecture title  
    Feature Selection: Wrapper methods
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/varsel_space.png
  }{
  \item Understand how wrapper methods work
  \item Forward + backward search, EAs
  \item Advantages and disadvantages
}

\begin{vbframe}{Introduction}

    \begin{itemize}
      \item Wrapper methods emerge from the idea that different sets of features can be optimal for different learners
      %\item Use the learner itself to assess the quality of the feature sets
      %\item Evaluation on a test set or resampling techniques are used
      \item Wrapper is a discrete search strategy for $S$, where objective criterion is test error of learner as function of $S$. Criterion can also be calculated on train set, approximating test error (AIC, BIC)
    \end{itemize}
    $\Rightarrow$ Use the learner to assess the quality of the feature sets
\vspace{-0.1cm}
    \begin{center}
     \includegraphics[width = 0.3\textwidth]{figure/searchspace_binary.png}\\
     \scriptsize{Hasse diagram illustrating search space. Knots are connected if Hamming distance = 1 \\(Source: Wikipedia)}
    \end{center}


\end{vbframe}

\begin{comment}

\begin{vbframe}{Introduction}

    Wrappers have the following components:


    \begin{itemize}
      \item A set of starting values
      \item Operators to create new points out of the given ones
      \item A termination criterion
    \end{itemize}


    \begin{figure}
      \includegraphics[width=8cm]{figure_man/varsel_space.png}
      % \caption{Space of all feature sets for 4 features.
      % The indicated relationships between the sets insinuate a greedy search strategy which either adds or removes a feature.}
      % Übersetzung von:
      % Raum aller Feature-Mengen bei 4 Features. Die eingezeichnete Nachbarschaftsbeziehung unterstellt eine Art ,,gierige'' Suchstrategie, bei der wir entweder ein Feature hinzufügen oder entfernen.}
    \end{figure}


  \end{vbframe}
\end{comment}

  \begin{vbframe}{Objective function}

    %\small

    Given $p$ features, \textbf{best-subset selection problem} is to find subset $S \subseteq \{ 1, \dots p \}$ optimizing objective $\Psi: \Omega \rightarrow \R$:
    \vspace{-0.15cm}
    %measuring the learner's generalization performance. The solution $S^*$ to the problem is
    $$S^{*}  \in \argmin_{{S \in \Omega}} \{ \Psi(S) \}$$
    %
    \vspace{-0.8cm}
    \begin{itemize}
    \setlength{\itemsep}{0.8em}
     \item $\Omega$  = search space of all feature subsets $S\subseteq\{ 1, \dots, p \}$. Usually we encode this by
      %(i.e., $\Omega \subseteq \mathcal{P}(\{ 1, \dots, p \})$, $\mathcal{P}$ denoting the power set)
      bit vectors, i.e., $\Omega = \{0, 1\}^p$ (1 = feat. selected)
      %It will be clear from the context which variant we refer to.
     \item Objective $\Psi$ can be different functions, e.g., AIC/BIC for LM or cross-validated performance of a learner
     \item Poses a discrete combinatorial optimization problem over search space of size = $2^p$, i.e., grows exponentially in $p$ (power set)%as it is the power set of $\{1,\ldots,p\}$
      %also known as $L_0$ regularization.
     \item Unfortunately can not be solved efficiently in general (NP hard; see, e.g., \citelink{NATARAJAN1995SPARSE})
     \item Can avoid searching entire space by employing efficient search strategies, traversing search space in a ``smart" way %that finds performant feature subsets

    \end{itemize}

  \end{vbframe}


\begin{comment}
 \begin{vbframe}{How difficult is best-subset selection?}

    \begin{itemize}
    \setlength{\itemsep}{1em}
      \item Size of search space = $2^p$, i.e., grows exponentially in $p$ as it is the power set of $\{1,\ldots,p\}$
      \item Finding best subset is discrete combinatorial optimization problem.
      %also known as $L_0$ regularization.
     \item It can be shown that this problem unfortunately can not be solved efficiently in general (NP hard; see, e.g., \citelink{NATARAJAN1995SPARSE})
     \item We can avoid having to search the entire space by employing efficient search strategies, moving through the search space in a smart way that finds performant feature subsets
     %\item By employing efficient search strategories, we can avoid searching the entire space.
    %\item Of course this does not mean that we have to search the entire space, since there are more efficient search strategies.
    %  \item Formally spoken: One can show that the problem is NP-hard!
    %  \item This means that the problem cannot be solved in polynomial (P) time: ${\mathcal{O}} (p^c)$, where $c \in \N$ indicates the degree o the polynom.
    % \end{blocki}
    %
    % \framebreak
    %
    % \begin{blocki}{How difficult is it to solve the introduced optimization problem, hence, to find the optimal feature set?}
    %  \item More precisely, the proof demonstrates that this problem cannot be approximated within any constant, unless P = NP.
    %
    %   The latter means, that if you find an algorithm that solves a more difficult class of problems (than this optimization problem) in polynomial time, this implies that you found how to solve all easier problems (including our optimization problem) in polynomial time.
    % \item \textbf{Attention}: This does not imply that it is useless trying to construct strategies which work in practice!
    %\item Thus our problem now consists of moving through the search space in a smart and efficient way, thereby finding a particularly good set of features.
    \end{itemize}
  \end{vbframe}

\begin{frame}{Greedy forward search}

    \begin{blocki}{}
      \item Let $S \subset \{1, \dots, p \}$, where $\{1, \dots p \}$ are feature indices
      \item Start with the empty feature set $S = \emptyset$
      \item For a given set $S$, generate all $S_j = S \cup \{j\}$ with $j \notin S$.
      \item Evaluate the classifier on all $S_j$ and use the best $S_j$
      \item Iterate over this procedure
      \item Terminate if:
        \begin{enumerate}
          \item the performance measure doesn't improve enough
          \item a maximum number of features is used
          \item a given performance value is reached
        \end{enumerate}
    \end{blocki}

    \end{frame}
\end{comment}

\begin{frame}{Greedy forward search}
Let $S \subset \{1, \dots, p \}$ be subset of feature indices.
\vspace{-0.01cm}
    \begin{enumerate}
      %\item Let $S \subset \{1, \dots, p \}$, where $\{1, \dots p \}$ are feature indices
      \item Start with the empty feature set $S = \emptyset$
      \item For a given set $S$, generate all $S_j = S \cup \{j\}$ with $j \notin S$.
      \item Evaluate the classifier on all $S_j$ and use the best $S_j$
      \end{enumerate}
    %\vspace{-0.2cm}
    \textbf{Example} GFS on a subset of bike sharing data with features windspeed, temp., humidity and feeling temp. Node value is RMSE.
    \begin{center}
    \includegraphics[width = 0.45\textwidth]{figure/fs-wrappers-powerset-tree-1.png}
    \end{center}

\end{frame}
    %\framebreak

\begin{frame}[noframenumbering]{Visualization of GFS}
\begin{enumerate}
    \setcounter{enumi}{3}
    \item Iterate over this procedure
\end{enumerate}
    \begin{center}
      \includegraphics[width = 0.65\textwidth]{figure/fs-wrappers-powerset-tree-2.png}
      \end{center}
      %\framebreak
\end{frame}

\begin{frame}[noframenumbering]{Visualization of GFS}
\begin{enumerate}
    \setcounter{enumi}{3}
    \item Iterate over this procedure
\end{enumerate}

    \begin{center}
      \includegraphics[width = 0.65\textwidth]{figure/fs-wrappers-powerset-tree-3.png}
    \end{center}
\end{frame}

\begin{frame}[noframenumbering]{Visualization of GFS}
    \begin{center}
      \includegraphics[width = 0.6\textwidth]{figure/fs-wrappers-powerset-tree-4.png}
      \end{center}
      \vspace{-0.2cm}
 \begin{enumerate}
     \setcounter{enumi}{4}
     \item Terminate if performance does not improve further or max. number of features is used
 \end{enumerate}
\end{frame}

%\begin{frame}[noframenumbering]{Visualization of GFS}
%    \begin{center}
%    \includegraphics[width = 0.65\textwidth]{figure/fs-wrappers-powerset-all-4.png}
%    \end{center}
%  \end{frame}


\begin{vbframe}{Greedy backward search}

    \begin{blocki}{}
      \item Start with the full index set of features $S = \{1, \ldots, p\}$.
      \item For a given set $S$ generate all

      $S_j = S \setminus\{j\}$ with $j \in S$.
      \item Evaluate the classifier on all $S_j$\
        and use the best $S_j$.
      \item Iterate over this procedure.
      \item Terminate if:
        \begin{itemize}
          \item the performance drops drastically, or
          \item falls below given threshold.
        \end{itemize}
      \end{blocki}

      \begin{itemize}
          \item GFS is much faster and generates sparser feature selections
          \item GBS much more costly and slower, but sometimes slightly better.
      \end{itemize}

  \end{vbframe}

  \begin{frame}{Visualization of GBS}
    %\begin{enumerate}
    %\setcounter{enumi}{3}
    %\item Iterate over this procedure
    %\end{enumerate}
    \textbf{Example} Greedy Backward Search on bike sharing data
    \begin{center}
      \includegraphics[width = 0.65\textwidth]{figure/fs-wrappers-backwards-powerset-tree-4.png}
      \end{center}
      %\framebreak
\end{frame}

  \begin{vbframe}{Extensions}
    \vspace{-0.1cm}
    \begin{itemize}
      \setlength{\itemsep}{1.2em}
      \item Eliminate or add multiple features at once to increase speed
      \item Allow alternating forward and backward search (also known as stepwise model selection by AIC/BIC in statistics)
      \item Randomly sample candidate feature subsets in each iteration
      \item Focus search on regions of feature subsets where an improvement is more likely
      %\item Use improvements of earlier iterations.
      %\item Alternatively, we can also use a $(\mu+\lambda)$-Evolutionary Strategy to perform FS, applying random recombination and mutation operations
    \end{itemize}
\end{vbframe}

\begin{vbframe}{Extensions: Genetic Algorithms for FS}
    \textbf{Example}\, Template for $(\mu+\lambda)$-Evolutionary Strategy applied to FS
\begin{enumerate}
    %\item Start with a random subset $S$ represented as a bit vector.
    %\item In each iteration, randomly flip part of bits to obtain new subset $S'$
    %\item Evaluate performance on $S'$
    %\item If $S'$ is superior, update $S\leftarrow S'$, otherwise keep $S$
    %\item Use stopping criteria such as performance threshold or compute/time budget
    \item Initialization: $\mu$ random bit vectors (feature inclusion/exclusion)
    \item Evaluate model performance for bit vectors
    \item Select $\mu$ fittest bit vectors (parents)
    \item Generate $\lambda$ offspring applying crossover and mutation
    \item Select $\mu$ fittest bit vectors from $(\mu+\lambda)$ options for next generation
    \item Repeat steps 2-5 until stopping criterion is met
    %\item Result: Highest fitness bit vector = best feature subset
    %\item Use CV or validation set for eval. to avoid overfitting.
\end{enumerate}
%\vspace{-0.01cm}
\begin{columns}[c]
    \begin{column}{0.5\textwidth}
            \hspace{1cm}
            \includegraphics[width = 0.75\textwidth]{figure/genetic-alg.png}
        %\hspace{1cm}
        \citelink{MITSUO1996GENETIC}
    \end{column}
    %
    \begin{column}{0.5\textwidth}
        \begin{itemize}
        \setlength{\itemsep}{0.8em}
            \item Use CV/validation set for evaluation to avoid overfitting
            \item Choice of $\mu$ and $\lambda$ allows some control over exploration vs. exploitation trade-off
            \item See our \citelink{OPTIMIZATIONLECTURE} for further information
        \end{itemize}
    \end{column}
\end{columns}
%\begin{center}
%    \includegraphics[width = 0.35\textwidth]{figure/genetic-alg.png}
%\end{center}

\end{vbframe}

\begin{comment}
    \begin{algorithm}[H]
    \begin{algorithmic}[1]
      \State Start with a random set of features $S$ (bit vector $b$).
      \Repeat
      \State Flip a couple of bits in $b$ with probability $p$
      \State Generate set $S^\prime$ and bit vector $b^\prime$
      \State Measure the classifier's performance on $S^\prime$
      \State If $S^\prime$ performs better than $S$, update $S \leftarrow S^\prime$, otherwise $S \leftarrow S$.
      \Until One of the following conditions is met:
        \begin{itemize}
          \item A given performance value is reached.
          \item Budget is exhausted.
        \end{itemize}
        \caption{A simple 1+1 genetic algorithm}
    \end{algorithmic}
    \end{algorithm}
\end{comment}

%\framebreak
%\end{vbframe}
%\begin{center}
%\begin{minipage}{0.48\textwidth}
%    \includegraphics[width=\textwidth]{figure/var-selection1.png}
%\end{minipage}\hfill
%\begin{minipage}{0.48\textwidth}
%    \includegraphics[width=\textwidth]{figure/var-selection2.png}
%\end{minipage}
%\end{center}
%Bit representation of the best variables included up to iteration $t$

% this is in the optim slides regarding featsel using EA
\begin{comment}
\textbf{Aim:} Use a $(\mu + \lambda)$ selection strategy for feature selection.\\
\vspace*{0.2cm}
Our iterative algorithm with $100$ iterations is as follows:
\begin{enumerate}
\item Initialize the population and evaluate it. Therefore, encode a chromosome of an individual as a bit string of length $p$, i.e. $\textbf{z} \in \{0, 1\}^p$. Where $z_j =1$ means that variable $j$ is included in the model.
\item Apply the variation and evaluate the fitness function. As fitness function, select BIC of the model belonging to the corresponding variable configuration $\textbf{z} \in \{0, 1\}^p$.
\item Finally, use $(\mu + \lambda)$-selection strategy as the survival selection with population size of $\mu = 100$ and $\lambda =50$ offspring.
\end{enumerate}

In addition:

\begin{itemize}
\item for the mutation, use bit flip with $p = 0.3$
\item for the recombination, use Uniform crossover with $p=0.5$
\end{itemize}

\lz

By exploiting \textbf{Greedy} as a selection strategy, ensure that you always choose individuals with the best fitness.
\end{comment}



\begin{vbframe}{Extensions: Genetic Algorithms for FS}
\vspace{-0.3cm}
        %\vspace{1.3cm}
        \begin{center}
        \includegraphics[width=0.53\textwidth]{figure/var-selection1.png}
            %\hspace{0.2cm} \newline
        \includegraphics[width=0.53\textwidth]{figure/var-selection2.png}
     \end{center}
    \vspace{-0.5cm}
\textbf{Top}: BIC over number of iterations.\\
\textbf{Bottom}: Bit representation of selected features over iterations.
\end{vbframe}


    %\framebreak

    %\vspace{0.9cm}
    %\begin{center}
    %\begin{figure}
    %    \includegraphics[height = 0.7\textheight]{figure/var-selection2.png}
    %\end{figure}
    %Bit representation of the best variables included up to iteration $t$
    %\end{center}


  \begin{vbframe}{Wrappers}

    \begin{blocki}{Advantages:}
      \item Can be combined with any learner
      \item Any performance measure can be used
      \item Optimizes the desired criterion directly
    \end{blocki}

    \lz

    \begin{blocki}{Disadvantages:}
      \item Evaluating target function is expensive
      \item Does not scale well with number of features%if number of features becomes large
      \item Does not use additional info about model structure
      \item Nested resampling becomes necessary
    \end{blocki}

  \end{vbframe}

    % \framebreak
    %
    % <<size="tiny", echo=TRUE>>=
    % # specify the search strategy.
    % # We want to use forward search:
    % ctrl = makeFeatSelControlSequential(method = "sfs")
    % ctrl
    %
    % # Selected features
    % sfeats = selectFeatures(learner = "regr.lm", task = bh.task,
    %   resampling = rdesc, control = ctrl, show.info = FALSE)
    % sfeats
    % @
    %
    % \framebreak
    %
    % <<size="tiny", echo=TRUE>>=
    % # Visualize optimization path
    % analyzeFeatSelResult(sfeats)
    % @
    %
    % \framebreak
    %
    % <<size="tiny", echo=TRUE>>=
    % # Fuse a base-learner with a search strategy (here: sfs)
    % lrn = makeFeatSelWrapper("classif.rpart", resampling = rdesc,
    %   control = ctrl, show.info = FALSE)
    % res = resample(lrn, iris.task, resampling = rdesc,
    %   show.info = FALSE, models = TRUE, extract = getFeatSelResult)
    % res$extract[1:5]
    % @

  \endlecture
\end{document}

