\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Feature Selection
  }{% Lecture title  
    Feature Selection: Introduction
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/feature_sel_vs_extr.png
  }{
  \item Too many features can be harmful in prediction
  \item Selection vs. extraction
  \item Types of selection methods
}

\begin{vbframe}{Introduction}

    Feature selection: \\
    Finding a well-performing, 
    hopefully small set of features for a task.
    
    %\begin{center}
    %\includegraphics{figure_man/varsel_overview.png}
    %\end{center}

    \lz
    Feature selection is critical for 
    \begin{itemize}
       \item reducing noise and overfitting
       \item improving performance/generalization
       \item enhancing interpretability by identifying most informative features
    \end{itemize}

 \vspace{0.5cm}
   

    Features can be selected based on domain knowledge, or data-driven algorithmic approaches. We focus on the latter here.
  \end{vbframe}


%  \begin{vbframe}{Overview}
%    \begin{center}
%    \includegraphics{figure_man/varsel_overview.png}
%    \end{center}
%
%    \lz
%
%    It is the task of statisticians, data analysts and machine learners to filter out the relevant information which is \textbf{useful} for prediction!
%  \end{vbframe}


  \begin{vbframe}{Motivation}
    \begin{itemize}
    \setlength{\itemsep}{0.8em}
      \item Naive view:
        \begin{itemize}
          \item More features $\rightarrow$ more information $\rightarrow$ discriminant power $\uparrow$
          \item Model is not harmed by irrelevant features since their parameters can simply be estimated as 0.
        \end{itemize}
     %\item In practice there are many reasons why this is not the case!
     \item In practice, irrelevant and redundant features can \enquote{confuse} learners (see \textbf{curse of dimensionality}) and worsen performance.
     \item Example: In linear regression, $R^2$ is monotonically increasing in $p$, but adding irrelevant features leads to overfitting (capturing noise). %instead of underlying relationship.
  \end{itemize}

  \begin{center}
     \includegraphics[width = 0.5\textwidth]{figure/avoid_overfitting_02.png}\\
    \end{center}

  \framebreak

    \begin{itemize}
    \setlength{\itemsep}{1.0em}
      \item In high-dimensional data sets, we often have prior information that many features are either irrelevant %or redundant 
      or of low quality
      \item Having redundant features can cost something during prediction %cost something 
      (money or time)
      %\item Training data are limited.
      %\item Computational resources are limited.
      \item Many models require $n > p$ data. Thus, we either need to
      %\item Thus, we either need
      \begin{itemize}
        \item adapt models to high-dimensional data (e.g., regularization)
        \item design entirely new procedures for $p>n$ data
        \item use filter preprocessing methods from this lecture
      \end{itemize}
    \end{itemize}
  \end{vbframe}

  \begin{vbframe}{Size of datasets}
Many new forms of technical measurements and connected data leads to availability of extremely high-dimensional data sets.

\vspace{0.5cm}
    \begin{itemize}
    \setlength{\itemsep}{1.2em}
      \item \textbf{Classical setting}: Up to around $10^2$ features, feature selection might be relevant, but benefits often negligible.
      \item \textbf{Datasets of medium to high dimensionality}:
        At around $10^2$ to $10^3$ features, classical approaches can still work well, while principled feature selection helps in many cases.
      \item \textbf{High-dimensional data}: $10^3$ to $10^9$ or more features.
        Examples: micro-array / gene expression data and text categorization (bag-of-words features).
        If we also have few observations, scenario is called $p \gg n$.
    \end{itemize}

  \end{vbframe}

  \begin{vbframe}{Feature selection vs. extraction}

    \begin{columns}
      %Both graphs taken out from Tim Conrad's presentation for Novisad (see cim2/external_material/tim_conrad_novisad)

      \column{0.49\textwidth}
      %\textbf{Feature selection}

      \medskip

      \includegraphics{figure_man/feature_selection.png}

      \smallskip

      \begin{itemize}
        \item Creates a subset of original features $\xv$ by selecting $\tilde{p} < p$ features $\bm{f}$.
        %\item Selected features are subset of $\xv$.
        \item Retains information on selected individual features.
      \end{itemize}

      \column{0.49\textwidth}
      %\textbf{Feature extraction}

      \medskip

      \includegraphics{figure_man/feature_extraction.png}

      \smallskip

      \begin{itemize}
        \item Maps $p$ features in $\xv$ to $\tilde{p}$ extracted features $\bm{f}$.
        %\item Forms linear or nonlinear combinations of the original features.
        \item Info on individual features can be lost through (non-)linear combination.
      \end{itemize}

    \end{columns}

    % \vspace{0.3cm}
    % 
    % {\tiny{Source: Hsiao-Yun Huang. Regularized Double Nearest Neighbor Feature Extraction for Hyperspectral Image Classification \code{\url{https://dokumen.tips/documents/regularized-double-nearest-neighbor-feature-extraction-for-hyperspectral-image-5694e4566f1f3.html}}}\par}

    \framebreak

    
    %\framebreak
    \begin{itemize}
    \footnotesize
        \item Both FS and FE contribute to\\ 1) dimensionality reduction and 2) simplicity of models
        \item FE can be unsupervised (PCA, multidim scaling, manifold learning) or supervised (supervised PCA, partial least squares)        
        \item FE can produce lower dim projections which can work better than FS; whether FE+model is interpretable depends on how interpretable extracted features are
    \end{itemize}

    \vspace{0.2cm}


    \begin{center}
     %feature_sel_vs_extr.R
     \includegraphics[width = 0.7\textwidth]{figure_man/feature_sel_vs_extr.png}\\
    \end{center}
    %\vspace{-0.2cm}
    %\scriptsize{Projection onto $x_1$ axis (feature selection) yields overlapping mixture components, while projection onto the hyperplane perpendicular to the first principal component (feature extraction) separates them.}
    %\normalsize


  \end{vbframe}

  \begin{vbframe}{Types of feature selection methods}
 
  In rest of the chapter, we introduce different types of methods for FS:

  \begin{itemize}
    \item Filters: evaluate relevance of features using statistical properties such as correlation with target variable
    \item Wrappers: use a model to evaluate subsets of features
    \item Embedded methods: integrate FS directly into specific model - we look at them in their dedicated chapters (e.g., CART, $L_0$, $L_1$)
  \end{itemize}

      \textbf{Example: embedded method (Lasso)} regularizing model params with $L1$ penalty %in the empirical risk 
      enables ``automatic" feature selection:
      \vspace{-0.28cm}
      $ \riskrt = \risket + \lambda \|\thetab\|_1 = \sumin \left(\yi - \thetab^\top \xi \right)^2 +\lambda \sum_{j=1}^p |\theta_j| $
      %are very popular for high-dimensional data.
      %\item The penalty shrinks the coefficients towards 0 in the final model.
      %\item Many (improved) variants: group LASSO, adaptive LASSO, ElasticNet, ...
      %\item Has some very nice optimality results: e.g., compressed sensing.
\vspace{0.1cm}
  \begin{center}
  \includegraphics[width=0.65\textwidth]{figure/regu_example_lasso_ridge.png}
  %\footnotesize{Lasso vs ridge regularization.}
  \end{center}


  \end{vbframe}



  \endlecture
\end{document}