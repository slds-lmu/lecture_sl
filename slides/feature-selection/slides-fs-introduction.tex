\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/feature_sel_vs_extr.png}
\newcommand{\learninggoals}{
  \item Too many features can be harmful in prediction
  \item Selection vs. extraction
  \item Types of selection methods
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Feature Selection: Introduction}
 \lecture{Introduction to Machine Learning}

  \begin{vbframe}{Introduction}

    Feature selection: Finding a well-performing, 
    hopefully small set of features for a task.
    
    %\begin{center}
    %\includegraphics{figure_man/varsel_overview.png}
    %\end{center}

    \lz
    Feature selection is critical for 
    \begin{itemize}
       \item reducing noise and overfitting
       \item improving performance/generalization
       \item enhancing interpretability by identifying most informative features
    \end{itemize}

 \vspace{0.5cm}
   

    Features can be selected based on domain knowledge, or data-driven algorithmic approaches.
  \end{vbframe}


%  \begin{vbframe}{Overview}
%    \begin{center}
%    \includegraphics{figure_man/varsel_overview.png}
%    \end{center}
%
%    \lz
%
%    It is the task of statisticians, data analysts and machine learners to filter out the relevant information which is \textbf{useful} for prediction!
%  \end{vbframe}


  \begin{vbframe}{Motivation}
    \begin{itemize}
    \setlength{\itemsep}{0.8em}
      \item Naive view:
        \begin{itemize}
          \item More features $\rightarrow$ more information $\rightarrow$ discriminant power $\uparrow$
          \item Model is not harmed by irrelevant features since their parameters can simply be estimated as 0.
        \end{itemize}
     %\item In practice there are many reasons why this is not the case!
     \item In practice, irrelevant and redundant features can \enquote{confuse} learners (see \textbf{curse of dimensionality}) and worsen performance.
     \item Example: In linear regression, $R^2$ is monotonically increasing in $p$, but adding irrelevant features leads to overfitting (capturing noise). %instead of underlying relationship.
  \end{itemize}

  \begin{center}
     \includegraphics[width = 0.5\textwidth]{figure/avoid_overfitting_02.png}\\
    \end{center}

  \framebreak

    \begin{itemize}
    \setlength{\itemsep}{1.0em}
      \item In high-dimensional data sets, we often have prior information that many features are either irrelevant %or redundant 
      or of low quality
      \item Having redundant features can cost something during prediction %cost something 
      (money or time)
      %\item Training data are limited.
      %\item Computational resources are limited.
      \item Many models require $n > p$ data. Thus, we either need to
      %\item Thus, we either need
      \begin{itemize}
        \item adapt models to high-dimensional data (e.g., regularization)
        \item design entirely new procedures for $p>n$ data
        \item use the preprocessing methods addressed in this lecture
      \end{itemize}
    \end{itemize}
  \end{vbframe}

  \begin{vbframe}{Size of datasets}
Increasing availability of measuring methods, everything connected to everything via networks makes data sets with extremely high dimensionality available.

\vspace{0.5cm}
    \begin{itemize}
    \setlength{\itemsep}{1.2em}
      \item \textbf{Classical setting}: Up to around $10^2$ features, feature selection might be relevant, but benefits often negligible.
      \item \textbf{Datasets of medium to high dimensionality}:
        At around $10^2$ to $10^3$ features, classical approaches can still work well, while principled feature selection helps in many cases.
      \item \textbf{High-dimensional data}: $10^3$ to $10^9$ or more features.
        Examples are, e.g., micro-array / gene expression data and text categorization (bag-of-words features).
        If, in addition, observations are few, the scenario is called $p \gg n$.
    \end{itemize}

  \end{vbframe}

  \begin{vbframe}{Feature selection vs. extraction}

    \begin{columns}
      %Both graphs taken out from Tim Conrad's presentation for Novisad (see cim2/external_material/tim_conrad_novisad)

      \column{0.49\textwidth}
      %\textbf{Feature selection}

      \medskip

      \includegraphics{figure_man/feature_selection.png}

      \smallskip

      \begin{itemize}
        \item Creates a subset of original features $\xv$ by selecting $\tilde{p} < p$ features $\bm{f}$.
        %\item Selected features are subset of $\xv$.
        \item Retains information on selected individual features.
      \end{itemize}

      \column{0.49\textwidth}
      %\textbf{Feature extraction}

      \medskip

      \includegraphics{figure_man/feature_extraction.png}

      \smallskip

      \begin{itemize}
        \item Maps $p$ features in $\xv$ to $\tilde{p}$ extracted features $\bm{f}$.
        %\item Forms linear or nonlinear combinations of the original features.
        \item Info on individual features can be lost through (non-)linear combination.
      \end{itemize}

    \end{columns}

    % \vspace{0.3cm}
    % 
    % {\tiny{Source: Hsiao-Yun Huang. Regularized Double Nearest Neighbor Feature Extraction for Hyperspectral Image Classification \code{\url{https://dokumen.tips/documents/regularized-double-nearest-neighbor-feature-extraction-for-hyperspectral-image-5694e4566f1f3.html}}}\par}

    \framebreak

    
    %\framebreak
    \begin{itemize}
    \footnotesize
        \item Both FS and FE contribute to\\ 1) dimensionality reduction and 2) simplicity of models
        \item FE can be unsupervised (PCA, multidim scaling, manifold learning) or supervised (supervised PCA, partial least squares)        
        \item FE can produce lower dim projections which can work better than FS; whether FE+model is interpretable depends on how interpretable extracted features are
    \end{itemize}

    \vspace{0.2cm}


    \begin{center}
     %feature_sel_vs_extr.R
     \includegraphics[width = 0.7\textwidth]{figure_man/feature_sel_vs_extr.png}\\
    \end{center}
    %\vspace{-0.2cm}
    %\scriptsize{Projection onto $x_1$ axis (feature selection) yields overlapping mixture components, while projection onto the hyperplane perpendicular to the first principal component (feature extraction) separates them.}
    %\normalsize


  \end{vbframe}

  \begin{vbframe}{Types of feature selection methods}
 
  In rest of the chapter, we introduce different types of methods for FS:

  \begin{itemize}
    \item Filters: evaluate relevance of features using statistical properties such as correlation with target variable
    \item Wrappers: use a model to evaluate subsets of features
    \item Embedded methods: integrate FS directly into specific model - we look at them in their dedicated chapters (e.g., CART, $L_0$, $L_1$)
  \end{itemize}

      \textbf{Example: embedded method (Lasso)} regularizing model params with $L1$ penalty %in the empirical risk 
      enables ``automatic" feature selection:
      \vspace{-0.28cm}
      $ \riskrt = \risket + \lambda \|\thetab\|_1 = \sumin \left(\yi - \thetab^\top \xi \right)^2 +\lambda \sum_{j=1}^p |\theta_j| $
      %are very popular for high-dimensional data.
      %\item The penalty shrinks the coefficients towards 0 in the final model.
      %\item Many (improved) variants: group LASSO, adaptive LASSO, ElasticNet, ...
      %\item Has some very nice optimality results: e.g., compressed sensing.
\vspace{0.1cm}
  \begin{center}
  \includegraphics[width=0.65\textwidth]{figure/regu_example_lasso_ridge.png}
  %\footnotesize{Lasso vs ridge regularization.}
  \end{center}


  \end{vbframe}



  \endlecture
\end{document}