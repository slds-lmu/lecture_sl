\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Feature Selection
  }{% Lecture title  
    Feature Selection: Motivating Examples
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/tibshirani_fig_18_1_mod.png
  }{
  \item Understand the practical importance of feature selection
  \item Understand that models with integrated selection do not always work
  \item Know different categories of selection methods
}

\begin{vbframe}{Motivating example 1: Regularization}
  In case of $p \gg n$,  overfitting becomes increasingly problematic, as can be shown by the following simulation study:

  \begin{itemize}
  %\setlength{\itemsep}{1.15em}
    \item For each of 100 simulation iterations:
    \item Simulate 3 datasets with $n=100$, $p \in \{ 20, 100, 1000 \}$.
%    \item This can be demonstrated with a simulation study of three datasets of feature dimensionalities $p \in \{ 20, 100, 1000 \}$ and $n=100$ samples over 100 simulation runs.
    %\item Investigation of three different dimensionalities of the input space: $p \in \{ 20, 100, 1000 \}$.
    \item Features are drawn from a standard Gaussian with pairwise correlation $\rho=0.2$.
      \item Target is simulated as
    $ y = \sum_{j=1}^p x_j \theta_j + \sigma\varepsilon $, where $\varepsilon$ and $\bm{\theta}$ are both sampled from standard Gaussians, and $\sigma$ is fixed such that the signal-to-noise ratio is $\var (\E[y|X]) / \sigma^2 = 2$.
    \item Three ridge regression models with $\lambda \in \{ 0.001, 100, 1000 \}$ are fitted to each simulated dataset.
  \end{itemize}

  \framebreak

  \begin{itemize}
  \small
    \item Boxplots show the relative test error ($\text{RTE} = \text{test error} / \text{Bayes error }\sigma^2$) over 100 simulations for the different values of $p$ and $\lambda$.
    %\item Relative test error = test error / Bayes error $\sigma^2$.
    % $\text{Relative test error} = \text{test error} / \text{Bayes error }\sigma^2$
  \normalsize
  \end{itemize}
  \begin{center}
  %\vspace{0.1cm}
  \includegraphics[width = 0.8\textwidth]{figure_man/tibshirani_fig_18_1_mod.png}
  %\footnotesize{Hastie (2009). The Elements of Statistical Learning}
  \end{center}
  \vspace{-0.3cm}
  \begin{itemize}\small
  %\item On the x-axis the effective degrees of freedom (averaged over the 100 simulation runs) are displayed:
  %$$df(\lambda) = \sum_{j=1}^p \frac{d_j^2}{d^2_j + \lambda}$$
  %where $d$ are the singular values of $x$.
  %\begin{itemize}
  %  \item For $\lambda = 0$ (linear regression): $df(\lambda) = p$.
  %  \item For $\lambda \rightarrow \infty$: $df(\lambda) \rightarrow 0$
  %\end{itemize}
  \item Lowest RTE is obtained at $\lambda = 0.001$  for $p=20$, %df=20%
  at $\lambda = 100$ for $p=100$, %df=35%
  and at $\lambda = 100$ for $p=100$.%df=43%.
  %\item For $p=100$, $\lambda = 100$ has smallest relative test error. %df=35%
  %\item For $p=1000$, $\lambda = 1000$ has smallest relative test error. %df=43%
 \item Optimal amount of regularization increases monotonically in $p$ here.
  \item[$\Rightarrow$] High-dimensional settings require more complexity control through regularization or feature selection.
  %More regularization for high dimensional data seems reasonable.
  \end{itemize}
  \end{vbframe}

  \begin{vbframe}{Motivating ex. 2: Comparison of Methods}
  Generalization performance of eight classification methods on micro-array data with $| \Dtrain | = 144$, $| \Dtest | = 54$, $p=16, 063$ genes and a categorical target encoding the type of cancer with 14 classes.
\vspace{0.25cm}
  \begin{center}
  \includegraphics[width=0.7\textwidth]{figure_man/tibshirani_tab_18_1.png}

  \footnotesize{Hastie (2009). The Elements of Statistical Learning}
  \end{center}
  Methods need at least regularization or built-in FS to perform well. Possible to build good, small models which helps in interpretation
  \end{vbframe}

\begin{vbframe}{Motivating ex. 3: Integrated selection}
\vspace{-0.15cm}
\textbf{Set-up for simulated micro-array data:}
\vspace{-0.1cm}
  \begin{itemize}\small
  \setlength{\itemsep}{0.7em}
  %\item We simulate micro-array data with function \code{sim.data} of the package \pkg{penalizedSVM}.
  %\item With \code{sim.data} one can simulate micro-array data.
  \item We generate $n=200$ samples with $p=100$ features drawn from a MV Gaussian
  \item 50\% are relevant for the target and 50\% have no influence
  \item Among informative features, 25 are positively and 25 negatively correlated with target,  using weights $\{-1,1\}$
  \item Target is simulated from Bernoulli distribution using linear predictor as log-odds (linear decision boundary!)
  %\item The other $\code{ng-nsg}=50$ genes have no influence.
  %\item The gene ratios are drawn from a multivariate normal distribution.
  \end{itemize}
  \vspace{-0.35cm}

%chris.kolb: der plot hat keine lineare decision boundary f√ºr die simulierten Daten gezeigt (zu viele features damit man es in einem 2d plot sieht), siehe "gene-data-points.pdf" im figure ordner des chunks. Daher hat Holger jetzt selbst Daten simuliert die zwar jetzt andere sind, aber das Prinzip immerhin zeigen das damit gezeigt werden soll

  \begin{center} \includegraphics[width=0.39\textwidth]{figure/fs-micro-array.png}
  \end{center}

  \framebreak

  \begin{itemize}
  \setlength{\itemsep}{1.1em}
  \item We compare several classifiers regarding their misclassification rate, of which two have integrated FS (rpart and rForest).
  \item Since we have few observations, we use repeated 10-fold cross-validation with 10 repetitions.
  \end{itemize}
  \vspace{0.2cm}
  \begin{table}[ht]
    \begin{center}
      \begin{tabular}{rrrrrrr}
        \hline
        & rpart & lda & logreg & nBayes & knn7  & rForest \\
        \hline
        all feat. & 0.44 & 0.27 & 0.25 & 0.32 & 0.37 & 0.36 \\
        relevant feat. & 0.44 & 0.18 & 0.19 & 0.27 & 0.33 & 0.30 \\
        \hline
      \end{tabular}
    \end{center}
  \end{table}
\vspace{0.1cm}
\begin{itemize}
\setlength{\itemsep}{0.8em}
    \item[$\Rightarrow$] Different to Ex. 2, models with integrated FS do not work ideally here. Also, methods with lin. decision boundary are better due to our simulation set-up.
    \item[$\Rightarrow$]   Performance improves significantly for most methods when only trained on informative features.
\end{itemize}

\end{vbframe}

  \endlecture
\end{document}
