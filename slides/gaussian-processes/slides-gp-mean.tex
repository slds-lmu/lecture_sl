\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-gp}
\input{../../latex-math/ml-svm}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{
    Gaussian Processes
  }{
    Mean functions for GPs
  }{
  figure/gp_sample/2_4.pdf
  }{
  \item Trends can be modeled via specification of the mean function
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei}[sep=L]{non-zero-mean functions}
\item Previously: common assumption of zero-mean prior $$m(\xv) \equiv 0$$
\item Not necessarily drastic limitation: \textbf{posterior} mean can be $\neq 0$
$$\E(\fstar | \Xstar, \Xmat, \ystar) = \Kstar \Ky^{-1} \yv$$
% $$\fstar | \Xstar, \Xmat, \fv \sim \normal(\Kstar^{T}\Kmat^{-1}\fv, \Kstarstar - \Kstar^T \Kmat ^{-1}\Kstar)$$
\item Still: can make sense to explicitly model mean fun \\$\Rightarrow$ interpretability, prior knowledge, \dots
\item Assuming $\gpmk$, the posterior mean becomes
$$m(\Xstar) + \Kstar \Ky^{-1} (\yv - m(\Xmat))$$
(posterior variance unchanged)
\end{framei}

\begin{framei}[sep=L]{example: non-zero-mean prior}
\item GPs with non-zero-mean priors: aka GPs with \textbf{trend}
\vfill
\splitVTT{
\imageC[1]{figure/gp_sample/1_1.pdf}
}{
\imageC[1]{figure/gp_sample/2_1.pdf}
}
\vfill
\splitVTT{
\imageC[1]{figure/gp_sample/2_3.pdf}
}{
\imageC[1]{figure/gp_sample/2_4.pdf}
}
\end{framei}

\begin{framei}[sep=L]{basis function approach}
\item Often difficult to come up with fixed mean function
\item Possible solution: specify basis functions + infer coeffs from data
\item Aka semiparametric GP
\item E.g., consider $$g(\xv) = b(\xv)^T\bm{\beta} + \fx, \quad f  \sim \gpzk$$
$\Rightarrow$ intuition: data close to global LM with residuals modeled by GP
\item Estimation of $g(\xv)$: \furtherreading{RASMUSSENWILLIAMS2006GPML}
\end{framei}

\endlecture
\end{document}
