\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-gp}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{
    Gaussian Processes
  }{
    Mean functions for GPs
  }{
  figure/gp_sample/2_4.pdf
  }{
  \item Trends can be modeled via specification of the mean function
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei}[sep=L]{non-zero-mean functions}
\item Previously: common assumption of zero-mean prior $$m(\xv) \equiv 0$$
\item Not necessarily drastic limitation: \textbf{posterior} mean can be $\neq 0$
$$\fv_\ast | \Xmat_*, \Xmat, \fv \sim \normal(\Kmat_{*}^{T}\Kmat^{-1}\fv, \Kmat_{**} - \Kmat_*^T \Kmat ^{-1}\Kmat_*)$$
\item Still: can make sense to explicitly model mean fun \\$\Rightarrow$ interpretability, prior knowledge, \dots
\item Assuming $\mathcal{GP}(m(\cdot), k(\cdot, \cdot))$, the posterior mean becomes
$$m(\Xmat_*) + \Kmat_* \Kmat_y^{-1} (\yv - m(\Xmat))$$
(posterior variance unchanged)
\end{framei}

\begin{framei}[sep=L]{example: non-zero-mean prior}
\item GPs with non-zero-mean priors: aka GPs with \textbf{trend}
\vfill
\splitVTT{
\imageC[1]{figure/gp_sample/1_1.pdf}
}{
\imageC[1]{figure/gp_sample/2_1.pdf}
}
\vfill
\splitVTT{
\imageC[1]{figure/gp_sample/2_3.pdf}
}{
\imageC[1]{figure/gp_sample/2_4.pdf}
}
\end{framei}

\begin{framei}[sep=L]{basis function approach}
\item Often difficult to come up with fixed mean function
\item Possible solution: specify basis functions + infer coeffs from data
\item E.g., consider $$g(\xv) = b(\xv)^T\bm{\beta} + \fx, \quad f  \sim \mathcal{GP}(\zero, k(\cdot, \cdot))$$
$\Rightarrow$ intuition: data close to global LM with residuals modeled by GP
\item Estimation of $g(\xv)$: \furtherreading{RASMUSSENWILLIAMS2006GPML}
\end{framei}

\endlecture
\end{document}
