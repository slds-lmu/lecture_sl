\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-gp}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{
    Gaussian Processes
  }{
    Gaussian Posterior Process and Prediction
  }{
  figure/gp_pred/post_variance.pdf
  }{
  \item Know how to derive the posterior process
  \item GPs are interpolating and spatial models
  \item Model noise via a nugget term
}

\begin{framei}[sep=L]{gp prediction}
\item So far: sampling from GP priors
\item More interesting than drawing random functions from GP priors: predict at unseen test point $\xv_\ast$ with $f_\ast = f(\xv_\ast)$
\item Given: training data with design matrix $\Xmat$, observed values $\fv = f(\Xmat) = \fvec$
\item Goal: infer distribution of $f_\ast | \xv_\ast, \Xmat, \fv$ \\$\Rightarrow$ update prior to posterior process
\end{framei}

\begin{framei}[sep=M]{posterior process}
\item Again assuming $f \sim \mathcal{GP}(\zero, k(\cdot, \cdot))$, we get
$$\begin{bmatrix}
\fv \\ f_\ast
\end{bmatrix} \sim  
\normal \left(\zero, \begin{bmatrix} \Kmat & \bm{k}_* \\ \bm{k}_*^T & \bm{k}_{**}\end{bmatrix}\right)$$
with $\bm{k}_* = [k(\xv_*, \xi[1]), \dots, k(\xv_*, \xi[n])]$,  $ \bm{k}_{**}\ = k(\xv_*, \xv_*)$
\item General rule for condition of Gaussian RVs
\begin{itemize}
\item $\bm{z} \sim \normal(\mu, \Sigma)$, partition $\bm{z} = (\bm{z}_1, \bm{z}_2)$ s.t. $\bm{z}_1 \in \R^{m_1}, \bm{z}_2 \in \R^{m_2}$, $$\mu = (\mu_1, \mu_2), \quad \Sigma = \begin{bmatrix} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22} \end{bmatrix} $$
\item Conditional distribution $\bm{z}_2 ~|~ \bm{z}_1 = \bm{a}$ is MV normal 
$$\normal(\mu_2 + \Sigma_{21} \Sigma_{11}^{-1} (\bm{a} - \mu_1), \Sigma_{22} - \Sigma_{21}\Sigma_{11}^{-1} \Sigma_{12})$$
\end{itemize}
\item Apply to posterior process given $\fv$ observed $$f_* ~|~ \xv_*, \Xmat, \fv \sim \normal(\bm{k}_{*}^{T}\Kmat^{-1}\fv, \bm{k}_{**} - \bm{k}_*^T \Kmat ^{-1}\bm{k}_*)$$
\item \textbf{Maximum-a-posteriori (MAP)} estimate: $\bm{k}_{*}^{T}\Kmat^{-1}\fv$
\end{framei}

\begin{framei}[sep=L]{example: 2 points}
\item Single training point $\xv = -0.5$, test point $\xv_* = 0.5$
\item Zero-mean GP with $k(\xv, \tilde{\xv}) = \exp(-\tfrac{1}{2} \| \xv - \tilde{\xv} \|^2)$ leads to 
$$\begin{bmatrix} \fv \\ f_* \end{bmatrix} \sim \normal\left(\zero, \begin{bmatrix} 1 & 0.61 \\ 0.61 & 1\end{bmatrix}\right)$$
\item Assuming we observe $\fx = 1$, compute posterior distribution
\begin{eqnarray*}
    f_* ~|~ \xv_*, \xv, \fv &\sim& \normal(\bm{k}_{*}^{T}\Kmat^{-1} \fv, k_{**} - \bm{k}_*^T \Kmat^{-1}\bm{k}_*) \\
    &\sim& \normal(0.61 \cdot 1 \cdot 1, 1 - 0.61 \cdot 1 \cdot 0.61) \\
    &\sim& \normal\left(0.61, 0.63\right) 
  \end{eqnarray*}
\item MAP-estimate: $f(\xv_*) = 0.61$, uncertainty estimate: $0.63$
\end{framei}

\foreach \i in {1, 2, 3, 4, 5, 6} {
\begin{framei}{example: 2 points}
\ifnum \i<3
\item Bivariate normal density + marginals for joint distribution of $f, f_*$
\else \ifnum \i<6
\item Update posterior distribution, conditioning on observed value
\item Possible predictor at $\xv_*$: MAP estimate
\fi 
\ifnum \i=6
\item Posterior uncertainty ($\pm2$ posterior SD) in grey
\fi 
\fi 
\vfill
\imageC[.8]{figure/gp_pred/\i.pdf}
\end{framei}
}

\begin{framei}[sep=L]{multiple test points}
\item Now consider multiple test points
$$\fv_* = [f(\xi[1]_*), ..., f(\xi[m]_*)]$$
\item Joint distribution (under zero-mean GP) becomes
$$
    \begin{bmatrix} \fv \\ \fv_* \end{bmatrix} \sim  
    \normal\left(\zero, \begin{bmatrix} \Kmat & \Kmat_* \\ \Kmat_*^T & \Kmat_{**} \end{bmatrix}\right)
  $$
with $\Kmat_* = (k(\xi, \xv_*^{(j)}))_{i,j}$, $\Kmat_{**} = (k(\xi[i]_*, \xi[j]_*))_{i,j}$
\item Again, employ rule of conditioning for Gaussians to get \textbf{posterior}
$$\fv_* ~|~ \Xmat_*, \Xmat, \fv \sim \normal(\Kmat_{*}^{T}\Kmat^{-1}\fv, \Kmat_{**} - \Kmat_*^T \Kmat ^{-1}\Kmat_*)$$
\item Allows to compute correlations between test points + draw samples from posterior process
\end{framei}

\begin{framei}[sep=L]{GP as interpolator}
\item MAP ``prediction'' for training point is exact function value
\begin{eqnarray*}
\fv ~|~ \Xmat, \fv &\sim& \normal(\Kmat\Kmat^{-1}\fv, \Kmat - \Kmat^T \Kmat^{-1} \Kmat) \\ &\sim& \normal(\fv, \zero)
\end{eqnarray*}
\item GP is function \textbf{interpolator}
\vfill
\imageC[.8]{figure/gp_pred/gp_interpolator.pdf}
\end{framei}

\begin{framei}[sep=M]{gp as spatial model}
\item Recall spatial property: output correlation depends on distance in input space
\item E.g., squared exponential kernel $k(\xv, \tilde{\xv}) = \exp \left(-\frac{\| \xv - \tilde{\xv} \|^2}{2\ls^2} \right)$
\item Strongly correlated predictions for points with spatial proximity
\item High posterior uncertainty for far-away points (0 at training locs)
\vfill
\splitV{
\imageC[1]{figure/gp_pred/post_mean.pdf}
}{
\imageC[1]{figure/gp_pred/post_variance.pdf}
}
\end{framei}

% \begin{vbframe}{GP as a spatial model}

% \vspace*{-0.3cm}

% \begin{itemize}
%   \begin{footnotesize}
%   \item The correlation among two outputs depends on distance of  the corresponding input points  $\xv$ and $\xv^\prime$ (e.g. Gaussian covariance kernel $k(\xv, \xv^\prime) = \exp \left(\frac{- \|\xv - \xv^\prime\|^2}{2 l^2}\right)$ )
%   \item Hence, close data points with high spatial similarity $k(\xv, \xv^\prime)$ enter into more strongly correlated predictions: $\bm{k}_*^\top \bm{K}^{-1} \fv$ ($\bm{k}_* := \left(k(\xv, \xv^{(1)}), ..., k(\xv, \xv^{(n)})\right)$).
%   \end{footnotesize}  


% \begin{center}
% \includegraphics[width=0.5\textwidth]{figure/gp_pred/post_mean.pdf}
% \end{center}


% \begin{footnotesize}
% Example: Posterior mean of a GP that was fitted with the Gaussian covariance kernel with $l = 1$. 
% \end{footnotesize}


% \framebreak 

% \item Posterior uncertainty increases if the new data points are far from the design points.
% \item The uncertainty is minimal at the design points, since the posterior variance is zero at these points.
% \end{itemize}


% \begin{center}
% \includegraphics[width=0.5\textwidth]{figure/gp_pred/post_variance.pdf}
% \end{center}

% \begin{footnotesize}
% Example (continued): Posterior variance. 
% \end{footnotesize}


% \end{vbframe}


% \section{Noisy Gaussian Process}

\begin{vbframe}{Noisy Gaussian Process}

\begin{itemize}
  \item So far, we implicitly assumed that we had access to the true function value $\fx$.
  \item For the squared exponential kernel, for example, we have
  $$
    \cov\left(f(\xi), f(\xi)\right) = 1.
  $$
  \item As a result, the posterior Gaussian process is an interpolator: 
  \begin{center}
    \includegraphics[width=0.8\textwidth]{figure/gp_pred/gp_interpolator.pdf}
  \end{center}

\framebreak 

  \item In reality, however, this is often not the case. 
  \item We often only have access to a noisy version of the true function value
  $$
    y = \fx + \eps, \eps \sim\normal\left(0, \sigma^2\right).
  $$
  \item Let us still assume that $\fx$ is a Gaussian process.
  \item Then,
  \begin{footnotesize} 
  \begin{eqnarray*}
    &&\cov(y^{(i)}, y^{(j)}) = \cov\left(f\left(\xi\right) + \epsilon^{(i)}, f\left(\xi[j]\right) + \epsilon^{(j)}\right) \\
    &=& \cov\left(f\left(\xi\right), f\left(\xi[j]\right)\right) + 2 \cdot \cov\left(f\left(\xi\right), \epsilon^{(j)}\right) + \cov\left(\epsilon^{(i)}, \epsilon^{(j)}\right) 
    \\ &=& k\left(\xi, \xi[j]\right) + \sigma^2 \delta_{ij}. 
  \end{eqnarray*}
  \end{footnotesize}
  \item $\sigma^2$ is called \textbf{nugget}. 
\end{itemize}

\framebreak 

\begin{itemize}
  \item Let us now derive the predictive distribution for the case of noisy observations. 
  \item The prior distribution of $y$, assuming that $f$ is modeled by a Gaussian process is then
  $$
    \bm{y} = \begin{pmatrix} \yi[1] \\ \yi[2] \\ \vdots \\ \yi[n] \end{pmatrix} \sim \normal\left(\bm{m}, \bm{K} + \sigma^2 \bm{I}_n \right),
  $$
  with 
  \begin{eqnarray*}
    \textbf{m} &:=& \left(m\left(\xi\right)\right)_{i}, \quad
    \textbf{K} := \left(k\left(\xi, \xv^{(j)}\right)\right)_{i,j}. 
  \end{eqnarray*}

  \framebreak 
  
  \item We distinguish again between 
  \begin{itemize}
    \item observed training points $\Xmat, \yv$, and 
    \item unobserved test inputs $\Xmat_*$ with unobserved values $\fv_*$
  \end{itemize} 
  and get
  $$
  \begin{bmatrix}
  \bm{y} \\
  \fv_*
  \end{bmatrix} \sim  
    \normal\biggl(\bm{0}, \begin{bmatrix} \Kmat + \sigma^2 \bm{I}_n & \Kmat_* \\ \Kmat_*^T & \Kmat_{**} \end{bmatrix}\biggr).
  $$

\framebreak

  \item Similarly to the noise-free case, we condition according to the rule of conditioning for Gaussians to get the posterior distribution for the test outputs $\fv_*$ at $\Xmat_*$: 

  \begin{eqnarray*}
    \fv_* ~|~ \Xmat_*, \Xmat, \bm{y} \sim \normal(\bm{m}_{\text{post}}, \bm{K}_\text{post}).
\end{eqnarray*}
  with 
  \begin{eqnarray*}
    \bm{m}_{\text{post}} &=& \Kmat_{*}^{T} \left(\Kmat+ \sigma^2 \cdot \id\right)^{-1}\bm{y} \\
    \bm{K}_\text{post} &=& \Kmat_{**} - \Kmat_*^T \left(\Kmat  + \sigma^2 \cdot \id\right)^{-1}\Kmat_*,
  \end{eqnarray*}
\item This converts back to the noise-free formula if $\sigma^2 = 0$.

\framebreak 

\item The noisy Gaussian process is not an interpolator any more.
\item A larger nugget term leads to a wider ``band'' around the observed training points.
\item The nugget term is estimated during training.


\begin{center}
    \includegraphics[width=0.8\textwidth]{figure/gp_pred/gp_regression.pdf}
\end{center}
\end{itemize}

\end{vbframe}



\section{Decision Theory for Gaussian Processes}

\begin{vbframe}{Risk Minimization for Gaussian Processes}

In machine learning, we learned about risk minimization. We usually choose a loss function and minimize the empirical risk  

$$
  \riske(f) := \sumin \Lxyi
$$
as an approximation to the theoretical risk

$$ 
  \riskf := \E_{xy} [\Lxy] = \int \Lxy \text{d}\Pxy. 
$$

\begin{itemize}
  \item How does the theory of Gaussian processes fit into this theory? 
  \item What if we want to make a prediction which is optimal w.r.t. a certain loss function? 
\end{itemize}

\framebreak 

\begin{itemize}
  \item The theory of Gaussian process gives us a posterior distribution 
  $$
    p(y ~|~\D)
  $$
  \item If we now want to make a prediction at a test point $\bm{x}_*$, we approximate the theoretical risk in a different way, by using the posterior distribution: 
  $$
    \mathcal{R}(y_* ~|~ \bm{x}_*) \approx \int L(\tilde y_*, y_*) p(\tilde y_*~|~\bm{x}_*, \D)d\tilde y_*. 
  $$
  \item The optimal prediciton w.r.t the loss function is then: 
  $$
    \hat y_* | \bm{x}_* = \argmin_{y_*} \mathcal{R}(y_*~|~ \bm{x}_*).
  $$
\end{itemize}


% In practical applications, we are often forced to make predictions. We need a point-like prediction that is \enquote{optimal} in some sense. 

% \lz

% We define \enquote{optimality} with respect to some loss function

% $$
% L(y_\text{true}, y_\text{guess}). 
% $$

% \vfill

% \begin{footnotesize}
% Notice that we computed the predictive distribution without reference to the loss function. In non-Bayesian paradigms, the model is typically trained by minimizing the empirical risk (or loss). In contrast, in the Bayesian setting there is a clear separation between the likelihood function (used for training in addition to the prior) and the loss function.
% \end{footnotesize}

% \framebreak 

% As we do not know the true value $y_\text{true}$ for our test input $\bm{x}_*$, we minimize w.r.t. to the expected loss called \textbf{risk} w.r.t. our model's opinion as to what the truth might be

% $$
% \mathcal{R}(y_\text{guess} | \bm{x}_*) = \int L(y_*, y_\text{guess}) p(y_*|\bm{x}_*, \D)dy_*. 
% $$

% Our best guess w.r.t. $L$ is then

% $$
% $$

% For quadratic loss $L(y, y^\prime) = (y - y^\prime)^2$ this corresponds to the posterior mean. 

\end{vbframe}


\endlecture
\end{document}