\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-gp}
\input{../../latex-math/ml-svm}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{
Gaussian Processes
}{
Covariance functions for GPs
}{
figure_man/covariance2D-2.png
}{
\item Covariance functions encode key assumptions about the GP
\item Common covariance functions like squared exponential and Matérn
}

\begin{framei}[sep=L]{valid covariance functions}
\item Recall marginalization property of GPs: for any $\Xsubset = \xdat \subset \Xspace$,
$$\fv = \fX \sim \Nmk$$
with $\mv = \mX$, $\Kmat = \kXX$
\item Cov. function (or kernel) determines cov / kernel / Gram matrix: $\Kmat = \kXX$
\item For $\Kmat$ to be a valid cov matrix it needs to be positive semi-definite (PSD) for any choice of inputs $\bm{X}$
\item Implication: only \textbf{PSD functions} (i.e., those that induce PSD $\Kmat$) are valid cov functions 

\item Also look at SVM chapter for background info on kernels, 
many further details in e.g. \furtherreading{DUVENAUDKERNEL}

\end{framei}

\begin{framei}[sep=L]{stationary covariance functions}
\item Recall concept of spatial correlation
$$\xv, \xtil \text{ close in } \Xspace \Rightarrow \fx, f(\xtil) \text{ close / more correlated in } \Yspace$$
\item Measure ``closeness'' via $\bm{d} = \xv - \xtil$ 
\item $\kcc$ called \textbf{stationary} $\Leftrightarrow$ function of $\bm{d}$ 
$$\kxxt = k(\bm{d})$$
\item Intuition: stationary $\kcc$ implies functions that do not depend on where the points lie in input space but only their difference 

\end{framei}

\begin{framei}{example: stationary covariance}
\item Let $f \sim \GPmk$ with $\kxxt = \exp(-\tfrac{1}{2}\|\bm{d}\|^2)$
\item Consider two points $\xi[1] = 3$ and $\xi[2] = 2.5$
\item If you want to know about the correlation between $f(\xi[1])$ and $f(\xi[2])$, compute $\bm{d}(\xi[1], \xi[2])$
\vfill
\splitV{
\imageC[.9]{figure/cov_funs/example_covariance_1.pdf}
}{
\imageC[.9]{figure/cov_funs/example_function_1_1.pdf}
}
\end{framei}

\begin{framei}{example: stationary covariance}
\item Suppose we observe $\yi[1] = -0.8$
\item $\xi[1], \xi[2]$ are close in $\Xspace$ space
\item Under the above GP assumption, $\yi[2]$ should be close to $\yi[1]$ 
\vfill
\splitV{
\imageC[.9]{figure/cov_funs/example_covariance_1.pdf}
}{
\imageC[.9]{figure/cov_funs/example_function_1_2.pdf}
}
\end{framei}

\begin{framei}{example: stationary covariance}
\item Consider now $\xi[3] = 5$
\item Further from $\xi[1]$ $\Rightarrow$ expect lower correlation between $\yi[3]$, $\yi[1]$ 
\vfill
\splitV{
\imageC[.9]{figure/cov_funs/example_covariance_2.pdf}
}{
\imageC[.9]{figure/cov_funs/example_function_2_1.pdf}
}
\end{framei}

\begin{framei}[sep=L]{properties of covariance functions}
\item Most cov. functions have 3 common properties
\item \textbf{Stationary}: $k = k(\bm{d})$ with $\bm{d} = \xv - \xtil$ \\
$\Rightarrow$ invariant to translations in $\Xspace$: $k(\xv, \xv + \bm{d}) = k(\zero, \bm{d})$\\
(so we sometimes abuse notation and write $k(\bm{d})$)
\item \textbf{Isotropic}: $k = k(\bm{r})$ with $r = \xxtnorm$ \\
$\Rightarrow$ invariant to rotations\\
(again slight notational abuse)
\item Isotropy implies stationarity
\item \textbf{Dot product}: $k = k(\xv^T \xtil)$
\end{framei}

\begin{framei}{constant kernel}
\item $\kxxt = \theta_0 \quad > 0$
\item Constant function priors
\item Global correlation irresp. of concrete inputs $\xv, \xtil$
\item Practically pretty useless
\vfill
\imageC[1]{figure/cov_funs/cov_constant.pdf}
\end{framei}

\begin{framei}{linear kernel}
\item $\kxxt = \theta_0 + \xv^T \xtil$
\item Linear function priors
\item Measures directional similarity: higher if vectors point in similar dirs
\item In general, non-stationary $\Rightarrow$ depends on locations of $\xv, \xtil$
\item See Bayesian linear model part
\vfill
\imageC[1]{figure/cov_funs/cov_linear.pdf}
\end{framei}

\begin{framei}{polynomial kernel}
\item $\kxxt = (\theta_0 + \xv^T \xtil )^p, \quad p \in \N$
\item Polynomial function priors
\item Allows for non-linearity through higher-order monomials \& interaction terms
\vfill
\imageC[1]{figure/cov_funs/cov_polynomial.pdf}
\end{framei}

\begin{framei}{periodic kernel}
\item E.g., radial periodic kernel: $\kxxt = \exp \left(\frac{-2\sin^2 (\pi \xxtnorm / m)}{\ls^2} \right)$
\item $m$: period, $\ls$: length-scale
\item $f(\xv)$ should be periodically similar to points with a distance which is a multiple of $m$;\\
for distances in between, this is modulated by $\ls$
\item Alternative: Product of 1D periodic kernels, with $m_j$ period in dimension $j$: $\kxxt =  \exp \left(\sum_j\frac{-2\sin^2 (\pi |x_j - \tilde{x}_j| / m_j)}{\ls^2} \right)$
\vfill
\imageC[1]{figure/cov_funs/cov_periodic.pdf}
\end{framei}

\begin{framei}{matérn kernel}
\item $\kxxt = \frac{1}{2^\nu \Gamma(\nu)} \left( \sqrt{2 \nu} \frac{\xxtnorm}{\ls} \right)^\nu K_\nu \left( \sqrt{2 \nu} \frac{\xxtnorm}{\ls} \right)$
\item $\nu$: smoothness param, $\Gamma$: gamma function, $\ls$: length scale, $K_\nu$: modified Bessel function
\item Stationary \& isotropic
\item Allows for controlled degree of smoothness via choice of $\nu$
\item $\nu$ also determines differentiability
\item Use for: non-linear functions with desired degree of smoothness
\vfill
\imageC[1]{figure/cov_funs/cov_matern.pdf}
\end{framei}

\begin{framei}{exponential kernel}
\item Aka Ornstein-Uhlenbeck kernel
\item $\kxxt = \exp \left(-\frac{\xxtnorm}{\ls} \right)$
\item Special case of Matérn kernel with $\nu = 0.5$
\item Non-smooth: continuous but not differentiable, can model functions with abrupt variations
\item Cov decays exponentially with distance (modulated by $\ls$)
\imageC[1]{figure/cov_funs/cov_exponential.pdf}
\end{framei}

\begin{framei}{squared exponential kernel}
\item Aka Gaussian kernel, RBF kernel
\item $\kxxt = \exp \left(-\frac{\xxtnorm^2}{2\ls^2} \right)$
\item Special case of Matérn kernel with $\nu = \infty$
\item Very smooth: continuous, $\infty$ differentiable (not always realistic)
\item Cov decays quickly $\Rightarrow$ quadratic in distance
\imageC[1]{figure/cov_funs/cov_squaredexp.pdf}
\end{framei}

\begin{framei}[sep=S]{Example: Brownian motion}
\item $ \kxxt = \prod_j \min (x_j, \tilde{x}_j)$
\item Physics application: random fluctuations of particles
\item With non-1D inputs aka Brownian sheet
\item Correlation in each dimension is 1D-like Brownian motion
\vfill
\imageC[.7]{figure/cov_funs/brownian2d.png}
\end{framei}

\begin{framei}[sep=M]{characteristic LS : isotropic case}
\item Every (isotropic) kernel can be written as $k(r)$ where %Key (hyper)parameter of many (isotropic) kernels $k(r)$ 
\item $r=||\xv-\xtil||$
\item E.g. $\kxxt = \exp \left(-\frac{\xxtnorm^2}{2\ls^2} \right)$ or $k(r) = \exp \left(-\frac{1}{2}(\frac{r}{\ls})^2\right)$ 
\item Controls how quickly function values become uncorrelated
\item High (low) $\ls$: smooth (wiggly) functions
\vfill
\imageC[1]{figure/gp_sample/zeromean_prior_10n_varying_ls.pdf}
\item In SVM kernels we sometimes called this bandwith
\end{framei}

\begin{framei}[sep=M]{characteristic LS : stationary case}
\item For stationary kernels $k(\bm{d})$ 
\item We modulate every distance component $d_j$ by an individual $\ls_j$
\item We can turn the isotropic examples from above into 
stationary ones -- with individual length scales
\item Write $||\bm{d}|| = \sum d_j^2$ and put an $1/\ls_j$ before each $d_j$
%\item Write $(\frac{r}{\ls})^2 = \frac{1}{\ls^2}\sum d_j^2 = \sum \frac{d_j^2}{\ls^2} $ and use dim-specific $\ls_j$
\item E.g. for squared exp:
$$
  k(\bm{d}) = \exp \left(- \tfrac{1}{2} \sumjp \frac{d_j^2}{\ell_j^2} \right)
$$
\item Also note: this is a product of 1D kernels, one for each input dim, the correlation in each dim is described by the 1D kernel and in it distance component modulated by $\ls_j$
\end{framei}


\begin{framei}[sep=L]{benefits of dim-wise length-scales}
\item $\ls_1,\dots, \ls_p$: characteristic length-scales
\item Intuition for $\ls_i$: how far to move along $i$-th axis for fun. values to become uncorrelated?
\item Implements \textbf{automatic relevance determination} (ARD): inverse of $\ls_i$ determines importance of $i$-th feature
\item Very large $\ls_i$ $\Rightarrow$ cov effectively independent of $i$-th feature
\item For features on different scales: rescale automatically by estimating $\ls_1,\dots, \ls_p$ 
\end{framei}



\begin{framei}[sep=M]{characteristic LS : Weighted Euclid dist}

\item Can even generalize the above principle
\item Move to weighted (squared) Euclidean distance
\item E.g. for squared exp again: 
$$
\kxxt = \exp \left(- \tfrac{1}{2}(\xv - \xtil)^T\bm{M} (\xv - \xtil )\right)
$$
\item This covers the case before
\item Possible choices for $\bm{M}$:
$$
\bm{M}_1 = \ls^{-2}\id_p \qquad \bm{M}_2 = \diag(\bm{\ls})^{-2} \qquad \bm{M}_3 = \Gamma \Gamma^T + \diag(\bm{\ls})^{-2}
$$
where $\bm{\ls} \in \R^p_+$, $\Gamma \in \R^{p \times k}$ 
\end{framei}


\begin{framei}{examples: characteristic ls}



\item Left: $\bm{M} = \id$ $\Rightarrow$ same variation in all directions
\item Middle: $\bm{M} = \diag(\bm{\ls})^{-2}$ $\Rightarrow$ less variation in $x_2$ direction ($\ls_2 > \ls_1$)
\item Right: $\bm{M} = \Gamma \Gamma^T + \diag(\bm{\ls})^{-2}$ with $\Gamma = (1, -1)^T$ and $\bm{\ls} = (6, 6)^T$ $\Rightarrow$ $\Gamma$ determines dir. of most rapid variation
\vfill
\imageC[1]{figure_man/covariance2D.png}

\sourceref{RASMUSSENWILLIAMS2006GPML}
\end{framei}

\endlecture
\end{document}

