\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-gp}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{
Gaussian Processes
}{
Covariance functions for GPs
}{
figure_man/covariance2D-2.png
}{
\item Covariance functions encode key assumptions about the GP
\item Know common covariance functions like squared exponential and Matérn
}

\begin{framei}[sep=L]{valid covariance functions}
\item Recall marginalization property of GPs: for any $\bm{X} = \xdat \subset \Xspace$,
$$\fv = f(\bm{X}) \sim \gaussmk$$
with $\mv = m(\bm{X})$, $\Kmat = k(\bm{X}, \bm{X})$
\item For $\Kmat$ to be a valid cov matrix it needs to be positive semi-definite (PSD) for any choice of inputs $\bm{X}$
\item Cov function (or kernel) determines cov matrix: $\Kmat = k(\bm{X}, \bm{X})$
\item Implication: only \textbf{PSD functions} (i.e., those that induce PSD $\Kmat$) are valid cov functions
\end{framei}

\begin{framei}[sep=L]{stationary covariance functions}
\item Recall concept of spatial correlation
$$\xv, \tilde{\xv} \text{ close in } \Xspace \Rightarrow f(\xv), f(\tilde{\xv}) \text{ close in } \Yspace$$
\item Measure ``closeness'' via $\bm{d} = \xv - \tilde{\xv}$
\item $k(\cdot, \cdot)$ called \textbf{stationary} $\Leftrightarrow$ function of $\bm{d}$ 
$$k(\xv, \tilde{\xv}) = k(\bm{d})$$
\item Intuition: stationary $k(\cdot, \cdot)$ implies functions that vary smoothly regardless of where we are in input space
\end{framei}

\begin{framei}{example: stationary covariance}
\item Let $f \sim \mathcal{GP}(m(\cdot), k(\cdot, \cdot)$ with $k(\xv, \tilde{\xv}) = \exp(-\tfrac{1}{2}\|\bm{d}\|^2)$
\item Consider two points $\xi[1] = 3$ and $\xi[2] = 2.5$
\item If you want to know about the correlation between $f(\xi[1])$ and $f(\xi[2])$, compute $\bm{d}(\xi[1], \xi[2])$
\vfill
\splitV{
\imageC[.9]{figure/cov_funs/example_covariance_1.pdf}
}{
\imageC[1]{figure/cov_funs/example_function_1_1.pdf}
}
\end{framei}

\begin{framei}{example: stationary covariance}
\item Suppose we observe $\yi[1] = -0.8$
\item $\xi[1], \xi[2]$ are close in $\Xspace$ space
\item Under the above GP assumption, $\yi[2]$ should be close to $\yi[1]$ 
\vfill
\splitV{
\imageC[.9]{figure/cov_funs/example_covariance_1.pdf}
}{
\imageC[1]{figure/cov_funs/example_function_1_2.pdf}
}
\end{framei}

\begin{framei}{example: stationary covariance}
\item Consider now $\xi[3] = 5$
\item Further from $\xi[1]$ $\Rightarrow$ expect lower correlation between $\yi[3]$, $\yi[1]$ 
\vfill
\splitV{
\imageC[.9]{figure/cov_funs/example_covariance_2.pdf}
}{
\imageC[1]{figure/cov_funs/example_function_2_1.pdf}
}
\end{framei}

\begin{framei}[sep=L]{properties of covariance functions}
\item Most cov functions belong to 3 common types
\item \textbf{Stationary}: $k = k(\bm{d})$ with $\bm{d} = \xv - \tilde{\xv}$ \\
$\Rightarrow$ invariant to translations in $\Xspace$: $k(\xv, \xv + \bm{d}) = k(\zero, \bm{d})$
\item \textbf{Isotropic}: $k = k(\bm{r})$ with $r = \| \xv - \tilde{\xv} \|$ \\
$\Rightarrow$ invariant to rotations of $\Xspace$
\item Isotropy implies stationarity
\item \textbf{Dot product}: $k = k(\xv^T \tilde{\xv})$
\end{framei}

\begin{framei}{constant kernel}
\item $k(\xv, \tilde{\xv}) = \sigma^2_0 \quad > 0$
\item Constant function priors
\item Global correlation irresp of concrete inputs $\xv, \tilde{\xv}$
\item Use for: global-effect models
\imageC[.9]{figure/cov_funs/cov_constant.pdf}
\end{framei}

\begin{framei}{linear kernel}
\item $k(\xv, \tilde{\xv}) = \sigma^2_0 + \xv^T \tilde{\xv}$
\item Linear function priors
\item Measures directional similarity: higher if vectors point in similar dirs
\item In general, non-stationary $\Rightarrow$ depends on absolute location of $\xv, \tilde{\xv}$
\item Use for: linear models
\imageC[.9]{figure/cov_funs/cov_linear.pdf}
\end{framei}

\begin{framei}{polynomial kernel}
\item $k(\xv, \tilde{\xv}) = (\sigma^2_0 + \xv^T \tilde{\xv} )^p, \quad p \in \N$
\item Polynomial function priors
\item Allows for non-linearity through higher-order monomials \& interaction terms
\item Use for: polynomial trends
\imageC[.9]{figure/cov_funs/cov_polynomial.pdf}
\end{framei}

\begin{framei}{periodic kernel}
\item E.g., $k(\xv, \tilde{\xv}) = \exp \left(\frac{-2\sin^2 (\pi \| \xv - \tilde{\xv}\| / m)}{\ls^2} \right)$
\item $m$: period, $\ls$: length-scale
\item Implements idea that $\xv$ should be similar to $\xv + m$, similarity decaying quadratically in $\ls$
\item Use for: periodic trends
\imageC[.9]{figure/cov_funs/cov_periodic.pdf}
\end{framei}

\begin{framei}{matérn kernel}
\item $k(\xv, \tilde{\xv}) = \frac{1}{2^\nu \Gamma(\nu)} \left( \sqrt{2 \nu} \frac{\| \xv - \tilde{\xv} \|}{\ls} \right)^\nu K_\nu \left( \sqrt{2 \nu} \frac{\| \xv - \tilde{\xv} \|}{\ls} \right)$
\item $\nu$: smoothness param, $\Gamma$: gamma function, $\ls$: length scale, $K_\nu$: modified Bessel function
\item Stationary \& isotropic
\item Allows for controlled degree of smoothness via choice of $\nu$
\item $\nu$ also determines differentiability
\item Use for: non-linear trend with desired degree of smoothness
\imageC[.9]{figure/cov_funs/cov_matern.pdf}
\end{framei}

\begin{framei}{exponential kernel}
\item Aka Ornstein-Uhlenbeck kernel
\item $k(\xv, \tilde{\xv}) = \exp \left(-\frac{\| \xv - \tilde{\xv} \|}{\ls} \right)$
\item Special case of Matérn kernel with $\nu = 0.5$
\item Non-smooth: continuous but not differentiable
\item Cov decays exponentially with distance (dep on $\ls$)
\item Use for: non-linear trend with abrupt variations
\imageC[.9]{figure/cov_funs/cov_exponential.pdf}
\end{framei}

\begin{framei}{squared exponential kernel}
\item Aka Gaussian kernel, RBF kernel
\item $k(\xv, \tilde{\xv}) = \exp \left(-\frac{\| \xv - \tilde{\xv} \|^2}{2\ls^2} \right)$
\item Special case of Matérn kernel with $\nu = \infty$
\item Very smooth: continuous, $\infty$ differentiable (not always realistic)
\item Cov decays quickly $\Rightarrow$ quadratic in $\ls$
\item Use for: smooth, non-linear trend
\imageC[.9]{figure/cov_funs/cov_squaredexp.pdf}
\end{framei}

\begin{framei}[sep=L]{characteristic length-scale}
\item Controls how quickly function values become uncorrelated
\item High (low) $\ls$: smooth (wiggly) functions
\vfill
\imageC[.8]{figure/gp_sample/varying_length_scale.pdf}
\item Akin to SVM bandwidth but more general: length-scales may vary across input dims
\end{framei}

\begin{framei}[sep=L]{choices for characteristic ls}
\item Reparameterize squared exponential kernel (for $p \geq 2$ dims) as
$$
k(\xv, \tilde{\xv}) = \exp \left(- \tfrac{1}{2}(\xv - \tilde{\xv})^T\bm{M} (\xv - \tilde{\xv} )\right)
$$
\item Possible choices for $\bm{M}$:
$$
\bm{M}_1 = \ls^{-2}\id_p \qquad \bm{M}_2 = \diag(\bm{\ls})^{-2} \qquad \bm{M}_3 = \Gamma \Gamma^T + \diag(\bm{\ls})^{-2}
$$
where $\bm{\ls} \in \R^p_+$, $\Gamma \in \R^{p \times k}$ 
\item (Most important) case $\bm{M}_2$ can also be written as
$$
  k(\bm{d}) = \exp \left(- \tfrac{1}{2} \sumjp \frac{d_j^2}{\ell_j^2} \right)
$$
\end{framei}

\begin{framei}[sep=L]{benefits of dim-wise length-scales}
\item $\ls_1,\dots, \ls_p$: characteristic length-scales
\item Intuition for $\ls_i$: how far to move along $i$-th axis for fun values to become uncorrelated?
\item Implements \textbf{automatic relevance determination} (ARD): inverse of $\ls_i$ determines importance of $i$-th feature
\item Very large $\ls_i$ $\Rightarrow$ cov effectively independent of $i$-th feature
\item For features are on different scales: rescale automatically by estimating $\ls_1,\dots, \ls_p$ 
\end{framei}

\begin{framei}{examples: characteristic ls}
\item Left: $\bm{M} = \id$ $\Rightarrow$ same variation in all directions
\item Middle: $\bm{M} = \diag(\bm{\ls})^{-2}$ $\Rightarrow$ less variation in $x_2$ direction ($\ls_2 > \ls_1$)
\item Right: $\bm{M} = \Gamma \Gamma^T + \diag(\bm{\ls})^{-2}$ with $\Gamma = (1, -1)^T$ and $\bm{\ls} = (6, 6)^T$ $\Rightarrow$ $\Gamma$ determines dir of most rapid variation
\vfill
\imageC[1]{figure_man/covariance2D.png}
\item Img source: \furtherreading{RASMUSSENWILLIAMS2006GPML}
\end{framei}

\endlecture
\end{document}

