\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-gp}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{
Gaussian Processes
}{
Stochastic Processes and Distributions on Functions
}{
figure_man/discrete/marginalization-more.png
}{
\item GPs model distributions over functions 
\item The marginalization property makes this distribution easily tractable
\item GPs are fully specified by mean and covariance function 
\item GPs are indexed families
}

\begin{framei}[sep=L]{Weight-Space View}
\item Until now: hypothesis space $\Hspace$ of parameterized functions $\fxt$ % (in particular, the space of linear functions)
\item Bayesian paradigm: distribution over parameters (weights) $\thetav$
\item Bayes' rule: update prior to posterior belief after observing data
$$
p(\thetav | \Xmat, \yv) 
= \frac{\text{likelihood} \cdot \text{prior}}{\text{marginal likelihood}} 
= \frac{p(\yv | \Xmat, \thetav) \cdot q(\thetav)}{p(\yv|\Xmat)}
$$
\end{framei}

\begin{framei}[sep=L]{Function-Space View}
\item New POV: rather than finding $\thetav$ to parameterize $\fxt$ in parameter space,  search space of admissible functions directly
\item Sticking to Bayesian inference, specify prior distribution \textbf{over functions} and update according to observed data points
\end{framei}

\begin{framei}{drawing from function priors}
\item Imagine we could draw functions from some prior distribution

\vfill

\imageC[.8]{figure/gp_sample/1_1.pdf}
\end{framei}

\foreach \i in{1,2,3} {
\begin{framei}{drawing from function priors}
\addtocounter{page}{0}
\item After observing some data points, we restrict sampling to functions consistent with the data
\vfill
\imageC[.8]{figure/gp_sample/2_\i.pdf}
\end{framei}
}

\begin{framei}{drawing from function priors}
\item Variety of admissible functions shrinks with observing more data
\vfill
\imageC[.8]{figure/gp_sample/2_4.pdf}
\item Intuitively: distributions over functions have ``mean'' \& ``variance''
\end{framei}

\begin{frame}{Weight-space vs. Function-space View}
\splitVTT{
\textbf{Weight-Space View}
\spacer
\begin{itemizeL}
\item Parameterize functions \\ (e.g., $\fxt = \thx$)
\item Define distributions on $\thetav$
\item Inference in param space $\Theta$
\end{itemizeL}
}{
\textbf{Function-Space View}
\spacer
\begin{itemizeL}
\item Work on functions directly \phantom{(e.g., $\fxt = \thx$)}
\item Define distributions on $f$
\item Inference in function space $\Hspace$
\end{itemizeL}
}
\end{frame}

% --------------------------------------------------

\section{Distributions on Functions}

\begin{framei}[sep=L]{discrete functions}
\item Let $\Xspace = \xdat$ be a finite set
\item Let $\Hspace = \{h ~|~ h: \Xspace \rightarrow \R\}$
\item Any $h \in \Hspace$ has finite domain with $n < \infty$ elements $\Rightarrow$ neat representation with $n$-dim vector 
$$\bm{h} = [h(\xi[1]), \dots, h(\xi[n] ) ]$$
\end{framei}

\foreach \i in{1,2,3}{
\begin{framei}{random discrete functions}
\item Case 1: input space $\Xspace = \setzo$
\item Example of functions living in this space
\vfill
\imageC[.7]{figure/discrete/example_2_\i.pdf}
\end{framei}
}

\foreach \i in{1,2,3}{
\begin{framei}{random discrete functions}
\item Case 2: input space $\Xspace = \{0, 0.25, 0.5, 0.75, 1\}$
\item Example of functions living in this space
\vfill
\imageC[.7]{figure/discrete/example_5_\i.pdf}
\end{framei}
}

\foreach \i in{1,2,3}{
\begin{framei}{random discrete functions}
\item Case 3: input space $ |\Xspace| = 10$
\item Example of functions living in this space
\vfill
\imageC[.7]{figure/discrete/example_10_\i.pdf}
\end{framei}
}

% \begin{vbframe}{Discrete Functions}

% For simplicity, let us consider functions with finite domains first. 

% \lz 


% Let $\mathcal{X} = \left\{\xv^{(1)}, \dots , \xv^{(n)}\right\}$ be a finite set of elements and $\Hspace$ the set of all functions from $\mathcal{X} \to \R$.

% \lz

% Since the domain of any $h(.) \in \Hspace$ has only $n$ elements, we can represent the function $h(.)$ compactly as a $n$-dimensional vector $$\bm{h} = \left[h\left(\xv^{(1)}\right), \dots, h\left(\xv^{(n)}\right)\right].$$
% \end{vbframe}


% \begin{frame}{Discrete Functions}

% \textbf{Example 1:} Let us consider $h: \Xspace \to \Yspace$ where the input space consists of \textbf{two} points $\Xspace = \{0, 1\}$. 

% \lz 

% Examples for functions that live in this space: 

% \begin{figure}[h]
% \foreach \x in{1,2,3} {
%   \includegraphics<\x>[width=0.7\linewidth]{figure/discrete/example_2_\x.pdf} \par
% }
% \end{figure}


% \end{frame}

% \begin{frame}{Discrete Functions}

% \textbf{Example 2:} Let us consider $h: \Xspace \to \Yspace$ where the input space consists of \textbf{five} points $\Xspace = \{0, 0.25, 0.5, 0.75, 1\}$.

% \lz 

% Examples for functions that live in this space: 

% \begin{figure}[h]
% \foreach \x in{1,2,3} {
%   \includegraphics<\x>[width=0.7\linewidth]{figure/discrete/example_5_\x.pdf}\par
% }
% \end{figure}

% \end{frame}


% \begin{frame}{Discrete Functions}

% \textbf{Example 3:} Let us consider $h: \Xspace \to \Yspace$ where the input space consists of \textbf{ten} points. 

% \lz 

% Examples for functions that live in this space: 

% \begin{figure}[h]
% \foreach \x in{1,2,3} {
%   \includegraphics<\x>[width=0.7\linewidth]{figure/discrete/example_10_\x.pdf}\par
% }
% \end{figure}

% \end{frame}


\begin{vbframe}{Distributions on Discrete Functions}

\vspace*{0.5cm}

One natural way to specify a probability function on discrete function $h \in \Hspace$ is to use the vector representation 
$$
  \bm{h} = \left[h\left(\xi[1]\right), h\left(\xi[2]\right), \dots, h\left(\xi[n]\right)\right]
$$ 


of the function.

\lz

Let us see $\bm{h}$ as a $n$-dimensional random variable. We will further assume the following normal distribution: 

$$
  \bm{h} \sim \mathcal{N}\left(\bm{m}, \bm{K}\right).
$$ 

\textbf{Note: } For now, we set $\bm{m} = \bm{0}$ and take the covariance matrix $\bm{K}$ as given. We will see later how they are chosen / estimated. 

\end{vbframe}

\begin{frame}{Discrete Functions}

\textbf{Example 1 (continued):} Let $h: \Xspace \to \Yspace$ be a function that is defined on \textbf{two} points $\Xspace$. We sample functions by sampling from a two-dimensional normal variable

$$
\bm{h} = [h(1), h(2)] \sim \mathcal{N}(\bm{m}, \bm{K})
$$


\begin{figure}[H]
\foreach \x in{1,2,3} {
  \includegraphics<\x>[width=0.4\linewidth]{figure/discrete/example_norm_2_\x_a.pdf} ~  \includegraphics<\x>[width=0.4\linewidth]{figure/discrete/example_norm_2_\x_b.pdf} 
} \par
\begin{footnotesize}
In this example, $m = (0, 0)$ and $K = \begin{pmatrix} 1 & 0.5 \\ 0.5 & 1 \end{pmatrix}$. 
\end{footnotesize}
\end{figure}

\end{frame}


\begin{frame}{Discrete Functions}

\textbf{Example 2 (continued):} Let us consider $h: \Xspace \to \Yspace$ where the input space consists of \textbf{five} points. We sample functions by sampling from a five-dimensional normal variable


$$
\bm{h} = [h(1), h(2), h(3), h(4), h(5)] \sim \mathcal{N}(\bm{m}, \bm{K})
$$

\begin{figure}[h]
\foreach \x in{1,2,3} {
  \includegraphics<\x>[width=0.4\linewidth]{figure/discrete/example_norm_5_\x_a.pdf} ~  \includegraphics<\x>[width=0.4\linewidth]{figure/discrete/example_norm_5_\x_b.pdf}
}
\end{figure}

\end{frame}

\begin{frame}{Discrete Functions}

\textbf{Example 3 (continued):} Let us consider $h: \Xspace \to \Yspace$ where the input space consists of \textbf{ten} points. We sample functions by sampling from ten-dimensional normal variable

$$
\bm{h} = [h(1), h(2), \dots, h(10)] \sim \mathcal{N}(\bm{m}, \bm{K})
$$

\begin{figure}[h]
\foreach \x in{1,2,3} {
  \includegraphics<\x>[width=0.4\linewidth]{figure/discrete/example_norm_10_\x_a.pdf} ~  \includegraphics<\x>[width=0.4\linewidth]{figure/discrete/example_norm_10_\x_b.pdf}
}
\end{figure}

\end{frame}


\begin{vbframe}{Role of the Covariance Function}

Note that the covariance controls the \enquote{shape} of the drawn function. Consider two extreme cases where function values are

\begin{enumerate}
  \item[a)] strongly correlated: $\bm{K} = $ \footnotesize$ \begin{pmatrix} 1 & 0.99 & \dots & 0.99 \\
  0.99 & 1 & \dots & 0.99 \\
  0.99 & 0.99 & \ddots & 0.99 \\
  0.99 & \dots & 0.99 & 1 \end{pmatrix}$ \normalsize
  \item[b)] uncorrelated: $\bm{K} = \id$
\end{enumerate}

\begin{figure}
  \includegraphics[width=0.35\linewidth]{figure/discrete/example_extreme_50_1.pdf} ~~  \includegraphics[width=0.35\linewidth]{figure/discrete/example_extreme_50_2.pdf}
\end{figure}


\framebreak 

\begin{itemize}
  \item \enquote{Meaningful} functions (on a numeric space $\Xspace$) may be characterized by a spatial property: \vspace*{0.2cm}
  \begin{itemize}
    \item[] If two points $\xi, \xi[j]$ are close in $\Xspace$-space, their function values $f(\xi), f(\xi[j])$ should be close in $\Yspace$-space. 
  \end{itemize} \vspace*{0.2cm}
  In other words: If they are close in $\Xspace$-space, their functions values should be \textbf{correlated}! \vspace*{0.4cm}
  \item We can enforce that by choosing a covariance function with  
  $$
    \bm{K}_{ij} \text{ high, if } \xi[i], \xi[j] \text{ close.}
  $$

  \framebreak 

  \item We can compute the entries of the covariance matrix by a function that is based on the distance between $\xi, \xi[j]$, for example: 
  
  \vspace*{0.2cm}
  \begin{enumerate}
    \item[c)] Spatial correlation: \begin{footnotesize}$K_{ij} = k(\xi[i], \xi[j]) = \exp\left(-\frac{1}{2}\left|\xi - \xi[j]\right|^2\right)$\end{footnotesize}
  \end{enumerate}
  
\begin{figure}
  \includegraphics[width=0.45\linewidth]{figure/discrete/example_extreme_50_4.pdf} ~~  \includegraphics[width=0.45\linewidth]{figure/discrete/example_extreme_50_3.pdf}
\end{figure}

\end{itemize}

\begin{footnotesize}
\textbf{Note}: $k(\cdot,\cdot)$ is known as the \textbf{covariance function} or \textbf{kernel}. It will be studied in more detail later on.
\end{footnotesize}

\end{vbframe}




% \begin{vbframe}
% \begin{figure}
% 	\centering
% 	\includegraphics{figure_man/discrete/sample2.png} \\
% 	\begin{footnotesize} If we sample again, we get another function. 
% 	\end{footnotesize}
% \end{figure}


% However, we are usually interested in functions with infinite domain size. 

% \lz 

% This idea is extended to infinite domain size via \textbf{Gaussian processes}. 

% \end{vbframe}


\section{Gaussian Processes}

\begin{vbframe}{From Discrete to Continuous Functions}

\begin{itemize}
  \item We defined distributions on functions with discrete domain by defining a Gaussian on the vector of the respective function values 
  $$
    \mathbf{h} = [h(\xi[1]), h(\xi[2]), \dots, h(\xi[n])] \sim \mathcal{N}(\bm{m}, \bm{K})
  $$

  \item We can do this for $n \to \infty$ (as \enquote{granular} as we want)
  \begin{figure}
    \includegraphics[width = 0.9\textwidth]{figure/discrete/example_limit.pdf}
  \end{figure}
\end{itemize}

\end{vbframe}

\begin{frame}{From Discrete to Continuous Functions}


\begin{itemize}
  \item No matter how large $n$ is, we are still considering a function over a discrete domain. 
  \item How can we extend our definition to functions with \textbf{continuous domain} $\Xspace \subset \R$?
\end{itemize}

\end{frame}


\begin{frame}{Gaussian Processes: Intuition}

\begin{itemize}
  \only<1>{
    \item Intuitively, a function $f$ drawn from \textbf{Gaussian process} can be understood as an \enquote{infinite} long Gaussian random vector. 
    \item It is unclear how to handle an \enquote{infinite} long Gaussian random vector!
  \lz 
  \begin{figure}
    \includegraphics[width=0.3\textwidth]{figure_man/question.png}
  \end{figure}
  }
  \only<2-4>{
    \item Thus, it is required that for \textbf{any finite set} of inputs $\{\xi[1], \dots, \xi[n]\} \subset \Xspace$, the vector $\mathbf{f}$ has a Gaussian distribution
    $$
      \bm{f} = \left[f\left(\xi[1]\right), \dots, f\left(\xi[n]\right)\right] \sim \mathcal{N}\left(\bm{m}, \bm{K}\right),
    $$ 
    with $\bm{m}$ and $\bm{K}$ being calculated by a mean function $m(.)$ / covariance function $k(.,.)$.
    \item This property is called \textbf{Marginalization Property}. 
    \begin{figure}
      \only<2>{\includegraphics[width=0.4\textwidth]{figure/discrete/example_marginalization_5.pdf}\includegraphics[width=0.5\textwidth]{figure_man/discrete/marginalization-5.png}}
      \only<3>{\includegraphics[width=0.4\textwidth]{figure/discrete/example_marginalization_10.pdf}\includegraphics[width=0.5\textwidth]{figure_man/discrete/marginalization-more.png}}
      \only<4>{\includegraphics[width=0.4\textwidth]{figure/discrete/example_marginalization_50.pdf}\includegraphics[width=0.5\textwidth]{figure_man/discrete/marginalization-more.png}}
   \end{figure}
    }
\end{itemize}

\end{frame}


\begin{vbframe}{Gaussian Processes}

This intuitive explanation is formally defined as follows: 

\lz 

A function $\fx$ is generated by a GP $\gp$ if for \textbf{any finite} set of inputs $\left\{\xv^{(1)}, \dots, \xv^{(n)}\right\}$, the associated vector of function values $\bm{f} = \left(f(\xv^{(1)}), \dots, f(\xv^{(n)})\right)$ has a Gaussian distribution

$$
\bm{f} = \left[f\left(\xi[1]\right),\dots, f\left(\xi[n]\right)\right] \sim \mathcal{N}\left(\bm{m}, \bm{K}\right),
$$

with 


\begin{eqnarray*}
\textbf{m} &:=& \left(m\left(\xi\right)\right)_{i}, \quad
\textbf{K} := \left(k\left(\xi, \xv^{(j)}\right)\right)_{i,j}, 
\end{eqnarray*}
 
where $m(\xv)$ is called mean function and $k(\xv, \xv^\prime)$ is called covariance function. 


\framebreak 

\vspace*{0.5cm} 

A GP is thus \textbf{completely specified} by its mean and covariance function

\vspace*{-0.2cm}
\begin{eqnarray*}
m(\xv) &=& \E[f(\xv)] \\
k(\xv, \xv^\prime) &=& \E\biggl[\left( f(\xv) - \E[f(\xv)] \right) \left( f(\xv^\prime) - \E[f(\xv^\prime)] \right)\biggr]
\end{eqnarray*}

\vfill

\textbf{Note}: For now, we assume $m(\xv) \equiv 0$. This is not necessarily a drastic limitation - thus it is common to consider GPs with a zero mean function. 

% \framebreak

% \vspace*{0.5cm}

% Intuitively, one can think of a function $f$ drawn from a Gaussian process prior as a Gaussian distribution with an \enquote{infinitely} long mean vector and an \enquote{infinite by infinite} covariance matrix.

% \lz

% Each dimension of the Gaussian corresponds to an element $\xv$ from the domain $\mathcal{X}$. The corresponding component of the random vector represents the value of $f(\xv)$.

% \lz

% The \textbf{marginalization property} makes it possible to handle this \enquote{infinite} representation: evaluations of the process on any finite number of points follow a multivariate normal distribution.

\end{vbframe}

\begin{vbframe}{Sampling from a Gaussian process Prior}

We can draw functions from a Gaussian process prior. Let us consider $\fx \sim \mathcal{GP}\left(0, k(\xv, \xv^\prime)\right)$ with the squared exponential covariance function $^{(*)}$

$$
k(\xv, \xv^\prime) = \exp\left(-\frac{1}{2\ls^2}\|\xv - \xv^\prime\|^2\right), ~~ \ls = 1.
$$
\vspace{-4cm}
This specifies the Gaussian process completely. 

\vspace{8cm}
\footnotesize
$^{(*)}$ We will talk later about different choices of covariance functions. 

\normalsize

\framebreak 

To visualize a sample function, we 

\begin{itemize}
  \item choose a high number $n$ (equidistant) points $\left\{\xv^{(1)}, \dots, \xv^{(n)}\right\}$
  \item compute the corresponding covariance matrix $\Kmat = \left(k\left(\xi, \xv^{(j)}\right)\right)_{i,j}$ by plugging in all pairs $\xv^{(i)}, \xv^{(j)}$ 
  \item sample from a Gaussian $\bm{f} \sim \mathcal{N}(\bm{0}, \bm{K})$. 
\end{itemize}

We draw $10$ times from the Gaussian, to get $10$ different samples.  

% Using $100$ equidistant points, we repeat the process of generating the Gaussian $10$ times ($10$ different functions) and draw each function by connecting the sampled values. 

% \lz

\begin{figure}
  \includegraphics[width=0.9\textwidth]{figure/gp_sample/different_samples.pdf}
\end{figure}

\vspace{-0.2cm}
Since we specified the mean function to be zero $m(\xv) \equiv 0$, the drawn functions have zero mean.

\end{vbframe}


\section{Gaussian Processes as Indexed Family}




\begin{vbframe}{Gaussian processes as an Indexed Family}

% \begin{block}{Definition}
% A \textbf{Gaussian process} is a (infinite) collection of random variables, any \textbf{finite} number of which have a \textbf{joint Gaussian distribution}.
% \end{block}

% \lz

A Gaussian process is a special case of a \textbf{stochastic process} which is defined as a collection of random variables indexed by some index set (also called an \textbf{indexed family}). What does it mean? 

\lz 

An \textbf{indexed family} is a mathematical function (or \enquote{rule}) to map indices $t \in T$ to objects in $\mathcal{S}$. 

\begin{block}{Definition}
A \textbf{family of elements in $\mathcal{S}$ indexed by $T$} (indexed family) is a surjective function 
\vspace*{-0.3cm}
\begin{eqnarray*}
s: T &\to& \mathcal{S} \\
   t &\mapsto& s_t = s(t) 
\end{eqnarray*}
\end{block}

\end{vbframe}

\begin{vbframe}{Indexed Family}

Some simple examples for indexed families are:

\vspace*{0.3cm}

\begin{minipage}{0.43\linewidth}
  \begin{itemize}
  \item finite sequences (lists): $T = \{1, 2, \dots, n\}$ and $\left(s_t\right)_{t \in T} \in \R$ \vspace{1cm}
  \item infinite sequences: $T = \N$ and $\left(s_t\right)_{t \in T} \in \R$
  \end{itemize}
\end{minipage}
\begin{minipage}{0.55\linewidth}
\includegraphics{figure_man/indexed_family/indexed_family_1.png} \\
\includegraphics{figure_man/indexed_family/indexed_family_2.png}
\end{minipage}


\framebreak

But the indexed set $\mathcal{S}$ can be something more complicated, for example functions or \textbf{random variables} (RV):

\begin{minipage}{0.43\linewidth}
  \vspace*{0.5cm}
  \begin{itemize}
    \item $T = \{1, \dots, m\}$, $Y_t$'s are RVs: Indexed family is a random vector. \vspace*{0.2cm}
    \item $T = \N$, $Y_t$'s are RVs: Indexed family is a stochastic process in discrete time \vspace*{0.2cm}
    \item $T = \Z^2$, $Y_t$'s are RVs: Indexed family is a 2D-random walk.
  \end{itemize}
\end{minipage}\hfill
\begin{minipage}{0.5\linewidth}
\includegraphics{figure_man/indexed_family/indexed_family_4.png} \\
\includegraphics{figure_man/indexed_family/indexed_family_3.png}
\end{minipage}

\end{vbframe}

\begin{frame}{Indexed Family}

\begin{itemize}
  \item A Gaussian process is also an indexed family, where the random variables $f(\xv)$ are indexed by the input values $\xv \in \Xspace$. 
  \item Their special feature: Any indexed (finite) random vector has a multivariate Gaussian distribution (which comes with all the nice properties of Gaussianity!). 
\end{itemize}

\begin{figure}
  \includegraphics<1>[width=0.7\textwidth]{figure_man/indexed_family/indexed_family_5.png} \par
  \only<1>{\begin{footnotesize} Visualization for a one-dimensional $\Xspace$. \end{footnotesize}}
  \includegraphics<2>[width=0.6\textwidth]{figure_man/indexed_family/indexed_family_6.png}\par
  \only<2>{\begin{footnotesize} Visualization for a two-dimensional $\Xspace$. \end{footnotesize}}
\end{figure}

\end{frame}


\endlecture
\end{document}
