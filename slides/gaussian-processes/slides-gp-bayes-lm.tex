\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-gp}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{
Gaussian Processes
}{
Bayesian Linear Model
}{
figure/bayes_lm/posterior_5_3.pdf
}{
\item Know the Bayesian linear model
\item The Bayesian LM returns a (posterior) distribution instead of a point estimate
\item Know how to derive the posterior distribution for a Bayesian LM 
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei}{data situation}
\item $\D = \Dset$: i.i.d. training set from some unknown distribution
\imageC[.6]{figure/bayes_lm/example.pdf}
\item $\Xmat \in \R^{n \times p}$: design matrix, where $i$-th row contains vector $\xi$
\item $\yv = \yvec$
\end{framei}

\begin{framei}[sep=L]{the bayesian linear model revisited}
\item Standard linear regression model for $i$-th observation, with $\thetav \in \R^p$ fixed but unknown
$$\yi = \fxi + \epsi = \thetav^T \xi + \epsi, \quad \text{for } i \in \nset$$
\item Bayesian perspective: $\thetav$ = RV with associated distribution
\item Function outputs $\fxi$ differ from observed values $\yi$ by some additive noise assumed to be i.i.d. Gaussian
$$\epsi \sim \mathcal{N}(0, \sigma^2) \quad \forall i$$ 
$\Rightarrow$ Independent of $\xv, \thetav$
\end{framei}

\begin{framei}[sep=L]{from prior to posterior}
\item Prior belief about parameter distribution, e.g., $\thetav \sim \normal(\zero, \tau^2 \id_p)$
\item Bayes' rule: update prior to posterior belief after observing data
$$
p(\thetav | \Xmat, \yv) 
= \frac{\text{likelihood} \cdot \text{prior}}{\text{marginal likelihood}} 
= \frac{p(\yv | \Xmat, \thetav) \cdot q(\thetav)}{p(\yv|\Xmat)}
$$
\item Gaussian family is ``self-conjugate'': Gaussian prior \& Gaussian likelihood $\Rightarrow$ Gaussian posterior 
$$
\thetav ~|~ \Xmat, \yv \sim \mathcal{N}(\sigma^{-2} \Kmat^{-1}\Xmat^T\yv, \Kmat^{-1})
$$
with $\Kmat:= \sigma^{-2}\Xmat^T\Xmat + \tau^{-2} \id_p$
\end{framei}

\begin{frame}{posterior contraction}
\vfill
\splitVTT{
\imageL{figure/bayes_lm/prior_1.pdf}
}{
\imageR{figure/bayes_lm/prior_2.pdf}
}
\end{frame}

\foreach \i in{5, 10, 20}{
\begin{frame}{posterior contraction}
\vfill
\splitVTT{
\imageL{figure/bayes_lm/posterior_\i_1.pdf}
}{
\imageR{figure/bayes_lm/posterior_\i_2.pdf}
}
\end{frame}
}

\begin{framei}[fs=small]{proof: gaussianity of posterior}
\item We want to show that for
\begin{itemize} \small
  \item Gaussian prior $\thetav \sim \normal(\zero, \tau^2 \id_p)$ and 
  \item Gaussian likelihood $\yv ~|~ \Xmat, \thetav \sim \normal(\Xmat^T \thetav, \sigma^2 \id_n)$ 
\end{itemize}
the resulting posterior is Gaussian: $\thetav ~|~ \Xmat, \yv \sim \normal(\sigma^{-2}\Kmat^{-1}\Xmat^T\yv, \Kmat^{-1})$ 
\vfill
\item Plug in Bayes' rule and keep only terms depending on $\thetav$
\begin{eqnarray*}
p(\thetav | \Xmat, \yv) &\propto& p(\yv | \Xmat, \thetav) q(\thetav) \quad \propto \quad \exp [-\tfrac{1}{2\sigma^2}(\yv - \Xmat\thetav)^T(\yv - \Xmat\thetav)-\tfrac{1}{2\tau^2}\thetav^T\thetav ] \\
&=& \exp \left[ -\tfrac{1}{2} (\textcolor{black!50}{\sigma^{-2}\yv^T\yv} - 2 \sigma^{-2} \yv^T \Xmat \thetav + \sigma^{-2}\thetav^T \Xmat^T \Xmat \thetav  + \tau^{-2} \thetav^T\thetav ) \right] \\
&\propto& \exp \left[ -\tfrac{1}{2} (\sigma^{-2}\thetav^T \Xmat^T \Xmat \thetav  + \tau^{-2} \thetav^T\thetav  - 2 \sigma^{-2} \yv^T \Xmat \thetav ) \right] \\
&=& \exp \biggl[ -\tfrac{1}{2}\thetav^T\underbrace{(\sigma^{-2} \Xmat^T \Xmat + \tau^{-2} \id_p )}_{:= \Kmat} \thetav  +\textcolor{orange}{\sigma^{-2} \yv^T \Xmat \thetav} \biggr] 
\end{eqnarray*}
\item Note how this resembles a normal density, except for the term in \textcolor{orange}{orange}
\item No need to worry about normalizing constant -- sole purpose: ensure density integrates to total prob of 1
\end{framei}

\begin{framei}[fs=small]{proof: gaussianity of posterior}
\item Get rid of nuisance term by introducing \textcolor{blue}{complementary terms} and compensating for the added quantities  (``creative 0'')
\begin{eqnarray*}
	p(\thetav | \Xmat, \yv) &\propto&  \exp[-\tfrac{1}{2}(\thetav \textcolor{blue}{- c})^T\Kmat  (\thetav \textcolor{blue}{- c}) \textcolor{blue}{- c^T \Kmat \thetav} + \underbrace{\textcolor{blue}{\tfrac{1}{2}c^T\Kmat c}}_{\text{doesn't depend on } \thetav} + \textcolor{orange}{\sigma^{-2} \yv^T \Xmat \thetav} ] \\
	&\propto& \exp [-\tfrac{1}{2}(\thetav \textcolor{blue}{- c})^T\Kmat  (\thetav \textcolor{blue}{- c}) \textcolor{blue}{- c^T \Kmat \thetav} + \textcolor{orange}{\sigma^{-2} \yv^T \Xmat \thetav} ]
\end{eqnarray*}
\item Choosing $c$  s.t. 
$\textcolor{blue}{- c^T \Kmat \thetav} + \textcolor{orange}{\sigma^{-2} \yv^T \Xmat \thetav} = 0$ 
leads to 
$p(\thetav | \Xmat, \yv) \propto \exp [-\tfrac{1}{2}(\thetav - c)^T\Kmat  (\thetav -c) ]$
\item Using that $\Kmat$ is symmetric, this implies 
\begin{eqnarray*}
&& \sigma^{-2} \yv^T \Xmat = c^T\Kmat \\
&\Leftrightarrow & \sigma^{-2} \yv^T \Xmat \Kmat^{-1} = c^T \\
&\Leftrightarrow& c = \sigma^{-2} \Kmat^{-1} \Xmat^T \yv
\end{eqnarray*}
\item Finally: $\thetav ~|~ \Xmat, \yv \sim  \normal(\sigma^{-2}\Kmat^{-1}\Xmat^T\yv, \Kmat^{-1}) \qed$
\end{framei}

\begin{framei}[sep=L]{posterior predictive distribution}
\item Based on the posterior 
$$
\thetav ~|~ \Xmat, \yv \sim  \normal(\sigma^{-2}\Kmat^{-1}\Xmat^T\yv, \Kmat^{-1})
$$
we can derive the predictive distribution of the Bayesian LM
\item For a new observation $\xv_*$ we get
$$
y_* ~|~ \Xmat, \yv, \xv_* \sim \mathcal{N}(\sigma^{-2}\yv^T \Xmat \Kmat^{-1}\xv_*, \xv_*^T\Kmat^{-1}\xv_*)
$$
\item NB: entire distribution with built-in confidence
% (applying the rules for linear transformations of Gaussians). 
\end{framei}

\foreach \i in{5, 10, 20} {
\begin{framei}{posterior mean and variance}
\item For every test input $\xv_*$, we get a posterior mean (orange) \& variance (grey region; $\pm 2\times$ standard deviation)
\vfill
\imageC[.5]{figure/bayes_lm/posterior_\i_3.pdf}
\end{framei}
}

\begin{framei}[sep=L]{summary: bayesian lm}
\item Bayesian perspective: whole distributions, rather than just point estimates, for $\thetav$
\item From posterior distribution of $\thetav$ we can derive a predictive distribution for $y_* = \thetav^T \xv_*$
\item Online updates: after observing new data points, recompute posterior $\Rightarrow$ decreasing uncertainty
\item Next up: generalize this idea to nonlinear functions
\end{framei}

\endlecture
\end{document}
