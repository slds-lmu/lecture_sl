\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-gp}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{
    Gaussian Processes
  }{
    Training of a Gaussian Process
  }{
  figure/gp_training/fit_vs_penalty.pdf
  }{
  \item Training of GPs via Maximum Likelihood estimation of its hyperparameters
  \item Computational complexity is governed by matrix inversion of the covariance matrix
}

\begin{framei}[sep=L]{Training of a Gaussian process}
\item All we need for GP predictions (in regression): matrix computations
\item Implicit assumption: fully specified cov function, incl. hyperparams
\item Nice GP property: numerical hyperparams of given cov function can be learned during training
\end{framei}

\begin{framei}[sep=L]{Training a GP via maximum likelihood}
\item Let $$y = \fx + \epsilon, \quad \epsilon \sim \normal(0, \sigma^2)$$
with $f \sim \mathcal{GP}(\zero, k(\cdot, \cdot | \lamv))$ for hyperparam config $\lamv$
\item This yields $\yv \sim \normal(\zero, \Kmat_y )$ with  $\Kmat_y = \Kmat + \sigma^2 \id$
\item We get the \textbf{marginal log-likelihood / evidence}
\begin{eqnarray*}
\log p(\yv | \bm{X}, \lamv) &= \log \left[(2 \pi )^{-n / 2} |\Kmat_y|^{-1 / 2} \exp(- \tfrac{1}{2} \yv^T \Kmat_y^{-1} \yv) \right] \\
&= - \tfrac{1}{2} \yv^T \Kmat_y^{-1} \yv - \tfrac{1}{2} \log | \Kmat_y | - \tfrac{n}{2} \log 2 \pi
\end{eqnarray*}
\item $\log p(\yv | \bm{X}, \lamv)$ depends on $\lamv$ (though   usually omitted in notation)
\end{framei}

\begin{framei}[sep=L]{marginal log-likelihood components}
\item Common cov function hyperparam: length-scale $\ls$
\item Recall: smaller $\ls$ $\Rightarrow$ more flexible model
\item Marginal log-lik terms have interpretable roles depending on $\ls$
\item \textbf{Data fit} $- \tfrac{1}{2} \yv^T \Kmat_y^{-1} \yv$: better for smaller $\ls$
\item \textbf{Smoothness term} $- \tfrac{1}{2} \log | \Kmat_y |$: smaller for smaller  $\ls$
\item \textbf{Normalization constant} $- \tfrac{n}{2} \log 2 \pi$
\end{framei}

% ----------------------------------------------------

\begin{framei}[sep=L]{mll components: example}
\item Let $f \sim \mathcal{GP}(\zero, k(\cdot, \cdot))$ with squared exp kernel $$k(\xv, \tilde{\xv}) = \exp(-\tfrac{1}{2 \ls^2} \| \xv - \tilde{\xv} \|^2)$$
\vfill
\splitV{
\imageC[1]{figure/gp_training/fit_vs_penalty.pdf}
}{
\imageC[1]{figure/gp_training/datapoints.pdf}
}
\end{framei}

\begin{framei}[sep=L]{mll components: example}
\item Let $f \sim \mathcal{GP}(\zero, k(\cdot, \cdot))$ with squared exp kernel $$k(\xv, \tilde{\xv}) = \exp(-\tfrac{1}{2 \ls^2} \| \xv - \tilde{\xv} \|^2)$$
\item $\ls = 0.2$: good fit but low smoothness term
\vfill
\splitV{
\imageC[1]{figure/gp_training/fit_vs_penalty_0_2.pdf}
}{
\imageC[1]{figure/gp_training/datapoints_0_2.pdf}
}
\end{framei}

\begin{framei}[sep=L]{mll components: example}
\item Let $f \sim \mathcal{GP}(\zero, k(\cdot, \cdot))$ with squared exp kernel $$k(\xv, \tilde{\xv}) = \exp(-\tfrac{1}{2 \ls^2} \| \xv - \tilde{\xv} \|^2)$$
\item $\ls = 2$: high smoothness but poor fit
\vfill
\splitV{
\imageC[1]{figure/gp_training/fit_vs_penalty_2.pdf}
}{
\imageC[1]{figure/gp_training/datapoints_2.pdf}
}
\end{framei}

\begin{framei}[sep=L]{mll components: example}
\item Let $f \sim \mathcal{GP}(\zero, k(\cdot, \cdot))$ with squared exp kernel $$k(\xv, \tilde{\xv}) = \exp(-\tfrac{1}{2 \ls^2} \| \xv - \tilde{\xv} \|^2)$$
\item $\ls = 0.5$: balancing fit and smoothness term
\vfill
\splitV{
\imageC[1]{figure/gp_training/fit_vs_penalty_0_5.pdf}
}{
\imageC[1]{figure/gp_training/datapoints_0_5.pdf}
}
\end{framei}

% ----------------------------------------------------

\begin{framei}[sep=L]{optimizing kernel hyperparameters}
\item Idea: set partial derivatives wrt hyperparams to 0
\begin{eqnarray*}
\pd{}{\lambda_j}
\log p(\yv ~|~ \bm{X}, \lamv) &=& \pd{}{\lambda_j}  \left(-\tfrac{1}{2}\yv^T\Kmat_y^{-1} \yv - \tfrac{1}{2} \log \left| \Kmat_y \right| - \tfrac{n}{2} \log 2\pi\right) \\ 
&=&\tfrac{1}{2} \yv^T \Kmat^{-1} \pd{\Kmat}{\lambda_j}\Kmat^{-1} \yv - \tfrac{1}{2} \text{tr}\left(\Kmat^{-1} \pd{\Kmat}{ \lamv} \right) \\
&=& \tfrac{1}{2} \text{tr}\left((\Kmat^{-1}\yv\yv^T\Kmat^{-1} - \Kmat^{-1})\pd{\Kmat}{\lambda_j}\right)
\end{eqnarray*}
using $\pd{}{\lambda_j} \Kmat^{-1} = - \Kmat^{-1}  \pd{\Kmat}{\lambda_j}\Kmat^{-1}$ and $\pd{}{\lamv} \log  |\Kmat| = \text{tr}\left(\Kmat^{-1} \pd{\Kmat}{\lamv} \right)$
\item Bottleneck: inverting (or rather, decomposing) $\Kmat$ \\$\Rightarrow$ $\order(n^3)$ for standard methods
\item Only $\order(n^2)$ per hyperparam / partial derivative once $\Kmat^{-1}$ is known $\Rightarrow$ small overhead, so use gradient-based optim 
\end{framei}

\begin{framei}[sep=L]{strategies for big data}
\item Kernels that yield sparse $\Kmat$ $\Rightarrow$ cheaper to invert
\item Subsample data $\Rightarrow$ $\order(m^3)$ with $m^3 \ll n^3$
\item \textbf{Bayesian committee}: combine estimates on different size-$m$ estimates $\Rightarrow$ $\order(nm^2)$
\item \textbf{Nystr√∂m approx}: low-rank approx from representative subset (``inducing points''): $\Kmat \approx \Kmat_{nm}\Kmat_{mm}^{-1}\Kmat_{mn}$ $\Rightarrow$ $\order(nmk + m^3)$ for rank-$k$-approx inverse of $\Kmat_{mm}$
\item Exploit structure in $\Kmat$ induced by kernels $\Rightarrow$ exact but complicated solutions; kernel-specific
\item Still active research area
\end{framei}

% \begin{vbframe}{Gaussian process as a linear smoother}
% 
% Let's consider mean prediction at training points only. For simplicity, we write $\Kmat:= \Kmat(\Xmat, \Xmat)$. The predicted mean values at the training points are
% 
% $$
% \bm{\bar f} = \Kmat(\Kmat + \sigma_n^2\id)^{-1}.
% $$
% 
% Let $\Kmat$ have the eigendecomposition $\Kmat = \sumin \lambda_i\bm{u}_i \bm{u}_i^T. $\lambda_i$ is the $i$-th eigenvalue, and $\bm{u}_i$ is the corresponding eigenvalue. The predicted mean can be written as
% 
% $$
% \bm{\bar f} = \sumin \frac{\gamma_i \lambda_i}{\lambda_i + \sigma_n^2}\bm{u}_i
% $$
% 
% 
% \end{vbframe}

\endlecture
\end{document}