\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-gp}

\title{Introduction to Machine Learning}

\begin{document}

% \newcommand{\fvec}{[f(\xi[1], \dots, \xi[n]]}
% \kxij ohne left right
\newcommand{\nmk}{\normal(\zero, \Kmat)} % replause gaussmk
\newcommand{\nzk}{\normal(\zero, \Kmat)}
\newcommand{\xxtnorm}{\| \xv - \xtil \|}
\newcommand{\kcc}{k(\cdot, \cdot)} 
\newcommand{\kxxt}{k(\xv, \xtil)} % already in ml-svm
\newcommand{\Xsubset}{\bm{X}}
\newcommand{\fX}{f(\Xsubset)}
\newcommand{\kXX}{k(\Xsubset, \Xsubset)}
\newcommand{\mX}{m(\Xsubset)}
\newcommand{\gpmk}{\mathcal{GP}(m(\cdot), k(\cdot, \cdot))}
\newcommand{\gpzk}{\mathcal{GP}(\zero, k(\cdot, \cdot))}
\newcommand{\Kmatinv}{\Kmat^{-1}}
\newcommand{\xstar}{\xv_\ast}
\newcommand{\ystar}{\yv_\ast}
\newcommand{\fstar}{\fv_\ast}
\newcommand{\Xstar}{\Xmat_\ast}
\newcommand{\Ky}{\Kmat_y}

\titlemeta{
    Gaussian Processes
  }{
    Training of a Gaussian Process
  }{
  figure/gp_training/nll_components.pdf
  }{
  \item Training of GPs via Maximum Likelihood estimation of its hyperparameters
  \item Computational complexity is governed by matrix inversion of the covariance matrix
}

\begin{framei}[sep=L]{Training of a Gaussian process}
\item All we need for GP predictions (in regression): matrix computations
\item Implicit assumption: fully specified cov function, incl. hyperparams
\item Nice GP property: numerical hyperparams of given cov function can be learned during training
\end{framei}

\begin{framei}[sep=L]{Training via maximum likelihood}
\item Let $$y = \fx + \epsilon, \quad \epsilon \sim \normal(0, \sigma^2)$$
with $f \sim \mathcal{GP}(\zero, k(\cdot, \cdot | \thetav))$ for hyperparam config $\thetav$
\item This yields $\yv \sim \normal(\zero, \Ky )$ with  $\Ky = \Kmat + \sigma^2 \id$
\item We get the \textbf{negative marginal log-likelihood / evidence}
\begin{eqnarray*}
-\log p(\yv | \bm{X}, \thetav) &= -\log \left[(2 \pi )^{-n / 2} |\Ky|^{-1 / 2} \exp(- \tfrac{1}{2} \yv^T \Ky^{-1} \yv) \right] \\
&= \textcolor{blue}{\tfrac{1}{2} \yv^T \Ky^{-1} \yv} \textcolor{orange}{+ \tfrac{1}{2} \log | \Ky |}  + \tfrac{n}{2} \log 2 \pi
\end{eqnarray*}
\item $-\log p(\yv | \bm{X}, \thetav)$ depends on $\thetav$ via $\Ky$ $\Rightarrow$ optimize for $\thetav$
\end{framei}

\begin{framei}[sep=L]{negative log-likelihood components}
\item Consider common cov type parameterized by length-scale: $\thetav = \ls$ (for simplicity: assume univariate rather than different for each dim) 
% \item Recall: smaller $\ls$ $\Rightarrow$ more flexible model
\item E.g., squared exponential kernel $$\kxxt = \exp(-\tfrac{1}{2 \ls^2} \xxtnorm^2)$$
\item For small $\ls$, cov decays quickly $\Rightarrow$ local model, $\Ky$ approaches $\sigma^2 \id$
\item \textbf{Data fit}: small $\ls$ $\Rightarrow$ small $\Ky^{-1} \yv$ (cov structure aligns well with observed data; small residuals) $\Rightarrow$ small $\textcolor{blue}{\tfrac{1}{2} \yv^T \Ky^{-1} \yv}$
% \item \textbf{Data fit} $- \tfrac{1}{2} \yv^T \Ky^{-1} \yv$: better for smaller $\ls$
\item \textbf{Complexity penalty}: small  $\ls$ $\Rightarrow$ large $\textcolor{orange}{\tfrac{1}{2} \log | \Ky |}$
\item Normalization constant $\tfrac{n}{2} \log 2 \pi$
\end{framei}

% ----------------------------------------------------

\begin{framei}[sep=L]{nll components: example}
\item Let $f \sim \mathcal{GP}(\zero, k(\cdot, \cdot))$ with squared exp kernel $$\kxxt = \exp(-\tfrac{1}{2 \ls^2} \xxtnorm^2)$$
\item Minimize NLL by trading off data fit \& complexity
\vfill
\splitV{
\imageC[1]{figure/gp_training/nll_components.pdf}
}{
\imageC[1]{figure/gp_training/datapoints.pdf}
}
\end{framei}

\foreach \i [count=\idx from 1] in {0.2, 2, 0.5}{
\begin{framei}[sep=L]{nll components: example}
\item Let $f \sim \gpzk$ with squared exp kernel $$\kxxt = \exp(-\tfrac{1}{2 \ls^2} \xxtnorm^2)$$
\item Minimize NLL by trading off data fit \& complexity
\ifnum \idx=1
\item $\ls = \i$: good fit but high complexity
\fi \ifnum \idx=2
\item $\ls = \i$: smooth but poor fit
\fi \ifnum \idx=3
\item $\ls = \i$: balancing data fit and smoothness
\fi
\vfill
\splitV{
\imageC[1]{figure/gp_training/nll_components_ls_\idx.pdf}
}{
\imageC[1]{figure/gp_training/datapoints_pred_\idx.pdf}
}
\end{framei}
}

% ----------------------------------------------------

\begin{framei}[sep=L]{optimizing kernel hyperparameters}
\item Set partial derivatives wrt hyperparams to 0
\begin{eqnarray*}
\pd{}{\theta_j}
\log p(\yv ~|~ \bm{X}, \thetav) &=& \pd{}{\theta_j}  \left(-\tfrac{1}{2}\yv^T\Ky^{-1} \yv - \tfrac{1}{2} \log \left| \Ky \right| - \tfrac{n}{2} \log 2\pi\right) \\ 
&=&\tfrac{1}{2} \yv^T \Kmatinv \pd{\Kmat}{\theta_j}\Kmatinv \yv - \tfrac{1}{2} \text{tr}\left(\Kmatinv \pd{\Kmat}{ \thetav} \right) \\
&=& \tfrac{1}{2} \text{tr}\left((\Kmatinv\yv\yv^T\Kmatinv - \Kmatinv)\pd{\Kmat}{\theta_j}\right)
\end{eqnarray*}
using $\pd{}{\theta_j} \Kmatinv = - \Kmatinv  \pd{\Kmat}{\theta_j}\Kmatinv$ and $\pd{}{\thetav} \log  |\Kmat| = \text{tr}\left(\Kmatinv \pd{\Kmat}{\thetav} \right)$
\item Bottleneck: inverting (or rather, decomposing) $\Kmat$ \\$\Rightarrow$ $\order(n^3)$ for standard methods
\item Only $\order(n^2)$ per hyperparam / partial derivative once $\Kmatinv$ is known $\Rightarrow$ small overhead, so use gradient-based optim 
\end{framei}

\begin{framei}[sep=L]{strategies for big data}
\item Kernels that yield sparse $\Kmat$ $\Rightarrow$ cheaper to invert
\item Subsample data $\Rightarrow$ $\order(m^3)$ with $m^3 \ll n^3$
\item \textbf{Bayesian committee}: combine estimates on different size-$m$ estimates $\Rightarrow$ $\order(nm^2)$
\item \textbf{Nystr√∂m approx}: low-rank approx from representative subset (``inducing points''): $\Kmat \approx \Kmat_{nm}\Kmat_{mm}^{-1}\Kmat_{mn}$ $\Rightarrow$ $\order(nmk + m^3)$ for rank-$k$-approx inverse of $\Kmat_{mm}$
\item Exploit structure in $\Kmat$ induced by kernels $\Rightarrow$ exact but complicated solutions; kernel-specific
\item Still active research area
\end{framei}

% \begin{vbframe}{Gaussian process as a linear smoother}
% 
% Let's consider mean prediction at training points only. For simplicity, we write $\Kmat:= \Kmat(\Xmat, \Xmat)$. The predicted mean values at the training points are
% 
% $$
% \bm{\bar f} = \Kmat(\Kmat + \sigma_n^2\id)^{-1}.
% $$
% 
% Let $\Kmat$ have the eigendecomposition $\Kmat = \sumin \theta_i\bm{u}_i \bm{u}_i^T. $\theta_i$ is the $i$-th eigenvalue, and $\bm{u}_i$ is the corresponding eigenvalue. The predicted mean can be written as
% 
% $$
% \bm{\bar f} = \sumin \frac{\gamma_i \theta_i}{\theta_i + \sigma_n^2}\bm{u}_i
% $$
% 
% 
% \end{vbframe}

\endlecture
\end{document}
