\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Information Theory
  }{% Lecture title  
    KL and Maximum Entropy
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/kl_log_diff_plot.png
  }{
  \item Know the defining properties of the KL
  \item Understand the relationship between the maximum entropy principle and minimum discrimination information
  \item Understand the relationship between Shannon entropy and relative entropy
}

\begin{vbframe} {Problems with Differential Entropy}
Differential entropy compared to the Shannon entropy:
\begin{itemize}
    \item Differential entropy can be negative
    \item Differential entropy is not invariant to coordinate transformations
\end{itemize}
$\Rightarrow$ Differential entropy is not an uncertainty measure and can not be meaningfully used in a maximum entropy framework. \\
\lz 
In the following, we derive an alternative measure, namely the KL divergence (relative entropy), that fixes these shortcomings by taking an inductive inference viewpoint.  \citelink{CATICHA2004RELATIVE}
\end{vbframe}
\begin{vbframe}{Inductive inference}
    We construct a "new" entropy measure $S(p)$ just by desired properties.\\
    \lz
    Let $\mathcal{X}$ be a measurable space with $\sigma$-algebra $\mathcal{F}$ and measure $\mu$ that can be continuous or discrete. \\
    We start with a prior distribution $q$ over $\mathcal{X}$ dominated by $\mu$ and a constraint of the form $$\int_D a(\xv) dq(\xv) = c \in \R$$
    with $D \in \mathcal{F}.$ The constraint function $a(\xv)$ is analogous to moment condition functions $g(\cdot)$ in the discrete case.
    We want to update the prior distribution $q$ to a posterior distribution $p$ that fulfills the constraint and is maximal w.r.t. $S(p).$  \\
    For this maximization to make sense, $S$ must be transitive, i.e., 
    $$S(p_1) < S(p_2), S(p_2) < S(p_3) \Rightarrow S(p_1) < S(p_3).$$
\end{vbframe}
\begin{vbframe}{Constructing the KL}
    \textbf{1) Locality} \\
    The constraint must only update the prior distribution in $D, i.e.,$ the region where it is active. \\
\includegraphics[width=0.3\linewidth]{figure_man/kl_me_constraint.png} \\
    \lz
    For this, it can be shown that the non-overlapping domains of $\mathcal{X}$ must contribute additively to the entropy, i.e.,
    $$S(p) = \int F(p(\xv), \xv) d\mu(\xv)$$
    where $F$ is an unknown function.
    
    \framebreak
    
    \textbf{2) Invariance to coordinate system} \\
    \lz 
    \includegraphics[width=0.5\linewidth]{figure_man/kl_me_cosy.png} \\
    Enforcing 2) results in 
    $$S(p) = \int \bm{\Phi}\left(\frac{dp}{dm}(\xv)\right)dm(\xv)$$
    where $\bm{\Phi}$ is an unknown function, $m$ is another measure on $\mathcal{X}$ dominated by $\mu$ and $\frac{dp}{dm}$ the Radonâ€“Nikodym derivative which becomes 
    \begin{itemize}
        \item the quotient of the respective pmfs for discrete measures,
        \item the quotient of respective pdfs (if they exist) for cont. measures.
    \end{itemize} 

    \framebreak

    \textbf{1 + 2)} 
    \\ $\Rightarrow m$ must be the prior distribution $q$, and our entropy measure must be understood relatively to this prior, so $S(p)$ becomes, in fact, $S(p\|q).$\\
    \lz 
    \textbf{3) Independent subsystems} \\
    \includegraphics[width=0.6\linewidth]{figure_man/kl_me_indep_sub.png} \\
    If the prior distribution defines a subsystem of $\mathcal{X}$ to be independent, then the priors can be independently updated, and the resulting posterior is just their product density.
    
    \framebreak

    \textbf{1 + 2 + 3)} \\
    Up to constants that do not change our entropy ranking, it follows that 
    $$S(p\|q) = - \int \log\left(\frac{dp}{dq}(\xv)\right)dp(\xv)$$
    which is just the negative KL, i.e., $-D_{KL}(p\|q).$

    \begin{itemize}
        \item With our desired properties, we ended up with KL minimization
        \item This is called the principle of minimum discrimination information, i.e., the posterior should differ from the prior as least as possible
        \item This principle is meaningful for continuous and discrete RVs
        \item The maximum entropy principle is just a special case when $\mathcal{X}$ is discrete and $q$ is the uniform distribution.
        \item Analogously, Shannon entropy can always be treated as negative KL  with uniform reference distribution.
    \end{itemize}
    
\end{vbframe}
\endlecture
\end{document}
