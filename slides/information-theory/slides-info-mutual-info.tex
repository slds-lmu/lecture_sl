\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure/entropy_plot.png}
\newcommand{\learninggoals}{
  \item Know the joint entropy
  \item Know conditional entropy as remaining uncertainty 
  \item Know mutual information as the amount of information of an RV obtained by another
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Joint Entropy and Mutual Information I}
\lecture{Introduction to Machine Learning}


\begin{vbframe}{Joint entropy}
\begin{itemize}
  \item The \textbf{joint entropy} of two discrete random variables $X$ and $Y$ with a joint distribution $p(x, y)$ is:
  $$ H(X,Y) = -\sum_{x \in \Xspace} \sum_{y \in \Yspace}  p(x,y) \log(p(x,y)),$$ 
  which can also be expressed as $$ H(X,Y) = -\E \left[ \log(p(X,Y)) \right].$$
  % where $I(x,y)$ is the self-information of $(x,y)$.
  % \item Intuitively, the joint entropy is a measure of the total uncertainty in the two variables $X$ and $Y$. In other words, it is simply the entropy of the joint distribution p(x,y).
  % \item $H(X,Y)$ is always non-negative.
  %\item $H(X,Y) \leq H(X) + H(Y)$, with equality if $X$ and $Y$ are independent.
  % \framebreak
  % \item More generally,
  % \begin{footnotesize}
  % $$ H(X_1, X_2, \ldots, X_n) = - \sum_{x_1 \in \Xspace_1} \ldots \sum_{x_n \in \Xspace_n} p(x_1,x_2, \ldots, x_n) \log_2(p(x_1,x_2, \ldots, x_n)) $$ 
  % \end{footnotesize}
  \item For continuous random variables $X$ and $Y$ with joint density $p(x,y)$, the differential joint entropy is:\\
  $$ h(X,Y) = - \int_{\Xspace \times \Yspace} p(x,y) \log p(x,y) dx dy$$
\end{itemize}

\begin{footnotesize}
For the rest of the section we will stick to the discrete case. Pretty much everything we show and discuss works in a completely analogous manner for the continuous case - if you change sums to integrals.
\end{footnotesize}

\end{vbframe}

\begin{vbframe}{Conditional entropy}
\begin{itemize}

\item The \textbf{conditional entropy} $H(Y|X)$ quantifies the uncertainty of $Y$ that remains if the outcome of $X$ is given.

\item $H(Y|X)$ is defined as the expected value of the entropies of the conditional distributions, averaged over the conditioning RV.
\item If $(X, Y) \sim p(x, y)$, the conditional entropy $H (Y|X)$ is defined as

% $$
% H(Y|X) = \sum_{x \in \Xspace} p_x(x) H(Y|X=x) \overset{(*)}{=} H(Y, X) - H(X).
% $$

\vspace{-0.2cm}
\footnotesize
\begin{equation*}\begin{aligned}
H(Y | X) &= \E_X[H(Y|X=x)] = \sum_{x \in \Xspace} p(x) H(Y | X=x) \\
&=-\sum_{x \in \Xspace} p(x) \sum_{y \in \Yspace} p(y | x) \log p(y | x) \\
&=-\sum_{x \in \Xspace} \sum_{y \in \Yspace} p(x, y) \log p(y | x) \\
&=-\E \left[\log p(Y | X) \right]. 
\end{aligned}\end{equation*}
\normalsize

\item For the continuous case with density $f$ we have $$h(Y|X) = - \int f(x,y) \log f(x|y) dx dy.$$
\end{itemize}

\end{vbframe}



\begin{vbframe} {Chain rule for entropy}
The \textbf{chain rule for entropy} is analogous to the chain rule for probability and, in fact, derives directly from it.
$$H(X, Y)=H(X)+H(Y | X)$$
\footnotesize
\textbf{Proof:}
%\begin{equation*}
$\begin{aligned}[t]
H(X, Y) &=-\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(x, y) \\
&=-\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(x) p(y | x) \\
&=-\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(x)-\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(y | x) \\
&=-\sum_{x \in \mathcal{X}} p(x) \log p(x)-\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(y | x) \\
&=H(X)+H(Y | X)
\end{aligned}
$
\normalsize
%\end{equation*}

\lz

n-Variable version:
$$H\left(X_{1}, X_{2}, \ldots, X_{n}\right)=\sumin H\left(X_{i} | X_{i-1}, \ldots, X_{1}\right).$$


%\log p(X, Y)=\log p(X)+\log p(Y | X)

% \textbf{Remarks:}
% \begin{itemize}
% \item From the proof follows that: $H(X, Y | Z)=H(X | Z)+H(Y | X, Z)$
% \item Note that $H(Y | X) \neq H(X | Y) ,$ although $H(X)-H(X | Y)=$
% $H(Y)-H(Y | X).$
% \end{itemize}
% \normalsize

  % \begin{itemize}
  %   \item The \textbf{chain rule for entropy} is analogous to the chain rule for probability and, in fact, derives directly from it.
  %   \item Using $p(x,y) = p(x|y)p(y) = p(y|x)p(x)$, the \enquote{self-information chain rule} is:
  % \end{itemize}
  %   \begin{equation*}
  %     \begin{array}{c}{-\log _{2} p(x, y)=-\log _{2} p(x | y)-\log _{2} p(y)=-\log _{2} p(y | x)-\log _{2} p(x)} \\ {I(x, y)=I(x | y)+I(y)=I(y | x)+I(x)}
  %     \end{array}
  % \end{equation*}
  % \begin{itemize}
  %   \item Taking the expectation, we arrive at the chain rule:
  % 
  %  \begin{equation*}
  %    \begin{aligned} 
  %      \mathbb{E}_{X, Y}[I(X, Y)] &=\mathbb{E}_{X, Y}[I(X | Y)]+\mathbb{E}_{X, Y}[I(Y)] \\ &=\mathbb{E}_{X, Y}[I(Y | X)]+\mathbb{E}_{X, Y}[I(X)] \Longleftrightarrow \\ H(X, Y) &=H(X | Y)+H(Y) \\ &=H(Y | X)+H(X)
  %    \end{aligned}
  %  \end{equation*}
  %  where $H(X | Y)$ and $H(Y | X)$ are the \textbf{conditional entropies.}
  % \end{itemize}
\end{vbframe}

% \begin{vbframe} {Chain rule for entropy (n variables)}


% For the case of n variables, let $X_{1}, X_{2}, \ldots, X_{n}$ be drawn according to $p\left(x_{1}, x_{2}, \ldots, x_{n}\right).$ Then

% $$H\left(X_{1}, X_{2}, \ldots, X_{n}\right)=\sumin H\left(X_{i} | X_{i-1}, \ldots, X_{1}\right).$$

% \textbf{Proof:$\quad$} By repeated application of the two-variable expansion rule for entropies, we have

% \footnotesize
% \begin{equation*}
% \begin{aligned}
% H\left(X_{1}, X_{2}\right) &=H\left(X_{1}\right)+H\left(X_{2} | X_{1}\right) \\
% H\left(X_{1}, X_{2}, X_{3}\right) &=H\left(X_{1}\right)+H\left(X_{2}, X_{3} | X_{1}\right)
% \\
% \vdots \\
% &=H\left(X_{1}\right)+H\left(X_{2} | X_{1}\right)+H\left(X_{3} | X_{2}, X_{1}\right)\\
% H\left(X_{1}, X_{2}, \ldots, X_{n}\right) &=H\left(X_{1}\right)+H\left(X_{2} | X_{1}\right)+\cdots+H\left(X_{n} | X_{n-1}, \ldots, X_{1}\right) \\
% &=\sumin H\left(X_{i} | X_{i-1}, \ldots, X_{1}\right).
% \end{aligned}
% \end{equation*}
% \normalsize
% \end{vbframe}

\begin{vbframe} {Joint and Conditional entropy}

The following relations hold:

\begin{equation*}
\begin{aligned}
H(X, X)       &= H(X)  \\
H(X | X)      &= 0  \\
H(X, Y | Z)   &=H(X | Z)+H(Y | X, Z)\\
\end{aligned}
\end{equation*}

Which can all be trivially derived from the previous considerations.

\lz

Furthermore, if $H(X|Y) = 0$, then $X$ is a function of $Y$, so for all $y$ with $p(y)>0$, there is only one $x$ with $p(x,y)>0$. 
Proof is not hard, but also not completely trivial.
\end{vbframe}

\begin{vbframe} {Mutual information}

%The \textbf{relative entropy} $D(p||q)$ (or Kullback-Leibler Divergence) is a measure of the inefficiency of assuming that the distribution is $q$ when the true distribution is $p$: 
% 
% \footnotesize
% \begin{equation*}\begin{aligned}
% D(p \| q) &=\sum_{x \in \Xspace} p(x) \log \frac{p(x)}{q(x)} 
% =\E_{p} \log \frac{p(X)}{q(X)}
% \end{aligned}\end{equation*}
% \normalsize

\begin{itemize}
\item The MI describes the amount of information about one random variable obtained through the other one or how different the joint distribution is from pure independence.
\item Consider two random variables $X$ and $Y$ with a joint probability mass function $p(x, y)$ and marginal probability mass functions $p(x)$ and $p(y)$. The MI $I (X;Y)$ is the Kullback-Leibler Divergence between the joint distribution and the product distribution $p(x)p(y)$:
\footnotesize
\begin{equation*}\begin{aligned}
I(X ; Y) &=\sum_{x \in \Xspace} \sum_{y \in \Yspace} p(x, y) \log \frac{p(x, y)}{p(x) p(y)} \\
&=D_{KL}(p(x, y) \| p(x) p(y)) \\
&=\E_{p(x, y)} \left[ \log \frac{p(X, Y)}{p(X) p(Y)} \right].
\end{aligned}\end{equation*}
\normalsize

\item For two continuous random variables with joint density $f(x,y)$:

\footnotesize
\begin{equation*}\begin{aligned}
I(X ; Y) &= \int f(x,y) \log \frac{f(x,y)}{f(x)f(y)} dx dy.
\end{aligned}
\end{equation*}
\normalsize

\end{itemize}

\end{vbframe}

\begin{vbframe} {Mutual information}

We can rewrite the definition of mutual information $I(X;Y)$ as

\begin{equation*}\begin{aligned}
I(X ; Y) &=\sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)} \\
&=\sum_{x, y} p(x, y) \log \frac{p(x | y)}{p(x)} \\
&=-\sum_{x, y} p(x, y) \log p(x)+\sum_{x, y} p(x, y) \log p(x | y) \\
&=-\sum_{x} p(x) \log p(x)-\left(-\sum_{x, y} p(x, y) \log p(x | y)\right) \\
&=H(X)-H(X | Y).
\end{aligned}\end{equation*}

Thus, mutual information $I(X;Y)$ is the reduction in the uncertainty
of $X$ due to the knowledge of $Y$.

\end{vbframe}

\begin{vbframe} {Mutual information}

The following relations hold:

\begin{equation*}
\begin{aligned}
I(X ; Y) &= H(X) - H(X | Y) \\
I(X ; Y) &= H(Y) - H(Y | X) \\
I(X ; Y) &\leq \min\{H(X),H(Y)\} \\
I(X ; Y) &= H(X) + H(Y) - H(X, Y) \\
I(X ; Y) &= I(Y ; X) \\
I(X ; X) &= H(X)\\
\end{aligned}
\end{equation*}

All of the above are trivial to prove.

% The mutual information of $X$ and $Y$, $I(X;Y)$, corresponds to the intersection of the
% information in $X$ with the information in $Y$.

\end{vbframe}

\begin{vbframe} {Mutual information - example}

Let $X, Y$ have the following joint distribution:

\begin{table}[]
  \begin{tabular}{c|c|c|c|c|}
    & $X_1$ & $X_2$ & $X_3$ & $X_4$ \\ 
    \hline
    $Y_1$ & $\frac{1}{8}$ & $\frac{1}{16}$ & $\frac{1}{32}$ & $\frac{1}{32}$ \\
    \hline
    $Y_2$ & $\frac{1}{16}$ & $\frac{1}{8}$ & $\frac{1}{32}$ & $\frac{1}{32}$ \\
    \hline
    $Y_3$ & $\frac{1}{16}$ & $\frac{1}{16}$ & $\frac{1}{16}$ & $\frac{1}{16}$ \\
    \hline
    $Y_4$ & $\frac{1}{4}$ & 0 & 0 & 0 \\
    \hline
  \end{tabular}
\end{table}

\lz

The marginal distribution of $X$ is $(\frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{8})$ and the marginal distribution of $Y$ is $(\frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4})$, and hence $H(X) = \frac{7}{4}$ bits and $H(Y) = 2$ bits.

\framebreak

The conditional entropy $H(X|Y)$ is given by:

\begin{equation*}
  \begin{aligned}
    H(X|Y) &= \sum_{i = 1}^4 p(Y = i) H(X | Y = i) \\
    &= \frac{1}{4} H \left( \frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{8} \right) +     \frac{1}{4} H \left( \frac{1}{4}, \frac{1}{2}, \frac{1}{8}, \frac{1}{8} \right) \\
    &+ \frac{1}{4} H \left( \frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4} \right) +     \frac{1}{4} H \left(1,0,0,0 \right) \\
    &=  \frac{1}{4} \cdot \frac{7}{4} + \frac{1}{4} \cdot \frac{7}{4} + \frac{1}{4} \cdot     2 + \frac{1}{4} \cdot 0 \\
    &= \frac{11}{8} \text{ bits}.
  \end{aligned}
\end{equation*}

Similarly, $H(Y|X) = \frac{13}{8}$ bits and $H(X,Y) = \frac{27}{8}$ bits.

\end{vbframe}


\endlecture
\end{document}



