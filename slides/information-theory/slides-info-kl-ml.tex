\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Information Theory
  }{% Lecture title  
    KL for ML
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/normal_distributions.png
  }{
  \item Understand why measuring distribution similarity is important in ML
  \item Understand the advantages of forward and reverse KL
}

\begin{vbframe} {Measuring Distribution Similarity in ML}
\begin{itemize}
    \item Information theory provides tools (e.g., divergence measures) to quantify the similarity between probability distributions
    
    \begin{center}
    \includegraphics[width=0.7\linewidth]{figure/normal_distributions.png}
    \end{center}
    
    \item The most prominent divergence measure is the KL divergence 
\item In ML, measuring (and maximizing) the similarity between probability distributions is a ubiquitous concept, which will be shown in the following.
\end{itemize}
\framebreak
\begin{itemize}
    \item \textbf{Probabilistic model fitting}\\
Assume our learner is probabilistic, i.e., we model $p(y| \mathbf{x})$ (for example, logistic regression, Gaussian process, ...).

\begin{center}
\includegraphics[width=0.5\linewidth]{figure/ftrue.pdf}
\end{center}

We want to minimize the difference between $p(y \vert \mathbf{x})$ and the conditional data generating process $\mathbb{P}_{y\vert\mathbf{x}}$ based on the data stemming from $\mathbb{P}_{y, \mathbf{x}}.$

\lz

Many losses can be derived this way. (e.g., cross-entropy loss)

\end{itemize}

\framebreak

\begin{itemize}
    \item \textbf{Feature selection}
In feature selection, we want to choose features the target strongly depends on. 

\begin{center}
\includegraphics[width=0.9\linewidth]{figure/gaussian_mixture_with_marginals.png}
\end{center}

We can measure dependency by measuring the similarity between $p(\mathbf{x}, y)$ and $p(\mathbf{x})\cdot p(y).$ \\
\lz
We will later see that measuring this similarity with KL  leads to the concept of mutual information.

\end{itemize}

\framebreak

\begin{itemize}
    \item \textbf{Variational inference (VI)}
%Our data can also induce probability distributions: 
By Bayes' theorem it holds that the posterior density $$p(\bm{\theta}\vert \mathbf{X}, \mathbf{y}) = \frac{p(\mathbf{y}|\mathbf{X}, \bm{\theta})p(\bm{\theta})}{\int p(\mathbf{y}|\mathbf{X}, \bm{\theta})p(\bm{\theta})d\bm{\theta}}.$$ However, computing the normaliziation constant $c = \int p(\mathbf{y}|\mathbf{X}, \bm{\theta})p(\bm{\theta})d\bm{\theta}$ analytically is usually intractable.

\begin{center}
\includegraphics[width=0.99\linewidth]{figure/gaussian_mixture_scatter.png}
\end{center}

In VI, we want to fit a density $q_{\bm{\phi}}$ with parameters $\bm{\phi}$ to 
    $p(\bm{\theta}\vert \mathbf{X}, \mathbf{y}).$
%This scenario fundamentally differs from the previous ones because we can now generate samples.

\end{itemize}

\end{vbframe}

\begin{vbframe}{KL divergence}

Divergences can be used to measure the similarity of distributions.\\
\lz For distributions $p, q$ they are defined such that
\begin{enumerate}
    \item $D(p, q) \geq 0,$
    \item $D(p, q) = 0$ iff $p = q.$
    %\item $D(p, p + dp)$ is a positive-definite quadratic form for infinitesimal displacements $dp$ from $p$.
\end{enumerate}
$\Rightarrow$ divergences can be (and often are) non-symmetrical. \\
 \lz
 
If the same measure dominates the distributions $p,q$, we can use KL. \\
For a target distribution $p$ and parametrized distribution $q_{\bm{\phi}}$, we call
\begin{itemize}
    \item $D_{KL}(p \| q_{\bm{\phi}})$ forward KL,
    \item $D_{KL}(q_{\bm{\phi}} \| p)$ reverse KL.
\end{itemize}
\lz
In the following, we highlight some properties of the KL that make it attractive from an ML perspective.

\framebreak

\begin{itemize}
    \item \textbf{Forward KL for probabilistic model fitting}
    \\ We have samples from the DGP $p(y, \xv)$ when we fit our ML model.
    \\
    \lz
    
    If we have a probabilistic ML model $q_{\bm{\phi}}$ the  expected forward KL
    $$\E_{\xv \sim p_{\xv}}D_{KL}(p(\cdot|\xv) \| q_{\bm{\phi}}(\cdot|\xv)) = \E_{\xv \sim p_{\xv}}\E_{y \sim p_{y|\xv}}\log\left(\frac{p(y|\xv)}{q_{\bm{\phi}}(y|\xv)}\right).$$
We can directly minimize this objective since 
\begin{align*}
         \nabla_{\bm{\phi}} \E_{\xv \sim p_{\xv}}D_{KL}(p(\cdot|\xv) \| q_{\bm{\phi}}(\cdot|\xv)) &=  \E_{\xv \sim p_{\xv}}\E_{y \sim p_{y|\xv}}\nabla_{\bm{\phi}}\log\left(
         p(y|\xv)\right) \\
         &- \E_{\xv \sim p_{\xv}}\E_{y \sim p_{y|\xv}}\nabla_{\bm{\phi}}\log\left(q_{\bm{\phi}}(y|\xv)\right) \\
         &= -\nabla_{\bm{\phi}} \E_{\xv \sim p_{\xv}}\E_{y \sim p_{y|\xv}}\log\left(q_{\bm{\phi}}(y|\xv)\right)
     \end{align*}
% Assuming we have i.i.d. observations, an unbiased estimator of this expected forward KL is
% $$\sumin \log\left(\frac{p(\yi|\xi)}{q_{\bm{\phi}}(\yi|\xi)}\right) \Rightarrow \text{can be used for mini-batching.} $$
     $\Rightarrow$ We can estimate the gradient of the expected forward KL without bias, although we can not evaluate $p(y\vert \xv)$ in general. 
\end{itemize}
 \framebreak

 \begin{itemize}
     \item \textbf{Reverse KL for VI} \\
     Here, we know our target density $p(\bm{\theta}\vert \mathbf{X}, \mathbf{y})$ only up to the normalization constant, and we do not have samples from it. \\
     \lz
     We can directly apply the reverse KL since for any $c\in \R_+$
     \begin{align*}
         \nabla_{\bm{\phi}} D_{KL}(q_{\bm{\phi}}\|p) &= \nabla_{\bm{\phi}} \E_{\bm{\theta} \sim q_{\bm{\phi}}}\log\left(\frac{q_{\bm{\phi}}(\bm{\theta})}{p(\bm{\theta})}\right) \\
         &= \nabla_{\bm{\phi}} \E_{\bm{\theta} \sim q_{\bm{\phi}}}\log\left(\frac{q_{\bm{\phi}}(\bm{\theta})}{p(\bm{\theta})}\right) - \underbrace{\nabla_{\bm{\phi}} \E_{\bm{\theta} \sim q_{\bm{\phi}}}\log c}_{=0}\\
         &= \nabla_{\bm{\phi}} \E_{\bm{\theta} \sim q_{\bm{\phi}}}\log\left(\frac{q_{\bm{\phi}}(\bm{\theta})}{c\cdot p(\bm{\theta})}\right).
     \end{align*}
     $\Rightarrow$ We can estimate the gradient of the reverse KL without bias (even if we only have an unnormalized target distribution)
 \end{itemize}
 \framebreak

The asymmetry of the KL has the following implications
\begin{itemize}
    \item Forward KL $D_{KL}(p\|q_{\bm{\phi}}) = \E_{\xv \sim p} \log\left(\frac{p(\xv)}{q_{\bm{\phi}}(\xv)}\right)$ is mass-covering since $p(\xv)\log\left(\frac{p(\xv)}{q_{\bm{\phi}}(\xv)}\right) \approx 0$ if $p(\xv) \approx 0$ and $q_{\bm{\phi}}(\xv) \not\gg p(\xv).$
        \item Reverse KL $D_{KL}(q_{\bm{\phi}}\|p) = \E_{\xv \sim q_{\bm{\phi}}} \log\left(\frac{q_{\bm{\phi}}(\xv)}{p(\xv)}\right)$ is mode-seeking (zero-avoiding) since $q_{\bm{\phi}}(\xv)\log\left(\frac{q_{\bm{\phi}}(\xv)}{p(\xv)}\right) \gg 0$ if $p(\xv) \approx 0$ and $q_{\bm{\phi}}(\xv) > 0$ 
\end{itemize}
 \begin{center}
\includegraphics[width=0.7\linewidth]{figure/kl_fitting_plot.png}
\end{center}
\small Figure: Optimal $q_{\bm{\phi}}$ when $q_{\bm{\phi}}$ is restricted to be Gaussian.
\end{vbframe}

\endlecture
\end{document}
