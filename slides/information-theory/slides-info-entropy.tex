\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Information Theory
  }{% Lecture title  
    Entropy I
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/entropy_plot_reordering.png
  }{
  \item Entropy measures expected information for discrete RVs
  \item Know entropy and its properties
}

\begin{vbframe}{Information Theory}

\begin{itemize}
  \item \textbf{Information Theory} is a field of study based on probability theory.
  \item Foundation was laid by Claude Shannon in 1948; since then been applied in: communication theory, computer science, optimization, cryptography, machine learning and statistical inference.

  \item Quantify the "amount" of information gained or 
    uncertainty reduced when a random variable is observed.

  \item Also about storing and transmitting information.
\end{itemize}

  \begin{center}
\includegraphics[width = 0.4\textwidth]{figure_man/claude-shannon.jpg} \\
\end{center}

\framebreak

\lz
\begin{itemize}
\setlength\itemsep{1.2em}
  \item We introduce the basic concepts from a probabilistic perspective, without referring too much to communication, channels or coding.
  \item We will show some proofs, but not for everything. We recommend 
    \textit{Elements of Information Theory} by Cover and Thomas as a reference for more. 
  \item The application of information theory to the concepts of statistics and ML can sometimes be confusing, we will try to make the connection as clear as possible.
  \item In this unit we develop entropy as a measure of uncertainty in terms of expected information.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Entropy as Surprisal and Uncertainty}
%\begin{itemize}
  %\item Entropy is often introduced in IT as a measure of
  %  expected information or in terms of bits needed for efficient coding, 
  %but for us in stats and ML the first type of intuition seems most useful.
%\end{itemize}


For a discrete random variable $X$ with domain $\Xspace \ni x$ and pmf $p(x)$:
\begin{equation*}
\begin{aligned} 
  H(X) := H(p) &= - \E[\log_2(p(X))]           &= -\sum_{x \in \Xspace} p(x) \log_2 p(x)\\ 
               &=   \E\left[\log_2\left(\frac{1}{p(X)}\right)\right] &= \sum_{x \in \Xspace} p(x) \log_2 \frac{1}{p(x)} 
\end{aligned} 
\end{equation*}

Some technicalities first:

\begin{itemize}
\setlength\itemsep{1.2em} 
\item $H$ is actually Greek capital letter \textbf{E}ta ($\eta$) for \textbf{e}ntropy
\item Base of the log simply specifies the unit we measure information in, usually bits (base 2) or 'nats' (base $e$)
\item If $p(x) = 0$ for an $x$, then $p(x) \log_2 p(x)$ is taken to be zero,\\
because $\lim _{p \rightarrow 0} p \log_2 p=0$. %for $x=0$.
\end{itemize}

  
\end{vbframe}

\begin{vbframe}{Entropy as Surprisal and Uncertainty}


\begin{equation*}
\begin{aligned} 
  H(X) = - \E[\log_2(p(X))]           &= -\sum_{x \in \Xspace} p(x) \log_2 p(x) 
\end{aligned} 
\end{equation*}

Now: What's the point?
\begin{itemize}
\item The negative log probabilities $-\log_2 p(x)$ are called "surprisal"
\item More surprising means less likely
\item PMFs surprising, so with higher H, when events more equally likely
\item Entropy is simply expected surprisal
\end{itemize}


\begin{center}
\includegraphics[width = 10cm ]{figure/entropy_calc.png} 
\end{center}
\vspace{-0.5cm}
\begin{itemize}
\item The final entropy is $H(X)=2.12$ (bits).
\end{itemize}


\end{vbframe}

\begin{vbframe}{Entropy Basic Properties}

$$H(X) := H(p) = - \E[\log_2(p(X))] = -\sum_{x \in \Xspace} p(x) \log_2 p(x)$$

\vspace{0.2cm}
  \begin{enumerate}
  \setlength\itemsep{1.2em} 
    \item Entropy is non-negative, so $H(X) \geq 0$    \item If one event has probability $p(x) = 1$, then $H(X)=0$
    \item Adding or removing an event with $p(x)=0$ doesn't change it
    \item $H(X)$ is continuous in probabilities $p(x)$
  \end{enumerate}
\vspace{0.2cm}  
All these properties follow directly from the definition.


\end{vbframe}

\begin{vbframe}{Entropy Re-Ordering}

\begin{enumerate}
\setcounter{enumi}{4}
   \item Symmetry. If the values $p(x)$ in the pmf are re-ordered, entropy does not change (proof is trivial).
\end{enumerate}
    
\begin{center}
\includegraphics[width = 10cm ]{figure/entropy_plot_reordering.png} \\
\end{center}

\end{vbframe}
  
\begin{vbframe}{Entropy of Uniform Distributions}

Let $X$ be a uniform, discrete RV with $g$ outcomes ($g$-sided fair die).

$$H(X) = - \sum_{i=1}^g \frac{1}{g} \log_2 \left(\frac{1}{g}\right) = \log_2 g$$

\vspace{0.2cm}
\begin{center}
\includegraphics[width = 0.8\textwidth]{figure/entropy_uniform_plot.png}
\end{center}

The more sides a die has, the harder it is to predict the outcome. 
Unpredictability grows \textit{monotonically} with the number of potential outcomes, but at a decreasing rate.
\end{vbframe}


\begin{vbframe}{Entropy is Maximal for Uniform}

\begin{center}
\includegraphics[width = 0.9\textwidth]{figure/max_entropy.png}
\end{center}

\begin{itemize}
   \item Naive observation:\\
   Entropy min for 1-point and max for uniform distribution
\end{itemize}

\end{vbframe}

\begin{vbframe}{Entropy is Maximal for Uniform}
\begin{enumerate}
\setcounter{enumi}{5}
\item Entropy is maximal for a uniform distribution,\\
      for domain of size $g$:  
      $H(X) \leq -g\frac{1}{g} \log_2(\frac{1}{g}) = log_2(g)$.
\end{enumerate}
\vspace{0.2cm}
%\textbf{Claim}: The entropy of a discrete random variable $X$ which takes on values in $\{x_1,x_2, \ldots, x_g\}$ with associated probabilities $\{p_1,p_2, \ldots, p_g\}$ is maximal when the distribution over $X$ is uniform.

\lz
\textbf{Proof}: 
So we want to maximize w.r.t. all $p_i$:

  $$\underset{p_{1}, p_{2}, \ldots, p_{g}}{\operatorname{argmax}} -\sum_{i=1}^{g} p_{i} \log _{2} p_{i}$$
  subject to
  $$\sum_{i=1}^g p_i = 1$$
  
  \framebreak
  The Lagrangian $L(p_1, \ldots, p_g, \lambda)$ is :
  $$L(p_1, \ldots, p_g, \lambda) = - \sum_{i=1}^g p_i \log_2(p_i) - \lambda \left( \sum_{i=1}^g p_i - 1 \right)$$
  
  Solving when requiring $\nabla L = 0$,
  \begin{gather*}
    \frac{\partial L(p_1, \ldots, p_g, \lambda)}{\partial p_i} = 0 = - \log_2(p_i) - \frac{1}{\log(2)} - \lambda \\
    \implies p_i = \frac{2^{- \lambda}}{e} \implies p_i = \frac{1}{g},
  \end{gather*}
  last step follows from that all $p_i$ are equal and constraint
  \vspace{0.2cm}\\
  \textbf{NB}: We also could have solved the constraint for $p_1$ and substitute $p_1=1-\sum_{i=2}^{g} p_i$ in the objective to avoid constrained optimization.

\end{vbframe}

\endlecture
\end{document}

