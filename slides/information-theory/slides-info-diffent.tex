\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Information Theory
  }{% Lecture title  
    Differential Entropy
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/diffent-quant.png
  }{
  \item Know that the entropy expresses expected information for continuous RVs
  \item Know the basic properties of the differential entropy
}

\begin{vbframe}{Differential Entropy} 
  \begin{itemize}
    \item For a continuous random variable $X$ with density function $f(x)$ and support $\Xspace$, the analogue of entropy is \textbf{differential entropy}:
    \small{
    $$ h(X) := h(f) := - \mathbb{E}[\log(f(x))]= - \int_{\Xspace} f(x) \log(f(x)) dx $$}
    \item The base of the log is again somewhat arbitrary, and we could either use 2 (and measure in bits) or e (to measure in nats).
    \item The integral above does not necessarily exist for all densities.
    \item Differential entropy lacks the non-negativeness of discrete entropy: $h(X) < 0$ is possible as $f(x) > 1$ is possible:
    \end{itemize}
\begin{minipage}{0.8\textwidth}
    \begin{center}
    \includegraphics[width = 0.95\textwidth]{figure/beta_entropy.png}
    \end{center}
\end{minipage}%
\begin{minipage}{0.2\textwidth}
  \scriptsize{The diffent. is given by the integral:\\ $h(X)=-0.48$.}
\end{minipage}
\end{vbframe}

\begin{vbframe}{Diff. Entropy of Uniform Distribution}
Let $X$ be a uniform random variable on $[0, a]$.
  \begin{equation*}
    \begin{aligned} 
      h(X) &= - \int_0^a f(x) \log(f(x)) dx \\
           &= - \int_0^a \frac{1}{a} \log\left(\frac{1}{a}\right) dx = \log(a) 
    \end{aligned}
  \end{equation*}
  \begin{itemize}
    \item For $a < 1$, $h(X) < 0$.
    \end{itemize}
    
    \begin{center}
    \includegraphics[width = 8cm ]{figure/uni_entropy.png}
    \end{center}
    
\end{vbframe}


\begin{vbframe}{Diff. Entropy of Gaussian}
Let $X \sim \normal(\mu, \sigma^2)$ and let us measure in nats:
\vspace{-0.2cm}
% The differential entropy is then given by
{\small
  \begin{equation*}
    \begin{aligned} 
     h(X) &= - \int_{\R} f(x) \log(f(x)) dx =  - \int_{\R} f(x) \log\left(\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}} \right) dx \\
          &= - \int_{\R} f(x) \log\left(\frac{1}{\sqrt{2 \pi \sigma^{2}}}\right) dx + \int_{\R} f(x) \frac{(x-\mu)^{2}}{2 \sigma^{2}} dx \\
          &= - \log \left(\frac{1}{\sqrt{2 \pi \sigma^{2}}}\right) \underbrace{\int_{\R} f(x) dx}_{= 1} + \frac{1}{2\sigma^2} \underbrace{\int_{\R} f(x) (x-\mu)^{2}\,dx}_{=: \sigma^2} \\
          &= \frac{1}{2} \log \left(2 \pi \sigma^{2}\right) + \frac{1}{2} =  \log(\sigma \sqrt{2\pi e})
    \end{aligned}
  \end{equation*}
  } \vspace{-0.5cm}
\begin{center}
    \includegraphics[width = 0.72\textwidth]{figure/normal_entropy.png}
\end{center}
\end{vbframe}


\begin{vbframe}{Diff. Entropy of Gaussian}
$$h(X) = - \int_{\R} f(x) \log(f(x)) dx = \log(\sigma \sqrt{2\pi e})$$
\vspace{-0.2cm}
  \begin{itemize}
    \item $h(X)$ is not a function of $\mu$ (see translation invariance later).
    \item As $\sigma^2$ increases, the differential entropy also increases.
    \item For $\sigma^2 < \frac{1}{2\pi e}\approx 0.059$, it is negative.
    \vspace{0.1cm}
    \begin{center}
    \includegraphics[width = 0.6\textwidth]{figure/normal_entropy_sigma.png}
    \end{center}
  \end{itemize}
\end{vbframe}


\begin{vbframe}{Diff. Entropy vs. Discrete}
It is not so simple as to characterize $h(X)$ as 
a straightforward generalization of $H(X)$ of a limiting process.
Consider the quantized random variable $X^\Delta$, which is defined by
$$X^\Delta = x_i \qquad \text{ if } \qquad i \Delta \leq X < (i + 1) \Delta$$  
\begin{center}
\includegraphics[width = 7cm ]{figure_man/diffent-quant.png} \\
\end{center}

If the density $f(x)$ of the random variable $X$ is Riemann-integrable, then
$$H(X^\Delta) + \log(\Delta) \rightarrow h(X) \text{ as } \Delta \rightarrow 0.$$
Thus, the entropy of an n-bit quantization of a continuous random variable 
$X$ is approximately $h(X) + n$.
\end{vbframe}

\begin{vbframe}{Joint Differential Entropy} 
\begin{itemize}
\item For a continuous random vector $X$ with density function $f(x)$ and support $\Xspace$, 
  differential entropy is also defined as:
$$ h(X) = h(X_1, \ldots, X_n) = h(f) = - \int_{\Xspace} f(x) \log(f(x)) dx $$
\item Hence this also defines the joint differential entropy for a set of continuous RVs.
\end{itemize}

\lz
  
Entropy of a multivariate normal distribution: 
If $X \sim N(\mu, \Sigma)$ is multivariate Gaussian, then
  $$h(X) = \frac{1}{2} \log(2 \pi e)^n |\Sigma| \qquad \text{(nats)}$$
\end{vbframe}

\begin{vbframe}{Properties of Differential Entropy} 
\begin{enumerate}
  \item $h(f)$ can be negative.
  \item $h(f)$ is additive for independent RVs.
  \item $h(f)$ is maximized by the multivariate normal, if we restrict 
    to all distributions with the same (co)variance, so
    $h(X) \leq \frac{1}{2} \log(2 \pi e)^n |\Sigma|.$
    % We postpone the proof to a later chapter, as it is based on Kullback-Leibler divergence.
    % $H(X) \leq -g\frac{1}{g} \log_2(\frac{1}{g}) = log_2(g)$.
    \item $h(f)$ is maximized by the continuous uniform distribution for a random variable with a fixed range.
\item Translation-invariant, $ h(X+a) = h(X)$. 
\item $h(aX) = h(X) + \log |a|$.
\item $h(AX) = h(X) + \log |A|$ for random vectors and matrix A.
\end{enumerate}
\lz
3) and 4) are slightly involved to prove, while the other properties are relatively straightforward to show

\end{vbframe}

\endlecture
\end{document}
