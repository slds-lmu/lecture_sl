\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure/l2_reg_hess_03_plot.png}
\newcommand{\learninggoals}{
  \item Have a geometric understanding of $L2$ regularization
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Geometric Analysis of L2 Regularization and Weight Decay}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Geometric Analysis of $L2$ Regularization}

Weight decay can be interpreted \textbf{geometrically}. 

\lz 

Let's use a quadratic Taylor approximation of the unregularized objective $\risket$ in the neighborhood of its minimizer $\thetah$,  

$$ \mathcal{\tilde R}_{\text{emp}}(\thetab)= \mathcal{R}_{\text{emp}}(\thetah) + \nabla_{\thetab} \mathcal{R}_{\text{emp}}(\thetah)\cdot(\thetab - \thetah) + \ \frac{1}{2} (\thetab - \thetah)^T \bm{H} (\thetab - \thetah), $$

where $\bm{H}$ is the Hessian matrix of $\risket$ evaluated at $\thetah$. 

\lz

% Because $\thetah = \argmin_{\thetab}\risket$,
\begin{itemize}
  \item The first-order term is 0 in the expression above because the gradient is $0$ at the minimizer.
  \item $\bm{H}$ is positive semidefinite, because we are at the minimizer.
\end{itemize}

\lz

\framebreak

\normalsize

The minimum of $\mathcal{\tilde R}_{\text{emp}}(\thetab)$ occurs where $\nabla_{\thetab}\mathcal{\tilde R}_{\text{emp}}(\thetab) = \bm{H}(\thetab - \thetah)$ is $0$.

Now we $L2$-regularize $\mathcal{\tilde R}_{\text{emp}}(\thetab)$, such that 
\[
\mathcal{\tilde R}_{\text{reg}}(\thetab) = \mathcal{\tilde R}_{\text{emp}}(\thetab) + \frac{\lambda}{2} \|\thetab\|^2_2\]
and solve this approximation of $\riskr$ for the minimizer $\hat{\thetab}_{\text{ridge}}$:
\begin{align*}
 \nabla_{\thetab}\mathcal{\tilde R}_{\text{reg}}(\thetab) = 0,\\
%  \lambda \thetab + \nabla_{\thetab}\mathcal{\tilde R}_{\text{emp}}(\thetab) = 0,\\
  \lambda \thetab + \bm{H}(\thetab - \thetah) = 0,\\
      (\bm{H} + \lambda \id) \thetab = \bm{H} \thetah,\\
      \hat{\thetab}_{\text{ridge}} = (\bm{H} + \lambda \id)^{-1}\bm{H} \thetah,
\end{align*}

% where $\id$ is the identity matrix.
This gives us a formula to see how the minimizer of the $L2$-regularized version is a transformation of the minimizer of the unpenalized version.
\vspace{0.2cm}



\framebreak

  \begin{itemize}
    \item As $\lambda$ approaches $0$, the regularized solution $\hat{\thetab}_{\text{ridge}}$ approaches $\thetah$. What happens as $\lambda$ grows?
    \item Because $\bm{H}$ is a real symmetric matrix, it can be decomposed as $\bm{H} = \bm{Q} \bm{\Sigma} \bm{Q}^\top$, where $\bm{\Sigma}$ is a diagonal matrix of eigenvalues and $\bm{Q}$ is an orthonormal basis of eigenvectors.
    \item Rewriting the transformation formula with this:
  \begin{equation*}
    \begin{aligned} 
    \hat{\thetab}_{\text{ridge}} &=\left(\bm{Q} \bm{\Sigma} \bm{Q}^{\top}+\lambda \id\right)^{-1} \bm{Q} \bm{\Sigma} \bm{Q}^{\top} \thetah \\ 
              &=\left[\bm{Q}(\bm{\Sigma}+\lambda \id) \bm{Q}^{\top}\right]^{-1} \bm{Q} \bm{\Sigma} \bm{Q}^{\top} \thetah \\ 
              &=\bm{Q}(\bm{\Sigma} + \lambda \id)^{-1} \bm{\Sigma} \bm{Q}^{\top} \thetah 
    \end{aligned}
  \end{equation*}
    \item Therefore, weight decay rescales $\thetah$ along the axes defined by the eigenvectors of $\bm{H}$. The component of $\thetah$ that is aligned with the $j$-th eigenvector of $\bm{H}$ is rescaled by a factor of $\frac{\sigma_j}{\sigma_j + \lambda}$, where $\sigma_j$ is the corresponding eigenvalue.
\end{itemize}

\framebreak

% \begin{footnotesize}
Firstly, $\thetah$ is rotated by $\bm{Q}^{\top}$, which we can interpret as a projection of $\thetah$ on the rotated coordinate system defined by the principal directions of $\bm{H}$:
% \end{footnotesize}

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure/l2_reg_hess_01_plot.png}\\
\end{figure}

\framebreak

% \begin{footnotesize}
Since, for $\lambda = 0$, the transformation matrix $(\bm{\Sigma} + \lambda \id)^{-1} \bm{\Sigma} = \bm{\Sigma}^{-1} \bm{\Sigma} = \id$, we simply arrive at $\thetah$ again after projecting back.
% \end{footnotesize}

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure/l2_reg_hess_02_plot.png}\\
\end{figure}


\framebreak
  
% \begin{footnotesize}
If $\lambda > 0$, the component projected on the $j$-th axis gets rescaled by $\frac{\sigma_j}{\sigma_j + \lambda}$ before $\hat{\thetab}_{\text{ridge}}$ is rotated back.
% \end{footnotesize}

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure/l2_reg_hess_03_plot.png}\\
\end{figure}


\framebreak

  
\begin{itemize} 
  \item Along directions where the eigenvalues of $\bm{H}$ are relatively large, for example, where $\sigma_j >> \lambda$, the effect of regularization is quite small.
  \item On the other hand, components with $\sigma_j << \lambda$ will be shrunk to have nearly zero magnitude.
  \item In other words, only directions along which the parameters contribute significantly to reducing the objective function are preserved relatively intact.
  \item In the other directions, a small eigenvalue of the Hessian means that moving in this direction will not significantly increase the gradient. For such unimportant directions, the corresponding components of $\thetab$ are decayed away.
  \end{itemize}
  
  \framebreak
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      %\textbf{Left Column}
      
      {\scriptsize
      \begin{itemize}\setlength{\itemsep}{1.0em}
          \item
          In the direction corresponding to smaller eigenvalue of Hessian of $\risket$, the objective function does not increase much when moving away from $\thetah$. Therefore, the regularizer has a strong effect on this direction and towards this, $\theta$ is pulled close to the origin.    
          \item In the second direction, the corresponding eigenvalue is large indicating high curvature. The objective function is very sensitive to movement along this direction and, as a result, the position of $\theta$ towards this is less affected by the regularization.
        \end{itemize}
      }
      \end{column}
    
      \begin{column}{0.5\textwidth}
      %\textbf{Right Column}
      
      \begin{figure}
            \centering
              \scalebox{0.9}{\includegraphics{figure/wd-l2-geom.png}}
              \caption{\footnotesize The solid ellipses represent the contours of the unregularized objective and the dashed circles represent the contours of the $L2$ penalty. At $\hat{\thetab}_{\text{ridge}}$, the competing objectives reach an equilibrium.}
          \end{figure}
      
      \end{column}
      
    \end{columns}

\end{vbframe}

\endlecture
\end{document}

