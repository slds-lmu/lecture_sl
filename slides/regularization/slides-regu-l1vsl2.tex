\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{Regularization}{Lasso vs. Ridge}{figure_man/l1_l2_hat.png}{
  \item Properties of ridge vs. lasso
  \item Coefficient paths
  \item What happens with corr. features
  \item Why we need feature scaling
}


\begin{vbframe}{Lasso vs. ridge Geometry}
$$ 
  \min_{\thetab} \sumin \left(\yi - \fxit\right)^2 \qquad \text{ s.t. } \|\thetab\|_p^p  \leq t 
$$ 
  \vspace{-0.5cm}
  \begin{figure}
    \centering
      \scalebox{0.5}{\includegraphics{figure_man/l1_l2_hat.png}}
  \end{figure}

  \begin{itemize}
    \item \small{In both cases (and for sufficiently large $\lambda$), the solution which minimizes $\riskrt$ is always a point on the boundary of the feasible region.
    \item As expected, $\hat{\thetab}_{\text{lasso}}$ and $\hat{\thetab}_{\text{ridge}}$ have smaller parameter norms than $\thetah$.}
    \item For lasso, solution likely touches a vertex of constraint region. \\
        Induces sparsity and is a form of variable selection.
    \item For $p>n$: lasso selects at most $n$ features \citelink{ZOUHASTIE}.
    
  \end{itemize}
  
\end{vbframe}

\begin{vbframe}{Coefficient Paths and 0-Shrinkage}

\textbf{Example 1: Motor Trend Car Roads Test (mtcars)} \\

%We cannot overfit here with an unregularized linear model as the task is so low-dimensional. But 
We see how only lasso shrinks to exactly 0.

\begin{figure}
\includegraphics[width=0.8\textwidth]{figure_man/l1_l2_regupaths_mse.pdf}\\
\end{figure}
\vspace{-0.3cm}
% Coef paths and cross-val. MSE for $\lambda$ values for ridge and lasso.\\
NB: No real overfitting here, as data is so low-dim.

\framebreak
\textbf{Example 2: High-dim., corr. simulated data: $p=50$; $n=100$}
$$ y = 10 \cdot (x_1 + x_2) + 5 \cdot (x_3 + x_4) + 1 \cdot \sum_{j = 5}^{14} x_j + \epsilon $$
36/50 vars are noise; $\epsilon \sim \normal \left(0, 1\right)$; $\xv \sim \normal \left(\mathbf{0}, \Sigma \right)$; 
$\Sigma_{k,l}=0.7^{|k-l|}$ 


%Coefficient histograms for different $\lambda$ values for ridge and lasso for simulated data along with the cross-validated MSE.

\begin{figure}
\includegraphics[width=0.6\textwidth]{figure/shrinkage_2.png}\\
\end{figure}

\end{vbframe}

\begin{vbframe}{Regularization and Feature Scaling}

  \begin{itemize}
    \item Typically we omit $\theta_0$ in penalty $J(\thetab)$ so that the ``infinitely'' regularized model is the constant model (but can be implementation-dependent).
    \item Unregularized LM has \textbf{rescaling equivariance}, if you scale some features, can simply "anti-scale" coefs and risk does not change.
    \item Not true for Reg-LM: if you down-scale features, coeffs become larger to counteract. They are then penalized stronger in $J(\thetab)$, making them less attractive without any relevenat reason.
    \item \textbf{So: usually standardize features in regularized models, whether linear or non-linear!}  
      
    % \item While ridge regression usually leads to smaller estimated coefficients, but still dense $\thetab$ vectors,
    %   the lasso will usually create a sparse $\thetab$ vector and can therefore be used for variable selection.
    %\item SVMs combine (usually) hinge loss with L2-regularization. But also for SMVs this concept is generalized to different losses and different penalties.
  \end{itemize}

\framebreak

%\textbf{Example:}\\

\footnotesize{
\begin{itemize}
    \item Let the DGP be $y = \sum_{j=1}^{5} \theta_j x_{j} +\varepsilon$ for $\thetab=(1,2,3,4,5)^\top$, $\varepsilon \sim \mathcal{N}(0,1)$ %and $n=100$
    \item Suppose $x_5$ was measured in $m$ but we change the unit to $cm$ ($\Tilde{x}_5=100 \cdot x_5$):
\end{itemize}
\vspace{-0.4cm}
\begin{table}[h]
\centering
\begin{tabular}{|c|c c c c c c|}
\hline
\textbf{Method} & \( \hat{\theta}_1 \) & \( \hat{\theta}_2 \) & \( \hat{\theta}_3 \) & \( \hat{\theta}_4 \) & \( \hat{\theta}_5 \) & MSE \\ \hline
OLS             & 0.983 & 2.147 & 3.005 & 3.917 & \textbf{5.204} & 0.812 \\ %\hline
OLS rescaled    & 0.983 & 2.147 & 3.005 & 3.917 & \textbf{0.052} & 0.812 \\ \hline
\end{tabular}
%\caption{Equivariant OLS estimates under rescaling of $x_5$}
\end{table}
\vspace{-0.1cm}
\begin{itemize}
    \item Estimate $\hat{\theta}_5$ gets scaled by $1/100$ while other estimates and MSE are invariant
    \item Running ridge regression with $\lambda=10$ on same data shows that rescaling of of $x_5$ does not result in inverse rescaling of $\hat{\theta}_5$ (everything changes!)
    \item This is because $\hat{\theta}_5$ now lives on small scale while $L2$ constraint stays the same. Hence remaining estimates can ``afford'' larger magnitudes.
\end{itemize}
\vspace{-0.4cm}
\begin{table}[h]
\centering
\begin{tabular}{|c|c c c c c c|}
\hline
\textbf{Method} & \( \hat{\theta}_1 \) & \( \hat{\theta}_2 \) & \( \hat{\theta}_3 \) & \( \hat{\theta}_4 \) & \( \hat{\theta}_5 \) & MSE \\ \hline
ridge           & 0.709 & 1.873 & 2.661 & 3.557 & \textbf{4.636} & 1.366 \\ %\hline
ridge rescaled  & 0.802 & 1.942 & 2.675 & 3.569 & \textbf{0.051} & 1.079 \\ \hline
\end{tabular}
%\caption{ridge estimates for $\lambda=10$ under rescaling of $x_5$}
\end{table}
}

\begin{itemize}
    \item For lasso, especially for very correlated features, we could arbitrarily force a feature out of the model through a unit change.
\end{itemize}

\framebreak

\begin{comment}
\textbf{Example:}
\begin{itemize}
\item Let the true data generating process be
$$ y = x_1 + \epsilon, \quad \epsilon \sim \normal \left(0, 1\right).$$
\item Let there be 5 features $x_1, \ldots, x_5 \sim \normal (0,1)$.
\item Using the lasso (package \texttt{glmnet}), we get
\footnotesize
\vspace{0.2cm}

\begin{table}[]
\begin{tabular}{lllll}
(Intercept) & x1    & x2    & x3    & x4    \\
-0.056      & 0.489 & 0.000 & 0.000 & 0.000 
\end{tabular}
\end{table}


\normalsize
\item But if we rescale any of the noise features, say $x_2 = 10000 \cdot x_2$ and don't use standardization, we get
\footnotesize
\vspace{0.2cm}

\begin{table}[]
\begin{tabular}{lllll}
(Intercept) & x1       & x2        & x3       & x4       \\
-0.106830   & 0.000000 & -0.000013 & 0.000000 & 0.000000         
\end{tabular}
\end{table}

\normalsize

\item This is due to the fact, that the coefficient of $x_2$ will live on a very small scale as the covariate itself is large. The feature will thus get less penalized by the $L1$-norm and is favored by lasso.
\end{itemize}
\end{comment}

\end{vbframe}

\begin{vbframe}{Correlated Features: $L1$ vs $L2$}

% \textbf{Example:} lasso vs ridge for two highly correlated variables ($X_4, X_5$). 

Simulation with $n=100$: 
$$y = 0.2x_1 + 0.2x_2 + 0.2x_3 + 0.2x_4 + 0.2x_5 + \epsilon$$
$x_1$-$x_4$ are independent, but $x_4$ and $x_5$ are strongly correlated.

\begin{center}
\includegraphics[width=0.6\textwidth]{figure/regu_example_multicollinearity.png}
\end{center}


\begin{itemize}
\item L1 removes $x_5$ early, L2 has similar coeffs for $x_4, x_5$ for larger $\lambda$
\item Also called ``grouping property'': for ridge highly corr. features tend to have equal effects; lasso however ``decides`` what to select
\item L1 selection is somewhat ``arbitrary''
\end{itemize}

\framebreak

\textbf{More detailed answer}: The ``random'' decision is in fact a complex deterministic interaction of data geometry (e.g., corr. structures), the optimization method, and its hyperparamters (e.g., initialization). The theoretical reason for this behavior relates to the convexity of the penalties \citelink{ZOUHASTIE}.

\vspace{0.1cm}

Considering perfectly collinear features $x_4=x_5$ in the last example, we can obtain some more formal intuition for this phenomenon:
\vspace{0.15cm}
\begin{itemize}
    \item Because $L2$ penalty is \textit{strictly} convex: $x_4=x_5 \implies \thetah_{4, ridge}=\thetah_{5, ridge}$ (grouping prop.)
    %\item Grouping property = highly corr. features tend to have equal effects
    \item $L1$ penalty is not \textit{strictly} convex. Hence no unique solution exists if $x_4=x_5$, and sum of coefficients can be arbitrarily allocated to both features while remaining minimizers (no grouping property!):\\
    For any solution $\thetah_{4,lasso},\thetah_{5,lasso}$, equivalent minimizers are given by 
    {\small $$\Tilde{\theta}_{4,lasso}=s\cdot(\thetah_{4,lasso}+\thetah_{5,lasso}) \,\,\text{and}\,\,\Tilde{\theta}_{5,lasso}=(1-s)\cdot(\thetah_{4,lasso}+\thetah_{5,lasso})\,\forall s\in[0,1]$$}
\end{itemize}

\framebreak



\end{vbframe}


\begin{vbframe}{Summary \citelink{TIBS1996} \citelink{ZOUHASTIE}}

\begin{itemize}
\item Neither ridge nor lasso can be classified as better overall
\item Lasso can shrink some coeffs to zero, so selects features; \\
ridge usually leads to dense solutions, with smaller coeffs
\item Lasso likely better if true underlying structure is sparse \\
ridge works well if there are many (weakly) influential features
\item Lasso has difficulties handling correlated predictors; \\
for high correlation, ridge dominates lasso in performance
\item Lasso: for (highly) correlated predictors, usually an `arbitrary'' one is selected, with large 
coeff, while the other are (nearly) zeroed
\item Ridge: coeffs of correlated features are similar
\end{itemize}

\end{vbframe}

\endlecture
\end{document}
