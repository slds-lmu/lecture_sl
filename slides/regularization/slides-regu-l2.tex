\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure/ridge_outside.png}
\newcommand{\learninggoals}{
  \item Know the regularized linear model
  \item Know ridge regression ($L2$ penalty)
  \item Understand correspondence to constrained optimization
  %\item Know Lasso regression ($L1$ penalty)
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Ridge Regression}
\lecture{Introduction to Machine Learning}


\begin{vbframe}{Regularization in the linear model}

  \begin{itemize} \setlength{\itemsep}{1.3em}
  \item Linear models can also overfit if we operate in a high-dimensional space with not that many observations.    
  \item The OLS estimator requires a full-rank design matrix.
  \item For highly correlated features, OLS becomes highly sensitive to random errors in the observed response, producing a large variance in the fit. 
  \item We now add a complexity penalty to the loss:
  $$
  \riskrt = \sumin \left(\yi - \thetab^\top \xi \right)^2 + \lambda \cdot J(\thetab). 
  $$ 
  \end{itemize}

\end{vbframe}

%\begin{vbframe}{Example: ridge Regression}
%Assume the data generating process $y=3x_{1} -2x_{2} +\epsilon $, where $\displaystyle \epsilon \sim N( 0,1)$. The true minimizer is given by $\theta ^{*} =( 3,-2)^{T}$.

%\begin{figure}
%\includegraphics[width=0.8\textwidth]{figure/lin_reg_l2.png}
%\end{figure}

%With increasing regularization, $\theta_{\textit{reg}}$ is pulled back to the origin.

%\end{vbframe}


% \section{ridge Regression}

\begin{vbframe}{Ridge Regression}
Intuitive measure of model complexity is deviation from 0-origin, as 0-model contains no effects. %Models close to this either have few active features or only weak effects. 
So we measure $J(\thetab)$ through a vector norm, shrinking coefs closer 0 (\textbf{shrinkage methods}).\\
\vspace{0.2cm}
\textbf{ridge regression} uses a simple $L2$ penalty:
\begin{eqnarray*}  
\thetah_{\text{ridge}} &=& \argmin_{\thetab} \sumin \left(\yi - \thetab^T \xi \right)^2 + \lambda \sum_{j=1}^{p} \theta_j^2 \\
&=& \argmin_{\thetab} \left(\yv - \Xmat \thetab\right)^\top \left(\yv - \Xmat \thetab\right) + \lambda \underbrace{\thetab^\top \thetab}_{\|\thetab\|_2^2}
\end{eqnarray*}

Optimization is possible (as in the normal LM) in analytical form:
$$\thetah_{\text{ridge}} = ({\Xmat}^T \Xmat  + \lambda \id)^{-1} \Xmat^T\yv$$

Name comes from the fact that we add positive entries along the diagonal "ridge" $\Xmat^T \Xmat$

\framebreak 

Let $y=3x_{1} -2x_{2} +\epsilon $, $ \epsilon \sim N( 0,1)$. The true minimizer is $\theta ^{*} =( 3,-2)^{T}$, with $ \thetah_{\text{ridge}} = \argmin_{\thetab} \|\yv - \Xmat \thetab\|^2 + \lambda \|\thetab\|^2 $.

\begin{figure}
\includegraphics[width=0.8\textwidth]{figure/lin_reg_l2.png}
\end{figure}
\vspace{-0.2cm}
{\small With increasing regularization, $\hat{\theta}_{\textit{ridge}}$ is pulled back to the origin\\ (contour lines show unregularized objective).}

\framebreak 
Contours of regularized objective for different $\lambda$ values.\\
$ \thetah_{\text{ridge}} = \argmin_{\thetab} \|\yv - \Xmat \thetab\|^2 + \lambda \|\thetab\|^2 $.

\begin{figure}
\includegraphics[width=0.8\textwidth]{figure/ridge_contours.png}
\end{figure}
\vspace{-0.2cm}
Green  = true minimizer of the unreg.objective and red = ridge solution.

\framebreak

We understand the geometry of these 2 mixed components in our regularized risk objective much better, if we formulate the optimization as a constrained problem (see this as Lagrange multipliers in reverse).

\vspace{-0.5cm}

\begin{eqnarray*}
\min_{\thetab} && \sumin \left(\yi - \fxit\right)^2 \\
  \text{s.t. } && \|\thetab\|_2^2  \leq t \\
\end{eqnarray*}

\vspace{-1.0cm}

\begin{figure}
\includegraphics[width=0.6\textwidth]{figure/ridge_constraints.png}
\end{figure}

\begin{footnotesize} 
NB: There is a bijective relationship between $\lambda$ and $t$: $\, \lambda \uparrow \,\, \Rightarrow \,\, t \downarrow$ and vice versa.
\end{footnotesize}

\framebreak

\begin{columns}
\begin{column}{0.5\textwidth}
\lz
\begin{figure}
\includegraphics[width=\textwidth]{figure/ridge_inside.png}
\end{figure}
\end{column}

\begin{column}{0.5\textwidth}
\begin{footnotesize} 
\begin{itemize}
  \item Inside constraints perspective: From origin, jump from contour line to contour line (better) until you become infeasible, stop before.
  \item We still optimize the $\risket$, but cannot leave a ball around the origin.
  \item $\risket$ grows monotonically if we move away from $\thetah$ (elliptic contours).
  \item Solution path moves from origin to border of feasible region with minimal $L_2$ distance.
\end{itemize}
\end{footnotesize}
\end{column}
\end{columns}


\framebreak

\begin{columns}
\begin{column}{0.5\textwidth}
\lz
\begin{figure}
\includegraphics[width=\textwidth]{figure/ridge_outside.png}
\end{figure}
\end{column}

\begin{column}{0.5\textwidth}
\begin{footnotesize} 
\begin{itemize}

	\item Outside constraints perspective: From $\thetah$, jump from contour line to contour line (worse) until you become feasible, stop then.
  \item So our new optimum will lie on the boundary of that ball.
  \item Solution path moves from unregularized estimate to feasible region of regularized objective with minimal $L_2$ distance.
\end{itemize}
\end{footnotesize}
\end{column}
\end{columns}

\framebreak

\begin{columns}
\begin{column}{0.5\textwidth}
\lz
\begin{figure}
\includegraphics[width=\textwidth]{figure_man/solution-path-ridge-only.png}
\end{figure}
\end{column}

\begin{column}{0.5\textwidth}
\lz
\begin{footnotesize} 
\begin{itemize}
    \item Here we can see entire solution path for ridge regression
    \item Cyan contours indicate feasible regions induced by different $\lambda$s
    \item Red contour lines indicate different levels of the unreg. objective
    \item Ridge solution (red points) gets pulled toward origin for increasing $\lambda$
\end{itemize}
\end{footnotesize}
\end{column}
\end{columns}

\end{vbframe}



\begin{vbframe}{Example: Polynomial Ridge Regression}

Consider $y=f(x)+\epsilon$ where the true (unknown) function is \(f(x) = 5 + 2x +10x^2 - 2x^3\) (in red).

\lz

We now fit the data using a \(d\)th-order polynomial
\[ f(x) = \theta_0 + \theta_1 x + \cdots + \theta_d x^d = \sum_{j = 0}^{d} \theta_j x^j\text{.} \]
Using model complexity $d = 10$ overfits:

\begin{center}
\includegraphics[width = 10cm ]{figure/poly_ridge_1.png} \\
\end{center}

\framebreak

With an $L2$ penalty we can now select $d$ "too large" but regularize our model by shrinking its coefficients. Otherwise we have to optimize over the discrete $d$.

\vfill

\begin{center}
\includegraphics[width = 11cm ]{figure/poly_ridge_2.png} \\
\end{center}


\begin{center}
\tiny
\begin{tabular}{ c| c c c c c c c c c c c c}
 $\lambda$ & $\theta_0$ & $\theta_1$ & $\theta_2$ & $\theta_3$ & $\theta_4$ & $\theta_5$ & $\theta_6$ & $\theta_7$ & $\theta_8$ & $\theta_9$ & $\theta_{10}$ \\ 
 \hline
 0.00 & 12.00 & -16.00 & 4.80 & 23.00 & -5.40 & -9.30 & 4.20 & 0.53 & -0.63 & 0.13 & -0.01 \\  
 10.00 & 5.20 &1.30 & 3.70 & 0.69 & 1.90 & -2.00 & 0.47 & 0.20 & -0.14 & 0.03 & -0.00 \\ 
 100.00 & 1.70 & 0.46 & 1.80 & 0.25 & 1.80 & -0.94 & 0.34 & -0.01 & -0.06 & 0.02 & -0.00
\end{tabular}
\end{center}


\end{vbframe}

% \section{Lasso Regression}

\begin{comment}
\begin{vbframe}{Lasso Regression}

Another shrinkage method is the so-called \textbf{Lasso regression} ({\scriptsize{least absolute shrinkage and selection operator}}), which uses an $L1$ penalty on $\thetab$:
\vspace{-0.2cm}
\begin{eqnarray*}
\thetah_{\text{Lasso}}= \argmin_{\thetab} \underbrace{\sumin \left(\yi - \thetab^T \xi\right)^2}_{\left(\yv - \Xmat \thetab\right)^\top \left(\yv - \Xmat \thetab\right)} + \lambda \|\thetab\|_1
\end{eqnarray*}
Optimization is much harder now. $\riskrt$ is still convex, but in general there is no analytical solution and it is non-differentiable.\\
\vspace{0.2cm}


\framebreak

Let $y=3x_{1} -2x_{2} +\epsilon $, $ \epsilon \sim N( 0,1)$. The true minimizer is $\theta ^{*} =( 3,-2)^{T}$. Consider $\lambda $ values of 0.01, 0.5, 1, 1.5, 2, 2.5, 10.

\begin{figure}
\includegraphics[width=0.7\textwidth]{figure/lin_reg_l1.png}
\end{figure}

With increasing regularization, $\theta_{\textit{ridge}}$ is pulled back to the origin. Contours = unreg. objective, dots = reg. solution for increasing $\lambda$.

%\textbf{NB}: lasso=least absolute shrinkage and selection operator.

\framebreak 

Contours of regularized objective for different $\lambda$ values.
\begin{figure}
\includegraphics[width=0.9\textwidth]{figure/lasso_contours.png}
\end{figure}

\framebreak

We can also rewrite this as a constrained optimization problem. The penalty results in the constrained region to look like a diamond shape.
\vspace{-0.2cm}
\begin{eqnarray*}
\min_{\thetab} \sumin \left(\yi - \fxit\right)^2\,
\text{subject to: } \|\thetab\|_1 \leq t
\end{eqnarray*}
The kinks in $L1$ enforce sparse solutions because ``the loss contours first hit the sharp corners of the constraint'' at coordinate axes where (some) entries are zero. 
\vspace{-0.1cm}
\begin{figure}%\includegraphics[width=0.3\textwidth]{figure_man/lasso_hat.png}\\
\includegraphics[width=0.95\textwidth]{figure/lasso_contours_cases.png}\\
\end{figure}

\end{vbframe}

\begin{vbframe}{$L1$ and $L2$ Reg. with Orthonormal Design}
For the special case of orthonormal design $\Xmat^{\top}\Xmat=\id$ we can derive closed-form a solution in terms of $\thetah_{\text{OLS}}=(\Xmat^{\top}\Xmat)^{-1}\Xmat^{\top}\yv=\Xmat^{\top}\yv$:
$$\thetah_{\text{Lasso}}=\text{sign}(\thetah_{\text{OLS}})(\vert \thetah_{\text{OLS}} \vert - \lambda)_{+}\quad(\text{sparsity})$$

The function $S(\theta,\lambda):=\text{sign}(\theta)(|\theta|-\lambda)_{+}$ is called the \textbf{soft thresholding} operator: For $|\theta|<\lambda$ it returns $0$, whereas params $|\theta|>\lambda$ are shrunken toward $0$ by $\lambda$.\\
\vspace{0.2cm}
Comparing this to $\thetah_{\text{ridge}}$ under orthonormal design we see qualitatively different behavior as $\lambda \uparrow$:
$$\thetah_{\text{ridge}}= ({\Xmat}^T \Xmat  + \lambda \id)^{-1} \Xmat^T\yv=((1+\lambda)\id)^{-1}\thetah_{\text{OLS}} = \frac{\thetah_{\text{OLS}}}{1+\lambda}\quad (\text{uniform downscaling})$$

\end{vbframe}

\begin{vbframe}{Comparing Solution paths for $L1$/$L2$}
\begin{itemize}
    \item ridge regression results in a smooth solution path with non-sparse parameters
    \item Lasso regression induces sparsity, but only for large enough $\lambda$
\end{itemize}
 \lz
\begin{figure}
\includegraphics[width=0.9\textwidth]{figure_man/solution_paths_l1_l2.png}\\
\end{figure}

\end{vbframe}

\begin{vbframe}{Effect of $L1$/$L2$ on Loss Surface}
Regularized empirical risk $\riskr(\theta_1,\theta_2)$ using squared loss for $\lambda \uparrow$. $L1$ penalty makes non-smooth kinks at coordinate axes more pronounced, while $L2$ penalty warps $\riskr$ toward a ``basin'' (elliptic paraboloid). 
 
\begin{figure}
\includegraphics[width=0.8\textwidth]{figure/reg_surfaces_l1_l2.png}\\
\end{figure}

\end{vbframe}
\end{comment}

\endlecture
\end{document}

