\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}



\title{Introduction to Machine Learning}


\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Regularization 
  }{% Lecture title  
    Ridge Regression
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/ridge_outside.png
  }{
  \item Regularized linear model
  \item Ridge regression / $L2$ penalty
  \item Understand parameter shrinkage
  \item Understand correspondence to constrained optimization
  %\item Know Lasso regression ($L1$ penalty)
}



\begin{vbframe}{Regularization in LM}

  \begin{itemize} \setlength{\itemsep}{1.3em}
  \item Can also overfit if $p$ large and $n$ small(er)
  \item OLS estimator requires full-rank design matrix
  \item For highly correlated features, OLS becomes sensitive to random errors in response, results in large variance in fit
  \item We now add a complexity penalty to the loss:
  $$
  \riskrt = \sumin \left(\yi - \thetab^\top \xi \right)^2 + \lambda \cdot J(\thetab). 
  $$ 
  \end{itemize}

\end{vbframe}

%\begin{vbframe}{Example: ridge Regression}
%Assume the data generating process $y=3x_{1} -2x_{2} +\epsilon $, where $\displaystyle \epsilon \sim N( 0,1)$. The true minimizer is given by $\theta ^{*} =( 3,-2)^{T}$.

%\begin{figure}
%\includegraphics[width=0.8\textwidth]{figure/lin_reg_l2.png}
%\end{figure}

%With increasing regularization, $\theta_{\textit{reg}}$ is pulled back to the origin.

%\end{vbframe}


% \section{ridge Regression}

\begin{vbframe}{Ridge Regression / L2 Penalty}
Intuitive measure of model complexity is deviation from 0-origin; coeffs then have no or a weak effect. 
So we measure $J(\thetab)$ through a vector norm, shrinking coeffs closer to 0.\\
\vspace{0.2cm}
\begin{eqnarray*}  
\thetah_{\text{ridge}} &=& \argmin_{\thetab} \sumin \left(\yi - \thetab^T \xi \right)^2 + \lambda \sum_{j=1}^{p} \theta_j^2 \\
%&=& \argmin_{\thetab} \left(\yv - \Xmat \thetab\right)^\top \left(\yv - \Xmat \thetab\right) + \lambda \thetab^\top \thetab \\
&=& \argmin_{\thetab} \| \yv - \Xmat \thetab \|_2^2  + \lambda \|\thetab\|_2^2
\end{eqnarray*}

Can still analytically solve this:
$$\thetah_{\text{ridge}} = ({\Xmat}^T \Xmat  + \lambda \id)^{-1} \Xmat^T\yv$$

Name: We add pos. entries along the diagonal "ridge" of $\Xmat^T \Xmat$

\framebreak 

Let $y=3x_{1} -2x_{2} +\epsilon $, $ \epsilon \sim N( 0,1)$. The true minimizer is $\theta ^{*} =( 3,-2)^{T}$, with $ \thetah_{\text{ridge}} = \argmin_{\thetab} \|\yv - \Xmat \thetab\|^2 + \lambda \|\thetab\|^2 $.

\begin{figure}
\includegraphics[width=0.8\textwidth]{figure/lin_reg_l2.png}
\end{figure}
\vspace{-0.2cm}
{\small With increasing regularization, $\hat{\theta}_{\textit{ridge}}$ is pulled back to the origin\\ (contour lines show unregularized objective).}

\framebreak 
Contours of regularized objective for different $\lambda$ values.\\
$ \thetah_{\text{ridge}} = \argmin_{\thetab} \|\yv - \Xmat \thetab\|^2 + \lambda \|\thetab\|^2 $.

\begin{figure}
\includegraphics[width=0.8\textwidth]{figure/ridge_contours.png}
\end{figure}
\vspace{-0.2cm}
Green  = true coefs of the DGP and red = ridge solution.

\framebreak

We understand the geometry of these 2 mixed components in our regularized risk objective much better, if we formulate the optimization as a constrained problem (see this as Lagrange multipliers in reverse).

\vspace{-0.5cm}

\begin{eqnarray*}
\min_{\thetab} && \sumin \left(\yi - \fxit\right)^2 \\
  \text{s.t. } && \|\thetab\|_2^2  \leq t \\
\end{eqnarray*}

\vspace{-1.0cm}

\begin{figure}
\includegraphics[width=0.6\textwidth]{figure/ridge_constraints.png}
\end{figure}

\begin{footnotesize} 
NB: There is a bijective relationship between $\lambda$ and $t$: $\, \lambda \uparrow \,\, \Rightarrow \,\, t \downarrow$ and vice versa.
\end{footnotesize}

\framebreak

\begin{columns}
\begin{column}{0.5\textwidth}
\lz
\begin{figure}
\includegraphics[width=\textwidth]{figure/ridge_inside.png}
\end{figure}
\end{column}

\begin{column}{0.5\textwidth}
\begin{footnotesize} 
\lz \lz
\begin{itemize}
  \item Inside constraints perspective: From origin, jump from contour line to contour line (better) until you become infeasible, stop before.
  \item We still optimize the $\risket$, but cannot leave a ball around the origin.
  \item $\risket$ grows monotonically if we move away from $\thetah$ (elliptic contours).
  \item Solution path moves from origin to border of feasible region with minimal $L_2$ distance.
\end{itemize}
\end{footnotesize}
\end{column}
\end{columns}


\framebreak

\begin{columns}
\begin{column}{0.5\textwidth}
\lz
\begin{figure}
\includegraphics[width=\textwidth]{figure/ridge_outside.png}
\end{figure}
\end{column}

\begin{column}{0.5\textwidth}
\begin{footnotesize} 
\lz \lz
\begin{itemize}
	\item Outside constraints perspective: From $\thetah$, jump from contour line to contour line (worse) until you become feasible, stop then.
  \item So our new optimum will lie on the boundary of that ball.
  \item Solution path moves from unregularized estimate to feasible region of regularized objective with minimal $L_2$ distance.
\end{itemize}
\end{footnotesize}
\end{column}
\end{columns}

\framebreak

\begin{columns}
\begin{column}{0.5\textwidth}
\lz
\begin{figure}
\includegraphics[width=\textwidth]{figure_man/solution-path-ridge-only.png}
\end{figure}
\end{column}

\begin{column}{0.5\textwidth}
\begin{footnotesize} 
\lz \lz
\begin{itemize}
    \item Here we can see entire solution path for ridge regression
    \item Cyan contours indicate feasible regions induced by different $\lambda$s
    \item Red contour lines indicate different levels of the unreg. objective
    \item Ridge solution (red points) gets pulled toward origin for increasing $\lambda$
\end{itemize}
\end{footnotesize}
\end{column}
\end{columns}

\end{vbframe}



\begin{vbframe}{Example: Polynomial Ridge Regression}

Consider $y=f(x)+\epsilon$ where the true (unknown) function is \(f(x) = 5 + 2x +10x^2 - 2x^3\) (in red).

\lz

Let's use a \(d\)th-order polynomial
\[ f(x) = \theta_0 + \theta_1 x + \cdots + \theta_d x^d = \sum_{j = 0}^{d} \theta_j x^j\text{.} \]
Using model complexity $d = 10$ overfits:

\begin{center}
\includegraphics[width = 10cm ]{figure/poly_ridge_1.png} \\
\end{center}

\framebreak

With an $L2$ penalty we can now select $d$ "too large" but regularize our model by shrinking its coefficients. Otherwise we have to optimize over the discrete $d$.

\vfill

\begin{center}
\includegraphics[width = 11cm ]{figure/poly_ridge_2.png} \\
\end{center}


\begin{center}
\tiny
\begin{tabular}{ c| c c c c c c c c c c c c}
 $\lambda$ & $\theta_0$ & $\theta_1$ & $\theta_2$ & $\theta_3$ & $\theta_4$ & $\theta_5$ & $\theta_6$ & $\theta_7$ & $\theta_8$ & $\theta_9$ & $\theta_{10}$ \\ 
 \hline
 0.00 & 12.00 & -16.00 & 4.80 & 23.00 & -5.40 & -9.30 & 4.20 & 0.53 & -0.63 & 0.13 & -0.01 \\  
 10.00 & 5.20 &1.30 & 3.70 & 0.69 & 1.90 & -2.00 & 0.47 & 0.20 & -0.14 & 0.03 & -0.00 \\ 
 100.00 & 1.70 & 0.46 & 1.80 & 0.25 & 1.80 & -0.94 & 0.34 & -0.01 & -0.06 & 0.02 & -0.00
\end{tabular}
\end{center}


\end{vbframe}

% \section{Lasso Regression}

\begin{comment}
\begin{vbframe}{Lasso Regression}

Another shrinkage method is the so-called \textbf{Lasso regression} ({\scriptsize{least absolute shrinkage and selection operator}}), which uses an $L1$ penalty on $\thetab$:
\vspace{-0.2cm}
\begin{eqnarray*}
\thetah_{\text{Lasso}}= \argmin_{\thetab} \underbrace{\sumin \left(\yi - \thetab^T \xi\right)^2}_{\left(\yv - \Xmat \thetab\right)^\top \left(\yv - \Xmat \thetab\right)} + \lambda \|\thetab\|_1
\end{eqnarray*}
Optimization is much harder now. $\riskrt$ is still convex, but in general there is no analytical solution and it is non-differentiable.\\
\vspace{0.2cm}


\framebreak

Let $y=3x_{1} -2x_{2} +\epsilon $, $ \epsilon \sim N( 0,1)$. The true minimizer is $\theta ^{*} =( 3,-2)^{T}$. Consider $\lambda $ values of 0.01, 0.5, 1, 1.5, 2, 2.5, 10.

\begin{figure}
\includegraphics[width=0.7\textwidth]{figure/lin_reg_l1.png}
\end{figure}

With increasing regularization, $\theta_{\textit{ridge}}$ is pulled back to the origin. Contours = unreg. objective, dots = reg. solution for increasing $\lambda$.

%\textbf{NB}: lasso=least absolute shrinkage and selection operator.

\framebreak 

Contours of regularized objective for different $\lambda$ values.
\begin{figure}
\includegraphics[width=0.9\textwidth]{figure/lasso_contours.png}
\end{figure}

\framebreak

We can also rewrite this as a constrained optimization problem. The penalty results in the constrained region to look like a diamond shape.
\vspace{-0.2cm}
\begin{eqnarray*}
\min_{\thetab} \sumin \left(\yi - \fxit\right)^2\,
\text{subject to: } \|\thetab\|_1 \leq t
\end{eqnarray*}
The kinks in $L1$ enforce sparse solutions because ``the loss contours first hit the sharp corners of the constraint'' at coordinate axes where (some) entries are zero. 
\vspace{-0.1cm}
\begin{figure}%\includegraphics[width=0.3\textwidth]{figure_man/lasso_hat.png}\\
\includegraphics[width=0.95\textwidth]{figure/lasso_contours_cases.png}\\
\end{figure}

\end{vbframe}

\begin{vbframe}{$L1$ and $L2$ Reg. with Orthonormal Design}
For the special case of orthonormal design $\Xmat^{\top}\Xmat=\id$ we can derive closed-form a solution in terms of $\thetah_{\text{OLS}}=(\Xmat^{\top}\Xmat)^{-1}\Xmat^{\top}\yv=\Xmat^{\top}\yv$:
$$\thetah_{\text{Lasso}}=\text{sign}(\thetah_{\text{OLS}})(\vert \thetah_{\text{OLS}} \vert - \lambda)_{+}\quad(\text{sparsity})$$

The function $S(\theta,\lambda):=\text{sign}(\theta)(|\theta|-\lambda)_{+}$ is called the \textbf{soft thresholding} operator: For $|\theta|<\lambda$ it returns $0$, whereas params $|\theta|>\lambda$ are shrunken toward $0$ by $\lambda$.\\
\vspace{0.2cm}
Comparing this to $\thetah_{\text{ridge}}$ under orthonormal design we see qualitatively different behavior as $\lambda \uparrow$:
$$\thetah_{\text{ridge}}= ({\Xmat}^T \Xmat  + \lambda \id)^{-1} \Xmat^T\yv=((1+\lambda)\id)^{-1}\thetah_{\text{OLS}} = \frac{\thetah_{\text{OLS}}}{1+\lambda}\quad (\text{uniform downscaling})$$

\end{vbframe}

\begin{vbframe}{Comparing Solution paths for $L1$/$L2$}
\begin{itemize}
    \item ridge regression results in a smooth solution path with non-sparse parameters
    \item Lasso regression induces sparsity, but only for large enough $\lambda$
\end{itemize}
 \lz
\begin{figure}
\includegraphics[width=0.9\textwidth]{figure_man/solution_paths_l1_l2.png}\\
\end{figure}

\end{vbframe}

\begin{vbframe}{Effect of $L1$/$L2$ on Loss Surface}
Regularized empirical risk $\riskr(\theta_1,\theta_2)$ using squared loss for $\lambda \uparrow$. $L1$ penalty makes non-smooth kinks at coordinate axes more pronounced, while $L2$ penalty warps $\riskr$ toward a ``basin'' (elliptic paraboloid). 
 
\begin{figure}
\includegraphics[width=0.8\textwidth]{figure/reg_surfaces_l1_l2.png}\\
\end{figure}

\end{vbframe}
\end{comment}

\endlecture
\end{document}

