\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\captionsetup[subfloat]{labelformat=empty}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{Regularization}{Weight Decay and L2}{figure/graddes_vs_weightdecay.png}{
  \item $L2$ regularization with GD is equivalent to weight decay
  \item Understand how weight decay changes the optimization trajectory
}




\begin{vbframe}{Weight decay vs. L2 Regularization}
Let's optimize $L2$-regularized risk of a model $\fxt$
\vspace{-0.2cm}
\[
\min_{\thetab} \riskrt = \min_{\thetab} \risket + \frac{\lambda}{2} \|\thetab\|^2_2
\]

by GD. The gradient is

\[
\nabla_{\thetab} \riskrt = \nabla_{\thetab} \risket + \lambda \thetab
\]

We iteratively update $\thetab$ by step size \(\alpha\) times the
negative gradient
\vspace{-0.2cm}
\begin{align*}
\thetab^{[\text{new}]} &= \thetab^{[\text{old}]} - \alpha \left(\nabla_{\thetab} \riske(\thetab^{[\text{old}]}) + \lambda \thetab^{[\text{old}]}\right) \\&=
\thetab^{[\text{old}]} (1 - \alpha \lambda) - \alpha \nabla_{\thetab} \riske(\thetab^{[\text{old}]})
\end{align*}
{\small
We see how $\thetab^{[old]}$ decays in magnitude -- for small $\alpha$ and $\lambda$ -- before we do the gradient step. Performing the decay directly, under this name, is a very well-known technique in DL - and simply $L2$ regularization in disguise (for GD).
}
\framebreak


In GD With WD, we slide down neg. gradients of $\riske$, \\
but in every step, we are pulled back to origin.
\begin{figure}
  \subfloat[Without WD]{\includegraphics[width=0.4\textwidth]{figure/graddes_vs_weightdecay_01.png}}
  \subfloat[With GD]{\includegraphics[width=0.4\textwidth]{figure/graddes_vs_weightdecay_02.png}}\\
\end{figure}


\framebreak

How strongly we are pulled back (for fixed $\alpha$) depends on $\lambda$:

\begin{figure}
  \subfloat[Small $\lambda$]{\includegraphics[width=0.4\textwidth]{figure/weightdecay_lambda_plot_01.png}}
  \subfloat[Large $\lambda$]{\includegraphics[width=0.4\textwidth]{figure/weightdecay_lambda_plot_02.png}}\\
\end{figure}
\end{vbframe}


\begin{vbframe}{Caveat and other optimizers}

\textbf{Caveat}: Equivalence of weight decay and $L2$ only holds for (S)GD!

\begin{itemize}\setlength{\itemsep}{0.5em}
    \item \citelink{HANSON1988} originally define WD ``decoupled'' from gradient-updates {\footnotesize $\alpha \nabla_{\thetab} \riske(\thetab^{[\text{old}]})$} as
    {\footnotesize $\thetab^{[\text{new}]} =
    \thetab^{[\text{old}]} (1 - \lambda') - \alpha \nabla_{\thetab} \riske(\thetab^{[\text{old}]})$}
    \item This is equivalent to modern WD/$L2$ (last slide) using reparameterization $\lambda'=\alpha \lambda$
   % \item Using this we see the WD is decoupled from the gradient updates
    \item Consequence: if there is optimal $\lambda'$, then optimal $L2$ penalty is tightly coupled to $\alpha$ as $\lambda=\lambda'/ \alpha$ (and vice versa)
    
    \item \citelink{LOSCH2019} show no equivalence of $L2$ and WD possible for adaptive methods like Adam (Prop. 2)
    \item In many cases where SGD+$L2$ works well, Adam+$L2$ underperforms due to non-equivalence with WD
    \item They propose a variant of Adam decoupling WD from gradient updates (AdamW), increasing performance over Adam+$L2$
\end{itemize}




\end{vbframe}


\endlecture
\end{document}

