\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure/graddes_vs_weightdecay.png}
\newcommand{\learninggoals}{
  \item Understand why $L2$ regularization in combination with gradient descent is equivalent to weight decay
  \item Understand how weight decay changes the optimization trajectory
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Regularization:\\ Weight Decay and L2}
\lecture{Introduction to Machine Learning}



\begin{vbframe}{Weight decay vs. L2 Regularization}
Let us optimize the $L2$-regularized risk of a model $\fxt$
\vspace{-0.2cm}
\[
\min_{\thetab} \riskrt = \min_{\thetab} \risket + \frac{\lambda}{2} \|\thetab\|^2_2
\]

by gradient descent. The gradient is

\[
\nabla_{\thetab} \riskrt = \nabla_{\thetab} \risket + \lambda \thetab.
\]

We iteratively update $\thetab$ by step size \(\alpha\) times the
negative gradient
\vspace{-0.2cm}
\begin{align*}
\thetab^{[\text{new}]} &= \thetab^{[\text{old}]} - \alpha \left(\nabla_{\thetab} \riske(\thetab^{[\text{old}]}) + \lambda \thetab^{[\text{old}]}\right) \\&=
\thetab^{[\text{old}]} (1 - \alpha \lambda) - \alpha \nabla_{\thetab} \riske(\thetab^{[\text{old}]}).
\end{align*}
{\small
The term \(\lambda \thetab^{[old]}\) causes the parameter
(\textbf{weight}) to \textbf{decay} in proportion to its size. This is a very well-known technique in deep learning - and simply $L2$ regularization in disguise (for gradient descent).
}
\framebreak

\textbf{Caveat}: Equivalence of weight decay and $L2$ only holds for (S)GD!

\vspace{0.1cm}

\begin{itemize}\setlength{\itemsep}{0.5em}
    \item \citebutton{Hanson and Pratt, 1998}{https://proceedings.neurips.cc/paper_files/paper/1988/file/1c9ac0159c94d8d0cbedc973445af2da-Paper.pdf} originally define WD ``decoupled'' from gradient-updates {\footnotesize $\alpha \nabla_{\thetab} \riske(\thetab^{[\text{old}]})$} as
    {\footnotesize $\thetab^{[\text{new}]} =
    \thetab^{[\text{old}]} (1 - \lambda') - \alpha \nabla_{\thetab} \riske(\thetab^{[\text{old}]})$}
    \item This is equivalent to modern WD/$L2$ (last slide) using reparameterization $\lambda'=\alpha \lambda$
   % \item Using this we see the WD is decoupled from the gradient updates
    \item Consequence: if there is optimal $\lambda'$, then optimal $L2$ penalty is tightly coupled to $\alpha$ as $\lambda=\lambda'/ \alpha$ (and vice versa)
    
    \item \citebutton{Loshchilov and Hutter, 2019}{https://arxiv.org/pdf/1711.05101} show no equivalence of $L2$ and WD possible for adaptive methods like Adam (Prop. 2)
    \item In many cases where SGD+$L2$ works well, Adam+$L2$ underperforms due to non-equivalence with WD
    \item They propose a variant of Adam decoupling WD from gradient updates (AdamW), increasing performance over Adam+$L2$
\end{itemize}



\framebreak

When we use weight decay, we follow the steepest slope of $\riske$ as for gradient descent, but in every step, we are pulled back to the origin.

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure/graddes_vs_weightdecay.png}\\
\end{figure}


\framebreak

How strongly we are pulled back to the origin (for a fixed stepsize $\alpha$) depends only on $\lambda$ as long as the procedure converges:

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure/weightdecay_lambda_plot.png}\\
\end{figure}


\end{vbframe}


\endlecture
\end{document}

