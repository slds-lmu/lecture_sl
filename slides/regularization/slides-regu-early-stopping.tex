\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Regularization 
  }{% Lecture title  
    Early Stopping
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/earlystop.png
  }{
  \item Know how early stopping works 
  \item Understand how early stopping acts as a regularizer
  \item Know early stopping imitates $L2$ regularization in some cases
}




\begin{vbframe}{Early Stopping}
  
  \begin{itemize}
    \item When training with an iterative optimizer such as SGD, it is commonly the case that, after a certain number of iterations, generalization error begins to increase even though training error continues to decrease.     
    \item \textbf{Early stopping} refers to stopping the algorithm early before the generalization error increases.
  \end{itemize}
  \begin{figure}
    \centering
      \scalebox{0.5}{\includegraphics{figure_man/earlystop.png}}
      \caption{After a certain number of iterations, the algorithm begins to overfit.}
  \end{figure}
\framebreak
  How early stopping works:
  \begin{enumerate}
    \item Split training data $\Dtrain$ into $\mathcal{D}_{\text{subtrain}}$ and $\mathcal{D}_{\text{val}}$ (e.g. with a ratio of 2:1).
    \item Train on $\mathcal{D}_{\text{subtrain}}$ and evaluate model using the validation set $\mathcal{D}_{\text{val}}$.
    \item Stop training when validation error stops decreasing (after a range of \enquote{patience} steps).
    \item Use parameters of the previous step for the actual model.
  \end{enumerate}
  More sophisticated forms also apply cross-validation.
\end{vbframe}

\begin{vbframe}{Early Stopping and $L2$ \citebutton{Goodfellow et al., 2016, p. 249 ff.}{https://www.deeplearningbook.org/contents/regularization.html}}
  \begin{table}
    \begin{tabular}{p{4cm}|p{6cm}}
    Strengths & Weaknesses \\
    \hline
    \hline
    Effective and simple & Periodical evaluation of validation error\\
    \hline
    Applicable to almost any model without adjustment \note{of objective function, parameter space, training procedure} & Temporary copy of $\thetab$ (we have to save the whole model each time validation error improves) \\
    \hline
    Combinable with other regularization methods & Less data for training $\rightarrow$ include $\mathcal{D}_{\text{val}}$ afterwards\\ \hline\hline
    \end{tabular}
  \end{table}
  \begin{itemize}
    \item For simple case of LM with squared loss and GD optim initialized at $\thetab=0$: Early stopping has exact correspondence with $L2$ regularization/WD: %Relation between
    optimal early-stopping iter $T_{\text{stop}}$ inversely proportional to  $\lambda$ scaled by step-size $\alpha$
    
 \end{itemize}
\begin{equation*}
T_{\text{stop}} \approx \frac{1}{\alpha \lambda} 
\Leftrightarrow \lambda \approx \frac{1}{T_{\text{stop}} \alpha}
\end{equation*}
  \begin{itemize}
    \item Small $\lambda$ ( regu. $\downarrow$) $\Rightarrow$ large $T_{\text{stop}}$ (complexity $\uparrow$) and vice versa
  \end{itemize}
\framebreak
  % \begin{itemize}
  %   \item[]
  % \end{itemize}
  % \begin{figure}
  %   \centering
  %     \includegraphics[width=11cm]{figure_man/early_stopping}
  %     \caption{Optimization path of early stopping (left) and weight decay (right) (Goodfellow et al. (2016))}
  % \end{figure}
  % \framebreak
  \begin{figure}
    \centering
      \scalebox{0.75}{\includegraphics{figure_man/earlystop_int_hat.png}}
      \tiny{\\Goodfellow et al. (2016)\\}
  \end{figure}
  
\footnotesize 
\textbf{Figure:} Effect of early stopping. \textit{Left:} The solid lines indicate contours of the square loss objective. Dashed line indicates trajectory taken by GD initialized at origin. Instead of reaching minimizer $\thetah$, ES results in trajectory stopping earlier at $\hat{\thetab}_{\text{ridge}}$. \textit{Right:} Effect of $L2$ regularization. Dashed circles indicate contours of $L2$ constraint which push minimizer of regularized cost closer to origin than minimizer of unregularized cost.
\end{vbframe}

\begin{vbframe}{SGD Trajectory and $L2$}
Solution paths for $L2$ regularized linear model closely matches SGD trajectory of unregularized LM initialized at $\thetab=0$ \citebutton{Ali et al., 2020}{https://proceedings.mlr.press/v119/ali20a/ali20a.pdf}
\lz
  \begin{figure}
    \centering
      %\scalebox{0.75}
      {\includegraphics{figure_man/ridge-vs-sgd-path.png}}
      %\scriptsize{\\Ali et al. (2020)\\}
  \end{figure}

\textbf{Caveat}: Initialization at the origin is crucial for this equivalence to hold, which is almost never used in practice in ML/DL applications

\end{vbframe}

\endlecture
\end{document}