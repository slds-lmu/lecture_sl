\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Regularization 
  }{% Lecture title  
    Early Stopping
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/earlystop.png
  }{
  \item Know how early stopping works 
  \item Understand how early stopping acts as a regularizer
}




\begin{vbframe}{Early Stopping}
  
  \begin{itemize}
    \item Especially for complex nonlinear models we can easily overfit
    \item In optimization: Often, after a certain number of iterations, generalization error begins to increase even though training error continues to decrease
  \end{itemize}

\lz
  
  \begin{figure}
    \centering
      \scalebox{0.5}{\includegraphics{figure_man/earlystop.png}}
  \end{figure}

  
\framebreak

    For iterative optimizers like SGD, \\
    we can monitor this step-by-step over small iterations:

\lz
    
  \begin{enumerate}
    \item Split train data $\Dtrain$ into $\mathcal{D}_{\text{subtrain}}$ and $\mathcal{D}_{\text{val}}$ (e.g. with ratio of 2:1)
    \item Train on $\mathcal{D}_{\text{subtrain}}$ and eval model on $\mathcal{D}_{\text{val}}$
    \item Stop when validation error stops decreasing \\
    (after a range of \enquote{patience} steps)
    \item Use parameters of the previous step for the actual model
  \end{enumerate}

\lz \lz
  
  More sophisticated forms also apply cross-validation.
\end{vbframe}

\begin{vbframe}{Early Stopping and $L2$ \citelink{GOODFELLOWREG}}
  \begin{table}
    \begin{tabular}{p{4cm}|p{6cm}}
    Strengths & Weaknesses \\
    \hline
    \hline
    Effective and simple & Periodical evaluation of validation error\\
    \hline
    Applicable to almost any model without adjustment \note{of objective function, parameter space, training procedure} & Temporary copy of $\thetav$ (we have to save the whole model each time validation error improves) \\
    \hline
    Combinable with other regularization methods & Less data for training $\rightarrow$ include $\mathcal{D}_{\text{val}}$ afterwards\\ \hline\hline
    \end{tabular}
  \end{table}
  \begin{itemize}
    \item For simple case of LM with squared loss and GD optim initialized at $\thetav=0$: Early stopping has exact correspondence with $L2$ regularization/WD: %Relation between
    optimal early-stopping iter $T_{\text{stop}}$ inversely proportional to  $\lambda$ scaled by step-size $\alpha$
    
 \end{itemize}
\begin{equation*}
T_{\text{stop}} \approx \frac{1}{\alpha \lambda} 
\Leftrightarrow \lambda \approx \frac{1}{T_{\text{stop}} \alpha}
\end{equation*}
  \begin{itemize}
    \item Small $\lambda$ ( regu. $\downarrow$) $\Rightarrow$ large $T_{\text{stop}}$ (complexity $\uparrow$) and vice versa
  \end{itemize}
\framebreak

\vspace{1cm}

    \centering
      \includegraphics[]{figure_man/earlystop_int_hat.png}
      \tiny{\\Goodfellow et al. (2016)\\}
  


\begin{itemize}
\item Solid lines are $\risket$ 
\item LHS: Trajectory of GD early stopped, initialized at origin
\item RHS: Constrained form of ridge regularization
\end{itemize}

\end{vbframe}

\begin{vbframe}{SGD Trajectory and $L2$ \citelink{ALI2020}}
Solution paths for $L2$ regularized linear model closely matches SGD trajectory of unregularized LM initialized at $\thetav=0$ 
\lz
  \begin{figure}
    \centering
      %\scalebox{0.75}
      {\includegraphics{figure/ridge_vs_sgd_path.png}}
      %\scriptsize{\\Ali et al. (2020)\\}
  \end{figure}

\textbf{Caveat}: Initialization at the origin is crucial for this equivalence to hold, which is almost never exactly used in practice in ML/DL applications

\end{vbframe}

\endlecture
\end{document}