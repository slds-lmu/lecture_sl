\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-ensembles.tex}


\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Regularization 
  }{% Lecture title  
    Bagging as Regularization (Deep-Dive)
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/bagging.pdf
  }{
  \item Understand that bagging can be seen as a form of regularization
  \item Know which factors influence the effectiveness of bagging
}


\begin{vbframe}{Recap: What is bagging?}

\begin{itemize}
  \item Bagging is short for \textbf{B}ootstrap \textbf{Agg}regation.
  \item It's an \textbf{ensemble method}, i.e., it combines many models into one 
        big \enquote{meta-model}. Ensembles often work much better than their members alone would.
  \item The components of an ensemble are called \textbf{base learners} (BLs)
      % that improves instable / high variance learners by variance smoothing
  \item In a \textbf{bagging} ensemble, all base learners are of the same type. The only difference between the models is the data they are trained on.\\
\end{itemize}

\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1xodP6ayu1Gay6mMKgzVWYEFmSoeG5kNuqsaTkFFmd78/edit
\includegraphics[width=0.45\textwidth]{figure_man/bagging.pdf}
\end{center}

\framebreak 
Specifically, we train base learners $\bl, m = 1, \dots, M$ on $M$ \textbf{bootstrap} samples of training data $\D$:
\begin{itemize}  \setlength{\itemsep}{1.0em}
  \item Draw $n$ observations from $\D$ with replacement
  \item Fit the base learner on each of the $M$ bootstrap samples to get models $\fh(x) = \blh, m = 1, \dots, M$
\item Aggregate predictions of the $M$ fitted base learners to get ensemble model $\fMh$ via \textbf{averaging} (regression) or \textbf{majority voting} (classification)
    %\item Posterior probs $\pikxh$ can be estimated as predicted class frequencies over the ensemble
\end{itemize}
\vspace{0.1cm}
{\small Bagging helps because variability of the averaged prediction over many base learners is smaller than variability of the predictions from one such model. If error of BL is mostly due to (random) variability and not structural reasons bagging helps reducing this variability.}


%\begin{center}
% FIGURE SOURCE: No source
%\includegraphics[width=0.6\textwidth]{figure_man/rf_majvot_averaging.png}
%\end{center}
%\end{vbframe}

% \begin{algorithm}[H]
%   \small
%   \setstretch{1.15}
%   \caption*{Bagging algorithm}
%   \begin{algorithmic}[1]
%     \State {\bf Input: } Dataset $\D$, base learner, number of bootstraps $M$
%     \For {$m = 1 \to M$}
%       \State Draw a bootstrap sample $\D^{[m]}$ from $\D$.
%       \State Train base learner on $\D^{[m]}$ to obtain model $\bl$
%     \EndFor
%     \State Aggregate the predictions of the $M$ estimators (via averaging or majority voting), to determine the bagging estimator:
%     \begin{align*}
%     \fM &= \frac{1}{M} \sum_{m=1}^M \bl \\
%     \text{or}\quad \fM &= \argmax_{k \in \Yspace} \sum_{m=1}^M \I\left(\bl = k\right)
%     \end{align*}
%   \end{algorithmic}
% \end{algorithm}

\end{vbframe}

\begin{vbframe}{Why/when does Bagging help?}

%In one sentence:\\
%\lz

%Because the variability of the average of the predictions of many base learner models is smaller than the variability of the predictions from one such base learner model.\\

%If the error of a base learner model is mostly due to (random) variability and not due to structural reasons, combining many such base learners by bagging helps reducing this variability.
%\framebreak
\small Assume we use quadratic loss and measure instability of the ensemble with
\begin{scriptsize}
$\ambifM = \tfrac{1}{M}\sum^M_{m} \left(\bl- \fM \right)^2$:
 \vskip -2em
 \begin{align*}
 \ambifM &= \tfrac{1}{M}\sum^M_{m} \left(\bl- \fM\right)^2 \\
         &= \tfrac{1}{M}\sum^M_{m} \left(\left(\bl - y\right)  + \left(y - \fM\right)\right)^2\\
         &= \tfrac{1}{M}\sum^M_{m} L(y, \bl) + L(y, \fM) \underbrace{- 2 \left(y - \tfrac{1}{M}\sum^M_{m=1}\bl\right)\left(y - \fM\right)}_{- 2 L\left(y, \fM\right)} \\[-.5em]  \end{align*}
\end{scriptsize}
\vspace{-0.3cm}
So, if we take the expected value over the data's distribution:
$${\scriptsize \Exy\left[L\left(y, \fM\right)\right] = \tfrac{1}{M}\sum^M_{m} \Exy\left[L\left(y, \bl \right)\right] - \Exy\left[\ambifM\right]}$$
$\Rightarrow$ Expected loss of the ensemble is lower than the average loss of single BL by the amount of instability in the ensemble's BLs. The more accurate and diverse the BLs, the better.
\normalsize
\framebreak
\end{vbframe}

\begin{vbframe}{Determinants of Bagging Effectiveness}

{\small How to make $\Exy\left[\ambifM\right]$ as large as possible?}
\begin{scriptsize}

%\shortintertext{How to make $\Exy\left[\ambifM\right]$ as large as possible?}
$$\Exy\left[L\left(y, \fM \right)\right] = \tfrac{1}{M}\sum^M_{m} \Exy\left[L\left(y, \bl \right)\right] - \Exy\left[\ambifM\right]$$ \\
{\small For simplicity, assume $\Exy\left[\bl\right] = 0$, $\var_{xy}\left[\bl\right] = \Exy\left[(\bl)^2\right] = \sigma^2$, $\corr_{xy}\left[\bl, b^{[m']}\right] = \rho$ for all $m, m'$.}
\begin{align*}
\implies 
\var_{xy}\left[\fM\right] &= \tfrac{1}{M} \sigma^2 +  \tfrac{M-1}{M} \rho \sigma^2 \qquad\left(... = \Exy\left[(\fM)^2\right]\right)\\
 \Exy\left[\ambifM\right] &= \tfrac{1}{M}\sum^M_{m} \Exy\left[\left(\bl- \fM\right)^2\right]\\
 & = \tfrac{1}{M}\left(M \Exy\left[(\bl)^2\right] + M \Exy\left[(\fM)^2\right] - 
     2 M \Exy\left[\bl\fM\right]\right) \\
  & = \sigma^2  + \Exy\left[(\fM)^2\right] - 2 \tfrac{1}{M}\sum^M_{m'} \underbrace{\Exy\left[\bl b^{[m']} \right]}_{\mathclap{\qquad\qquad\qquad\qquad= \cov_{xy}\left[\bl, b^{[m']} \right] + \Exy\left[\bl\right]\Exy\left[b^{[m']}\right]}} \\
  &=  \sigma^2  + \left(\tfrac{1}{M} \sigma^2 +   \tfrac{M-1}{M} \rho \sigma^2\right) - 2\left(\tfrac{M-1}{M} \rho\sigma^2 + \tfrac{1}{M}\sigma^2 + 0 \cdot 0 \right)\\
  &= \tfrac{M-1}{M} \sigma^2 (1-\rho)
\end{align*}
\end{scriptsize}

\begin{small}
\begin{align*}
\Exy\left[L\left(y, \fM\right)\right] &= \textcolor{blue}{\tfrac{1}{M}\sum^M_{m} \Exy\left[L\left(y, \bl \right)\right]} - \Exy\left[\ambifM\right]\\
\Exy\left[\ambifM\right] &\cong 
\textcolor{purple}{\frac{M-1}{M}} \textcolor{cyan}{\var_{xy}\left[\bl\right]} \textcolor{violet}{\left(1 - \corr_{xy}\left[\bl, b^{[m']}\right]\right)}
\end{align*}
\end{small}
\begin{itemize}
\item[$\Rightarrow$] \textcolor{blue}{\textbf{better base learners}} are better {\small (... duh)}
\item[$\Rightarrow$] \textcolor{purple}{\textbf{more base learners}} are better {\small (theoretically, at least...)}\\
\item[$\Rightarrow$] \textcolor{cyan}{\textbf{more variable base learners}} are better {\small(as long as their risk stays the same, of course!)}
\item[$\Rightarrow$] \textcolor{violet}{\textbf{less correlation between base learners}} is better:\\ bagging helps more if base learners are wrong in different ways so that their errors \enquote{cancel} each other out.\\
\end{itemize}


\end{vbframe}

\begin{vbframe}{Bagging Summary}

  \begin{itemize}
    \item Basic idea: fit the same model repeatedly on many \textbf{bootstrap} replications of the training data set and \textbf{aggregate} the results
    \item Gains in performance by reducing variance of predictions, but (slightly) increases the bias: it reuses training data many times, so small mistakes can get amplified.\\ Bagging is thus a \textbf{form of regularization}
    \item Works best for unstable/high-variance BLs, where small changes in training set can cause large changes in predictions:\\
    e.g., CART, neural networks, step-wise/forward/backward variable selection for regression\\
    \item Works best if BL predictions are only weakly correlated: they don't all make the same mistakes.
    \item Can degrade performance for stable methods like $k$-NN, LDA, Naive Bayes, linear regression
  \end{itemize}
\end{vbframe}



\endlecture
\end{document}
