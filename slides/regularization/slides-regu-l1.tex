\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{Regularization}{Lasso Regression}{figure/lin_model_regu_01.png}{
    \item Lasso regression / $L1$ penalty
    \item Know that lasso selects features
    \item Support recovery
}


% \section{Lasso Regression}

\begin{vbframe}{Lasso Regression}

Another shrinkage method is the so-called \textbf{lasso regression} ({\scriptsize{least absolute shrinkage and selection operator}}), which uses an $L1$ penalty on $\thetav$:
\vspace{0.4cm}
\begin{align*}
\thetah_{\text{lasso}}&= \argmin_{\thetav} \sumin \left(\yi - \thetav^T \xi\right)^2 + \lambda \sum_{j=1}^{p} \vert\theta_j\vert\\
&= \argmin_{\thetav}\left(\yv - \Xmat \thetav\right)^\top \left(\yv - \Xmat \thetav\right) + \lambda \|\thetav\|_1
\end{align*}

\vspace{0.4cm}

Optimization is much harder now. $\riskrt$ is still convex, but in general there is no analytical solution and it is non-differentiable.\\
\vspace{0.2cm}


\framebreak

Let $y=3x_{1} -2x_{2} +\epsilon $, $ \epsilon \sim N( 0,1)$. The true minimizer is $\theta ^{*} =( 3,-2)^{T}$. 
LHS = $L1$ regularization; RHS = $L2$
\begin{columns}
\begin{column}{0.5\textwidth}
\lz
\begin{figure}
\includegraphics[width=0.99\textwidth]{figure/lin_model_regu_01.png}
\end{figure}
\end{column}

\begin{column}{0.5\textwidth}
\lz
\begin{figure}
\includegraphics[width=0.99\textwidth]{figure/lin_model_regu_02.png}
\end{figure}
\end{column}
\end{columns}

%\begin{figure}
%\includegraphics[width=0.8\textwidth]{figure/lin_model_regu_01.png}
%\end{figure}

%\begin{figure}
%\includegraphics[width=0.8\textwidth]{figure/lin_model_regu_02.png}
%\end{figure}

\lz

With increasing regularization, $\hat{\theta}_{\textit{lasso}}$ is pulled back to the origin, but takes a different ``route''. $\theta_2$ eventually becomes 0!

%\textbf{NB}: lasso=least absolute shrinkage and selection operator.

\framebreak 

Contours of regularized objective for different $\lambda$ values.
\begin{figure}
\includegraphics[width=0.85\textwidth]{figure/reg_contours_01.png}
\end{figure}

Green  = true minimizer of the unreg.objective and red = lasso solution.

\framebreak

Regularized empirical risk $\riskr(\theta_1,\theta_2)$ using squared loss for $\lambda \uparrow$. $L1$ penalty makes non-smooth kinks at coordinate axes more pronounced, while $L2$ penalty warps $\riskr$ toward a ``basin'' (elliptic paraboloid). 
 
\begin{figure}
    \begin{minipage}{0.32\linewidth}
        \centerline{\includegraphics[width=\textwidth]{figure/reg_surfaces_l1_lam0.png}}
        \centerline{\includegraphics[width=\textwidth]{figure/reg_surfaces_l2_lam0.png}}
    \end{minipage}
   \begin{minipage}{0.32\linewidth}
        \centerline{\includegraphics[width=\textwidth]{figure/reg_surfaces_l1_lam1.png}}
        \centerline{\includegraphics[width=\textwidth]{figure/reg_surfaces_l2_lam1.png}}
    \end{minipage}
    \begin{minipage}{0.32\linewidth}
        \centerline{\includegraphics[width=\textwidth]{figure/reg_surfaces_l1_lam10.png}}
        \centerline{\includegraphics[width=\textwidth]{figure/reg_surfaces_l2_lam10.png}}
   \end{minipage}
\end{figure}

%\begin{figure}
%\includegraphics[width=0.8\textwidth]{figure/reg_surfaces_l1_l2.png}\\
%\end{figure}
\framebreak

We can also rewrite this as a constrained optimization problem. The penalty results in the constrained region to look like a diamond shape.
\vspace{-0.2cm}
\begin{eqnarray*}
\min_{\thetav} \sumin \left(\yi - \fxit\right)^2\,
\text{subject to: } \|\thetav\|_1 \leq t
\end{eqnarray*}
The kinks in $L1$ enforce sparse solutions because ``the loss contours first hit the sharp corners of the constraint'' at coordinate axes where (some) entries are zero. 
\vspace{-0.1cm}
\begin{figure}%\includegraphics[width=0.3\textwidth]{figure_man/lasso_hat.png}\\
\includegraphics[width=0.95\textwidth]
{figure/lasso_contour_cases.png}\\
\end{figure}

\end{vbframe}

\begin{vbframe}{$L1$ and $L2$ Reg. with Orthonormal Design}
\small For special case of orthonormal design $\Xmat^{\top}\Xmat=\id$ we can derive a closed-form solution in terms of $\thetah_{\text{OLS}}=(\Xmat^{\top}\Xmat)^{-1}\Xmat^{\top}\yv=\Xmat^{\top}\yv$:
\vspace{-0.1cm}
$$\thetah_{\text{lasso}}=\text{sign}(\thetah_{\text{OLS}})(\vert \thetah_{\text{OLS}} \vert - \lambda)_{+}\quad(\text{sparsity})\vspace{-0.1cm}$$
Function $S(\theta,\lambda):=\text{sign}(\theta)(|\theta|-\lambda)_{+}$ is called \textbf{soft thresholding} operator: For $|\theta|\leq\lambda$ it returns $0$, whereas params $|\theta|>\lambda$ are shrunken toward $0$ by $\lambda$.\\
%\vspace{0.05cm}
Comparing this to $\thetah_{\text{Ridge}}$ under orthonormal design: %we see qualitatively different behavior as $\lambda \uparrow$:
\vspace{-0.3cm}
$$\thetah_{\text{Ridge}}= ({\Xmat}^T \Xmat  + \lambda \id)^{-1} \Xmat^T\yv=((1+\lambda)\id)^{-1}\thetah_{\text{OLS}} = \frac{\thetah_{\text{OLS}}}{1+\lambda}\quad (\text{no sparsity})\vspace{-0.22cm}$$
%Soft threshold ensures exact zeros, while $L2$ penalty shrinks uniformly.
\vspace{-0.16cm}
\begin{figure}
\includegraphics[width=0.5\textwidth]{figure/soft_thresholding.png}\\
\end{figure}

\end{vbframe}

\begin{vbframe}{Comparing Solution paths for $L1$/$L2$}
\begin{itemize}
    \item Ridge results in  smooth solution path with non-sparse params
    \item Lasso induces sparsity, but only for large enough $\lambda$
\end{itemize}
 \lz
\begin{figure}
\includegraphics[width=0.9\textwidth]{figure/solution_paths_01.png}\\
\end{figure}

\end{vbframe}

%\begin{vbframe}{Effect of $L1$/$L2$ on Loss Surface}
%Regularized empirical risk $\riskr(\theta_1,\theta_2)$ using squared loss for $\lambda \uparrow$. $L1$ penalty makes non-smooth kinks at coordinate axes more pronounced, while $L2$ penalty warps $\riskr$ toward a ``basin'' (elliptic paraboloid). 
 
%\begin{figure}
%\includegraphics[width=0.8\textwidth]{figure/reg_surfaces_l1_l2.png}\\
%\end{figure}

%\end{vbframe}

\begin{vbframe}{Support Recovery of Lasso \citelink{ZHAO2006}}
\begin{small}
When can lasso select true support of $\thetav$, i.e., only the non-zero parameters? \\
Can be formalized as sign-consistency: 
\begin{align*}
\mathbb{P}\big(\text{sign}(\thetah)=\text{sign}(\thetav)\big) \to 1 \, \text{as} \, n \to \infty \quad (\text{where}\,\text{sign}(0):=0) 
\end{align*}
\\
%\vspace{0.1cm}
Suppose the true DGP given a partition into subvectors $\thetav=(\thetav_1, \thetav_2)$ is 
\begin{align*}
    \bm{Y}=\Xmat\thetav + \bm{\varepsilon} = \Xmat_1 \thetav_1 + \Xmat_2 \thetav_2 + \bm{\varepsilon}\,\text{with}\,\bm{\varepsilon}\sim (0,\sigma^2 \id)
\end{align*}
%$\bm{Y}=\Xmat\thetav + \bm{\varepsilon}$ with $\bm{\varepsilon}\sim (0,\sigma^2 \id)$ 
and only $\thetav_1$ is non-zero.
\vspace{0.1cm}
Let $\Xmat_1$ denote the $n \times q$ matrix with the relevant features and $\Xmat_2$ the matrix of noise features. It can be shown that $\thetah_{lasso}$ is sign consistent under an \textbf{irrepresentable condition}:
\begin{align*}
    \vert (\Xmat_2^{\top} \Xmat_1)(\Xmat_1^{\top} \Xmat_1)^{-1} \text{sign}(\thetav_1)\vert < \bm{1} \,\, (\text{element-wise})
\end{align*}

In fact, lasso can only be sign-consistent if this condition holds.

Intuitively, the irrelevant variables in $\Xmat_2$ must not be too correlated with (or \textit{representable} by) the informative features \citelink{MEINSHAUSEN2009}
\end{small}
\end{vbframe}

\endlecture
\end{document}

