\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{Regularization}{Other Regularizers}{figure_man/lasso_ridge_hat.png}{
  \item L1/L2 regularization induces bias
  \item Lq (quasi-)norm regularization
  \item L0 regularization 
  \item SCAD and MCP
  %\item Know regularization for invariance properties
}

\begin{vbframe}{Ridge and lasso are biased estimators} 
Although ridge and lasso have many nice properties, they are biased estimators and the bias does not (necessarily) vanish as $n \rightarrow \infty$.\\
\vspace{0.3cm}

For example, in the orthonormal case ($\Xmat^{\top}\Xmat=\bm{I}$) the bias of the lasso is
$$
\begin{cases}\mathbb{E}\left|\widehat{\theta}_j-\theta_j\right|=0 & \text { if } \theta_j=0 \\ \mathbb{E}\left|\widehat{\theta}_j-\theta_j\right| \approx \theta_j & \text { if }\left|\theta_j\right| \in[0, \lambda] \\ \mathbb{E}\left|\widehat{\theta}_j-\theta_j\right| \approx \lambda & \text { if }\left|\theta_j\right|>\lambda\end{cases}
$$
\vspace{0.3cm}

%The bias of the lasso for noise features is thus about $\lambda$ for large $|\theta|$.\\
\vspace{0.2cm}
To reduce the bias/shrinkage of regularized estimators various penalties were proposed, a few of which we briefly introduce now.

\end{vbframe}

\begin{vbframe}{$Lq$ regularization \citelink{FU2000}}

Besides $L1$/$L2$ we could use any $Lq$ (quasi-)norm penalty $\lambda \Vert \thetab \Vert_q^q$ 

\begin{figure}
  \scalebox{0.5}{\includegraphics{figure_man/lasso_ridge_hat.png}}\\
%\includegraphics[height=2.3cm]{figure_man/contour.pdf}
\caption{{\scriptsize \textit{Top:} loss contours and $L1$/$L2$ constraints.
\textit{Bottom:} Constraints for $Lq$ norms $\sum_j |\theta_j|^q$.}}
\end{figure}
\vspace{-0.4cm}
{\footnotesize
\begin{itemize}
    \item For $q<1$ penalty becomes non-convex but for $q>1$ no sparsity is achieved
    \item Non-convex $Lq$ has nice properties like \textbf{oracle property} \citelink{ZOUHASTIE}: consistent (+ asy. unbiased) param estimation and var selection
    \item Downside: non-convexity makes optimization even harder than $L1$\\
    (no unique global minimum but multiple local minima)
\end{itemize}
}
\end{vbframe}


\begin{vbframe}{L0 regularization}
\vspace{-0.3cm}
$$
\riskrt = \risket + \lambda \|\thetab\|_0 := \risket + \lambda \sum_j |\theta_j|^0.
$$
\vspace{-0.3cm}
\begin{figure}
\centering
\scalebox{0.9}{\includegraphics{figure/lq_penalty.png}}
\end{figure}
 

\begin{itemize}
\item L0 "norm" simply counts the nr of non-zero params
%\item $L0$ is zero for $\theta_j = 0$ (defining $0^0 := 0$) and constant on the true support (any $\theta_j \neq 0$)
\item Induces sparsity more aggressively than $L1$, but does not shrink
\item AIC and BIC are special cases of $L0$ 
\item $L0$-regularized risk is not continuous or convex
\item NP-hard to optimize; for smaller $n$ and $p$ somewhat tractable, efficient approximations are still current research
\end{itemize}
\end{vbframe}

\begin{vbframe}{SCAD \citelink{FAN2001}}

Smoothly Clipped Absolute Deviations:\\
non-convex, $\gamma>2$ controlls how fast penalty ``tapers off''
{\footnotesize
$$
\text{SCAD}(\theta \mid \lambda, \gamma)= \begin{cases}\lambda|\theta| & \text { if }|\theta| \leq \lambda \\ \frac{2 \gamma \lambda|\theta|-\theta^2-\lambda^2}{2(\gamma-1)} & \text { if } \lambda<|\theta|<\gamma \lambda \\ \frac{\lambda^2(\gamma+1)}{2} & \text { if }|\theta| \geq \gamma \lambda\end{cases}
$$
}

\begin{columns}

\begin{column}{0.5\textwidth}
%\textbf{Left Column}

{
\begin{itemize}
    \item Lasso, quadratic, then const
    \item Smooth
    \item Contrary to lasso/ridge, SCAD continuously relaxes penalization rate as $|\theta|$ increases above $\lambda$
\end{itemize}

%SCAD is asymptotically unbiased due to the ``clipping'' of the penalty.
}
\end{column}

\begin{column}{0.5\textwidth}
%\textbf{Right Column}

\begin{figure}
      \centering
        \scalebox{0.99}{\includegraphics{figure/nc_penalties_comparison.png}}
        %\caption{\footnotesize lasso vs non-convex SCAD and MCP penalties for scalar parameter $\thetab$}
    \end{figure}

\end{column}

\end{columns}

\end{vbframe}

\begin{vbframe}{MCP \citelink{ZHANG2010}}
Minimax Concave Penalty:\\
also non-convex; similar idea as SCAD with $\gamma>1$
$$
MCP(\theta | \lambda, \gamma)= \begin{cases}\lambda|\theta|-\frac{\theta^2}{2 \gamma}, & \text { if }|\theta| \leq \gamma \lambda \\ \frac{1}{2} \gamma \lambda^2, & \text { if }|\theta|>\gamma \lambda\end{cases}
$$

\begin{columns}

\begin{column}{0.4\textwidth}
%\textbf{Left Column}

{\scriptsize
\begin{itemize}\setlength{\itemsep}{1.0em}
    \item As with SCAD, MCP starts by applying same penalization rate as lasso, then smoothly reduces rate to zero as $|\theta|\,\uparrow$
    \item Different from SCAD, MCP immediately starts relaxing the penalization rate, while for SCAD rate remains flat until $|\theta|>\lambda$
    \item Both SCAD and MCP possess oracle property: they can consistently select true model as $n \to \infty$ while lasso may fail
\end{itemize}
}
\end{column}

\begin{column}{0.6\textwidth}
%\textbf{Right Column}

\begin{figure}
      \centering
        \scalebox{0.99}{\includegraphics{figure/nc_penalties_comparison.png}}
        %\caption{\footnotesize lasso vs non-convex SCAD and MCP penalties for scalar parameter $\thetab$}
    \end{figure}

\end{column}

\end{columns}


\end{vbframe}

\begin{vbframe}{Example: Comparing regularizers}

Let's compare coeff paths for lasso, SCAD, and MCP.\\
\vspace{0.15cm}
We simulate $n=100$ samples from the following DGP:
{\small
$$y = \xv^{\top} \thetab + \varepsilon\,,\quad \thetab =(4,-4,-2,2,0,\ldots,0)^{\top} \in \mathbb{R}^{1500}, \quad x_j,\varepsilon \sim \mathcal{N}(0,1)$$
}
\vspace{-1cm}

  \begin{figure}[h]
    \begin{minipage}{0.32\linewidth}
      \vspace{3pt}
      \centerline{\includegraphics[width=\textwidth]{figure/ncpen-compar-lasso.png}}
      %\caption{lasso}
    \end{minipage}
    \begin{minipage}{0.32\linewidth}
      \vspace{3pt}
      \centerline{\includegraphics[width=\textwidth]{figure/ncpen-compar-SCAD.png}}
      %\caption{SCAD}
    \end{minipage}
    \begin{minipage}{0.32\linewidth}
      \vspace{3pt}
      \centerline{\includegraphics[width=\textwidth]{figure/ncpen-compar-mcp.png}}
      %\caption{MCP}
    \end{minipage}
  \end{figure}
Vertical lines mark optimal $\lambda$ from 10CV.\\
\vspace{0.1cm}
\textbf{Conclusion}: Lasso underestimates true coeffs while SCAD/MCP achieve unbiased estimation and better variable selection

\end{vbframe}



% \begin{vbframe}{Regularization for Invariance}
% Another lens through which to view regularization is invariance, i.e., predictions should remain invariant under certain input transformations.\\
% In image classification, label ``cat'' should hold regardless of position or size of relevant object (translation/scale invariance)
% \begin{enumerate}\setlength\itemsep{1.02em}
%     \item \textbf{Pre-processing}: By computing invariant features under transformations, downstream models, too, will respect invariances
%     %\item \textbf{Explicit regularization}: Penalty for changes in model output under transformed inputs is added to loss
%     \item \textbf{Data augmentation}: Extend training data by replicating inputs under invariant transformations (e.g., flipping/rotating images)
%     \begin{figure}
%     \includegraphics[width=0.75\textwidth]{figure_man/data-augmentation-cat.png}\\
%     \end{figure}
%     \item \textbf{Network architecture}: Build invariance property directly into network structure, e.g. CNNs \citebutton{Geometric DL (Bronstein et al., 2021)}{https://arxiv.org/pdf/2104.13478.pdf}
% \end{enumerate}
%\end{vbframe}

\endlecture
\end{document}
