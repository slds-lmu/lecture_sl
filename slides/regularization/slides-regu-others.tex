\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/lasso_ridge_hat.png}
\newcommand{\learninggoals}{
  \item Know $L1$/$L2$ regularization induces bias
  \item Know Lq (quasi-)norm regularization
  \item Understand that L0 regularization simply counts number of non-zero parameters
  \item Know SCAD and MCP
  \item Know regularization for invariance properties
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Other Types of Regularization}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Ridge and lasso are biased estimators} 
Although ridge and lasso have many nice properties, they are biased estimators and the bias does not (necessarily) vanish as $n \rightarrow \infty$.\\
\vspace{0.3cm}

For example, in the orthonormal case ($\Xmat^{\top}\Xmat=\bm{I}$) the bias of the lasso is
$$
\begin{cases}\mathbb{E}\left|\widehat{\theta}_j-\theta_j\right|=0 & \text { if } \theta_j=0 \\ \mathbb{E}\left|\widehat{\theta}_j-\theta_j\right| \approx \theta_j & \text { if }\left|\theta_j\right| \in[0, \lambda] \\ \mathbb{E}\left|\widehat{\theta}_j-\theta_j\right| \approx \lambda & \text { if }\left|\theta_j\right|>\lambda\end{cases}
$$
\vspace{0.3cm}

The bias of the lasso for noise features is thus about $\lambda$ for large $|\theta|$.\\
\vspace{0.2cm}
To reduce the bias/shrinkage of regularized estimators various penalties were proposed, a few of which we briefly introduce now.

\end{vbframe}

\begin{vbframe}{$Lq$ regularization}
Besides $L1$/$L2$ we could use any $Lq$ (quasi-)norm penalty $\lambda \Vert \thetab \Vert_q^q$ \citebutton{Knight and Fu, 2000}{https://websites.umich.edu/~jizhu/jizhu/KnightFu-AoS00.pdf}.


\begin{figure}
  \scalebox{0.5}{\includegraphics{figure_man/lasso_ridge_hat.png}}\\
%\includegraphics[height=2.3cm]{figure_man/contour.pdf}
\caption{{\scriptsize \textit{Top:} loss contours and $L1$/$L2$ constraints.
\textit{Bottom:} Constraints for $Lq$ norms $\sum_j |\theta_j|^q$.}}
\end{figure}
\vspace{-0.4cm}
{\footnotesize
\begin{itemize}
    \item For $q<1$ penalty becomes non-convex but for $q>1$ no sparsity is achieved
    \item Non-convex $Lq$ regularization has some nice properties like \textbf{oracle property} \citebutton{Zou, 2006}{http://users.stat.umn.edu/~zouxx019/Papers/adalasso.pdf}: consistent (+ asy. unbiased) param estimation and var selection
    \item Downside: non-convexity of penalty makes optimization even harder than $L1$ (no unique global minimum but multiple local minima)
\end{itemize}
}
\end{vbframe}


\begin{vbframe}{L0 regularization}

  \begin{itemize}
    \item Consider the $L0$-regularized risk of a model $\fxt$
  $$
  \riskrt = \risket + \lambda \|\thetab\|_0 := \risket + \lambda \sum_j |\theta_j|^0.
  $$
      \item Unlike the $L1$ and $L2$ norms, the $L0$ "norm" simply counts the number of non-zero parameters in the model.
      \vspace{0.3cm}
    \begin{figure}
      \centering
        \scalebox{0.99}{\includegraphics{figure_man/lq-penalty-plots.png}}
        %\tiny{\\ Credit: Christos Louizos}
        \caption{\footnotesize $Lq$ (quasi-)norm penalties for a scalar parameter $\thetab$ for different values of $q$}
    \end{figure}

    \end{itemize}
    
\end{vbframe}

\begin{vbframe} {L0 regularization}

    \begin{itemize}
    \item For any parameter $\theta_j$, $L0$ is zero for $\theta_j = 0$ (defining $0^0 := 0$) and constant on the true support (any $\theta_j \neq 0$)
    \item $L0$ regularization induces sparsity in the parameter vector more aggressively than $L1$ regularization, but does not shrink concrete parameter values as L1 and L2 does (unbiased).
    \item Model selection criteria such as Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are special cases of $L0$ regularization (corresponding to specific values of $\lambda$).
    \item $L0$-regularized risk is not continuous, differentiable or convex
    \item NP-hard to optimize.
      For smaller $n$ and $p$ somewhat tractable, otherwise efficient approximations are still current research.
  \end{itemize}
\end{vbframe}

\begin{vbframe}{SCAD}

The SCAD ({\footnotesize{Smoothly Clipped Absolute Deviations}, \citebutton{Fan and Li, 2007}{https://www.tandfonline.com/doi/full/10.1080/00401706.2020.1801256?casa_token=JhnIrgzTysMAAAAA:Z216Mc0l0qPEBUOW7kL2W0NjHC9TxdU4J6RtVs6ME7MW3_rN7CwqXZMAjKUwZo2Qz5iPd-jzKc4ffA}}) penalty is non-convex regularizer with piece-wise definition using additional hyperparam $\gamma>2$ controlling how fast penalty ``tapers off'':
{\footnotesize
$$
\text{SCAD}(\theta \mid \lambda, \gamma)= \begin{cases}\lambda|\theta| & \text { if }|\theta| \leq \lambda \\ \frac{2 \gamma \lambda|\theta|-\theta^2-\lambda^2}{2(\gamma-1)} & \text { if } \lambda<|\theta|<\gamma \lambda \\ \frac{\lambda^2(\gamma+1)}{2} & \text { if }|\theta| \geq \gamma \lambda\end{cases}
$$
}

\begin{columns}

\begin{column}{0.4\textwidth}
%\textbf{Left Column}

{\scriptsize
The SCAD penalty 
\begin{enumerate}
    \item coincides with the lasso for small values until $|\theta|=\lambda$,
    \item then (smoothly) transitions to a quadratic up to $|\theta|=\gamma \lambda$,
    \item remains constant for all $|\theta|>\gamma \lambda$
\end{enumerate}

Contrary to lasso/ridge, SCAD continuously relaxes penalization rate as $|\theta|$ increases above $\lambda$. %SCAD is asymptotically unbiased due to the ``clipping'' of the penalty.
}
\end{column}

\begin{column}{0.6\textwidth}
%\textbf{Right Column}

\begin{figure}
      \centering
        \scalebox{0.99}{\includegraphics{figure_man/penalties-comparison.pdf}}
        %\caption{\footnotesize lasso vs non-convex SCAD and MCP penalties for scalar parameter $\thetab$}
    \end{figure}

\end{column}

\end{columns}



\end{vbframe}

\begin{vbframe}{MCP}

MCP ({\footnotesize{Minimax Concave Penalty},\citebutton{Zhang, 2010}{https://arxiv.org/pdf/1002.4734.pdf}}) is another non-convex regularizer with a similar idea to SCAD, defined as (for $\gamma>1$):

$$
MCP(\theta | \lambda, \gamma)= \begin{cases}\lambda|\theta|-\frac{\theta^2}{2 \gamma}, & \text { if }|\theta| \leq \gamma \lambda \\ \frac{1}{2} \gamma \lambda^2, & \text { if }|\theta|>\gamma \lambda\end{cases}
$$

\begin{columns}

\begin{column}{0.4\textwidth}
%\textbf{Left Column}

{\scriptsize
\begin{itemize}\setlength{\itemsep}{1.0em}
    \item As with SCAD, MCP starts by applying same penalization rate as lasso, then smoothly reduces rate to zero as $|\theta|\,\uparrow$
    \item Different from SCAD, MCP immediately starts relaxing the penalization rate, while for SCAD rate remains flat until $|\theta|>\lambda$
    \item Both SCAD and MCP possess oracle property: they can consistently select true model as $n \to \infty$ while lasso may fail
\end{itemize}
}
\end{column}

\begin{column}{0.6\textwidth}
%\textbf{Right Column}

\begin{figure}
      \centering
        \scalebox{0.99}{\includegraphics{figure_man/penalties-comparison.pdf}}
        %\caption{\footnotesize lasso vs non-convex SCAD and MCP penalties for scalar parameter $\thetab$}
    \end{figure}

\end{column}

\end{columns}


\end{vbframe}

\begin{vbframe}{Example: regression under different penalties}

  \begin{figure}[h]
    \begin{minipage}{0.32\linewidth}
      \vspace{3pt}
      \centerline{\includegraphics[width=\textwidth]{figure/other-pen-lasso.png}}
      \caption{lasso}
    \end{minipage}
    \begin{minipage}{0.32\linewidth}
      \vspace{3pt}
      \centerline{\includegraphics[width=\textwidth]{figure/other-pen-SCAD.png}}
      \caption{SCAD}
    \end{minipage}
    \begin{minipage}{0.32\linewidth}
      \vspace{3pt}
      \centerline{\includegraphics[width=\textwidth]{figure/other-pen-MCP.png}}
      \caption{MCP}
    \end{minipage}
  \end{figure}

\end{vbframe}



\begin{vbframe}{Regularization for Invariance}
Another lense through which to view regularization is invariance, i.e., predictions should remain invariant under certain input transformations.\\
In image classification, label ``cat'' should hold regardless of position or size of relevant object (translation/scale invariance)
\begin{enumerate}\setlength\itemsep{1.02em}
    \item \textbf{Pre-processing}: By computing invariant features under transformations, downstream models too will respect invariances
    %\item \textbf{Explicit regularization}: Penalty for changes in model output under transformed inputs is added to loss
    \item \textbf{Data augmentation}: Extend training data by replicating inputs under invariant transformations (e.g., flipping/rotating images)
    \begin{figure}
    \includegraphics[width=0.75\textwidth]{figure_man/data-augmentation-cat.png}\\
    \end{figure}
    \item \textbf{Network architecture}: Build invariance property directly into network structure, e.g. CNNs \citebutton{Geometric DL (Bronstein et al., 2021)}{https://arxiv.org/pdf/2104.13478.pdf}
\end{enumerate}


\end{vbframe}

\endlecture
\end{document}
