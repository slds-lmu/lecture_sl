\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{Regularization}{Elastic Net and regularized GLMs}{figure/lasso_ridge_enet_2d.png}{
  \item Compromise between L1 and L2
  \item Regularized logistic regression
}

\begin{vbframe}{Elastic Net as L1/L2 combo \citelink{ZOUHASTIE}}
\vspace{-0.7cm}
\small{
\begin{align*}
\mathcal{R}_{\text{elnet}}(\thetab) &=  \sumin (\yi - \thetab^\top \xi)^2 + \lambda_1 \|\thetab\|_1 + \lambda_2 \|\thetab\|_2^2 \\
&= \sumin (\yi - \thetab^\top \xi)^2 + \lambda \left( (1-\alpha) \|\thetab\|_1 + \alpha \|\thetab\|_2^2\right),\, \alpha=\frac{\lambda_2}{\lambda_1+\lambda_2}, \lambda=\lambda_1+\lambda_2
\end{align*}}
\begin{figure}
\vspace{-0.3cm}
\includegraphics[width=0.5\textwidth]{figure/lasso_ridge_enet_2d.png}\\
\end{figure}
\vspace{-0.3cm}
\begin{itemize}
\item 2nd formula is simply more convenient to interprete hyperpars;\\
    $\lambda$ controls how much we penalize, $\alpha$ sets the ``L2-portion''
\item Correlated features tend to be either selected or zeroed out together
\item Selection of more than $n$ features possible for $p>n$
\end{itemize}
\end{vbframe}

\begin{vbframe} {Simulated Example}

\footnotesize
50 data sets with $n=100$ for setups: $y =\xv^T \thetab + \epsilon; \quad \epsilon \sim N(0,1);
  \quad \xv \sim N(0, \Sigma)$: 
\vspace{-0.3cm}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{center}
{\footnotesize \textbf{Ridge} better for corr. features}: \\ 
$\thetab=(\underbrace{2,\ldots,2}_{5},\underbrace{0,\ldots,0}_{5})$\\
$ \Sigma_{k,l}=0.8^{|k-l|}$ 
  \end{center}
\end{column}
\begin{column}{0.5\textwidth} 
\begin{center}
{\footnotesize \textbf{Lasso} better for sparse without corr.:} \\
$\thetab=(2, 2, 2,\underbrace{0,\ldots,0}_{7})$ \\
$ \Sigma = I_p$ 
\end{center}
\end{column}
\end{columns}

\begin{figure}
\includegraphics[width=\textwidth]{figure/enet_lasso_ridge_mse.png}\\
\end{figure}
{\normalsize $\implies$ elastic net handles both cases well}
\framebreak

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure/enet_tradeoff.png}\\
\end{figure}


\footnotesize
LHS: ridge cannot perform variable selection compared to lasso/e-net. \\
Lasso more frequently ignores relevant features than e-net (longer tails in violin plot).\\
RHS: ridge estimates of noise features hover around $0$ while lasso/e-net produce $0$s.
%Since Elastic Net offers a compromise between Ridge and lasso, it is suitable for both data situations.

\end{vbframe}


% \section{Regularized Logistic Regression}

\begin{vbframe}{Regularized Logistic Regression}

\begin{itemize}

\item Penalties can be added very flexibly to any model based on ERM

\item E.g.: $L1$- or $L2$-penalized logistic regression for high-dim. spaces and feature selection


%\begin{align*}
%\riskrt &= \risket + \lambda \cdot J(\thetab) \\
%&= \sumin \mathsf{log} \left[1 + \exp \left(-\yi f\left(\left.\xi~\right|~ \thetab\right)\right)\right] + \lambda \cdot J(\thetab) \\
%&= \sumin \mathsf{log}\left[1 + \mathsf{exp}\left(-2\yi f\left(\left.\xi~\right|~ \thetab\right)\right)\right] + \lambda \cdot J(\thetab)
%\end{align*}

\item Now: LR with polynomial features for $x_1, x_2$ up to degree 7 and L2 penalty on 2D ``circle data'' below

\end{itemize}

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure/reg_logreg.png}\\
\end{figure}

\begin{itemize}
\item $\lambda = 0$: LR without penalty seems to overfit
\item $\lambda = 0.0001$: We get better
\item $\lambda = 1$: Fit looks pretty good
\end{itemize}


\end{vbframe}


\endlecture
\end{document}
