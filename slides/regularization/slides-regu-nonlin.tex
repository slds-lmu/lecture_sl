\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}



\title{Introduction to Machine Learning}


\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Regularization 
  }{% Lecture title  
    Non-Linear Models and Structural Risk Minimization
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/classifi_nn_w_size_2.png
  }{
  \item Regularization even more important in non-linear models
  \item Norm penalties applied similarly 
  \item Structural risk minimization 
}



%-------------------------------------------------------------------------------

\begin{frame}{Summary: Regularized Risk Minimization}

If we define (supervised) ML in one line, this might be it:

$$
\min_{\thetav} \riskrt= \min_{\thetav} \left(\sumin \Lxyit + \lambda \cdot J(\thetav) \right)
$$

Can choose for task at hand:

\begin{itemize}
  \item \textbf{hypothesis space} of $f$, controls how features influence prediction
  \item \textbf{loss} function $L$, measures how errors are treated
  \item \textbf{regularizer} $J(\thetav)$, encodes inductive bias 
\end{itemize}

\vfill

By varying these choices one can construct a huge number of different ML models. 
Many ML models follow this construction principle or can be interpreted through 
the lens of RRM.

\end{frame}

%-------------------------------------------------------------------------------

\begin{framei}[sep=L]{Regularization in Nonlinear Models}

  \item So far we have mainly considered regularization in LMs
  \item Can in general also be applied to non-linear models; vector-norm penalties require numeric params 
%    \item Often, non-linear models can be seen as LMs based on internally transformed features.  
  \item Here, we typically use $L2$ regularization, which
    still results in parameter shrinkage and weight decay
  \item For non-linear models, regularization is even more important / basically required to prevent overfitting
  \item Commonplace in methods such as NNs, SVMs, or boosting
  \item Prediction surfaces / decision boundaries become smoother

\end{framei}


%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------
\begin{frame}{Regularization in Nonlinear Models}

\splitVCC[0.65]
{{\small
Classification for \texttt{spirals} data.\\
NN with single hidden layer, size 10, $L2$ penalty:}}
{
\imageC[0.9]{figure/nn_size_10.png}
}

\only<1>{\imageC[0.9]{figure/classifi_nn_w_size_1.png}}
\only<2>{\imageC[0.9]{figure/classifi_nn_w_size_2.png}}
\only<3>{\imageC[0.9]{figure/classifi_nn_w_size_3.png}}
\only<4>{\imageC[0.9]{figure/classifi_nn_w_size_4.png}}

%\only<5>{\imageC{figure/classifi_nn_w_size_5.png}}
%\only<6>{\imageC{figure/classifi_nn_w_size_6.png}}

{\small
$\lambda$ affects smoothness of decision boundary and magnitude of weights.\\
When $\lambda$ is 0 (no regularization), absolute weights could be extremely large.
}

\end{frame}

\begin{frame}{Regularization in Nonlinear Models}

Prevention of overfitting can also be seen in CV.\\
Same settings as before, but each $\lambda$ is evaluated with
5x10 REP-CV

\imageC{figure/classifi_nn_err_decay.png}

Typical U-shape with sweet spot between overfitting and underfitting
\end{frame}


%-------------------------------------------------------------------------------
\begin{framei}[sep=L]{Structural Risk Minimization}

  \item Can also see this as an iterative process;\\
  more a ``discrete'' view on things
  %\item Thus far, we only considered adding a complexity penalty to empirical risk minimization. 
  \item SRM assumes that $\Hspace$ can be decomposed into increasingly complex hypotheses: 
  $\Hspace = \cup_{k \geq 1 }\Hspace_{k}$
  \item Complexity parameters can be, e.g. the degree of polynomials in linear models or the size of hidden layers in neural networks

\vfill


\imageC[0.6]{figure_man/fig-regu-srm-1}
% FIGURE SOURCE:https://docs.google.com/drawings/d/1qFoFSyuY4glsNvgYgIZ96yRcznOdA5q3oogI5fVBQ1A/edit?usp=sharing


\end{framei}

\begin{framei}[sep=M]{Structural Risk Minimization}

  \item SRM chooses the smallest $k$ such that the optimal model from $\Hspace_k$ found by ERM or RRM cannot significantly
        be outperformed by a model from a $\Hspace_m$ with $m > k$
  \item Principle of Occam's razor
  \item One challenge might be choosing an adequate complexity measure, as for some models, multiple exist

\imageC[0.6]{figure_man/fig-regu-srm-2}
% FIGURE SOURCE: https://docs.google.com/drawings/d/1mk_qVUbfOYwwmuE0AgmnPiNSMoX--pE_nZsWYND0IhQ/edit?usp=sharing


\end{framei}

%-------------------------------------------------------------------------------
\begin{frame} {Structural Risk Minimization}

{\small
Again \texttt{spirals}.\\
NN with 1 hidden layer, and fixed (small) L2 penalty.}

\vfill


\only<1>{

\splitVCC[0.5]{
\imageC{figure/classifi_nn_size_1.png}
}
{
\imageC{figure/nn_size_1.png}
}

}
\only<2>{

\splitVCC[0.5]{
\imageC{figure/classifi_nn_size_2.png}
}
{
\imageC{figure/nn_size_2.png}
}

}
\only<3>{

\splitVCC[0.5]{
\imageC{figure/classifi_nn_size_3.png}
}
{
\imageC{figure/nn_size_3.png}
}

}
\only<4>{

\splitVCC[0.5]{
\imageC{figure/classifi_nn_size_4.png}
}
{
\imageC{figure/nn_size_5.png}
}

}
\only<5>{

\splitVCC[0.5]{
\imageC{figure/classifi_nn_size_5.png}
}
{
\imageC{figure/nn_size_10.png}
}

}

\only<6>{

\splitVCC[0.5]{
\imageC{figure/classifi_nn_size_6.png}
}
{
\imageC{figure/nn_size_100.png}
}

}

{\small
Size affects complexity and smoothness of decision boundary}


\end{frame}

\begin{frame} {Structural Risk Minimization}
Again, complexity vs CV score. 

\imageC{figure/classifi_nn_err_size.png}

Minimal model with good generalization seems to size=10 

\end{frame}


\begin{frame} {Structural Risk Minimization and RRM}

RRM can also be interpreted through SRM, \\
if we rewrite it in constrained form:

\splitVCC[0.5]
{
\begin{eqnarray*}
\min_{\thetav} && \sumin \Lxyit  \\
  \text{s.t. } && \|\thetav\|_2^2  \leq t \\
\end{eqnarray*}
}{
\imageC[0.6]{figure/ridge_perspectives_04.png}
}

\vfill

Can interpret going through $\lambda$ from large to small as through $t$ from small to large.
Constructs series of ERM problems with hypothesis spaces $\Hspace_\lambda$, 
where we constrain norm of $\thetav$ to unit balls of growing sizes.
\end{frame}


\endlecture
\end{document}

