\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}



\title{Introduction to Machine Learning}


\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Regularization 
  }{% Lecture title  
    Non-Linear Models and Structural Risk Minimization
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/classifi_nn_w_size_2.png
  }{
  \item Regularization even more important in non-linear models
  \item Norm penalties applied similarly 
  \item Structural risk minimization 
}



%-------------------------------------------------------------------------------

\begin{vbframe}{Summary: Regularized Risk Minimization}

If we define (supervised) ML in one line, this might be it:

$$
\min_{\thetav} \riskrt= \min_{\thetav} \left(\sumin \Lxyit + \lambda \cdot J(\thetav) \right)
$$

Can choose for task at hand:

\begin{itemize}
  \item \textbf{hypothesis space} of $f$, controls how features influence prediction
  \item \textbf{loss} function $L$, measures how errors are treated
  \item \textbf{regularizer} $J(\thetav)$, encodes inductive 
  bias 
\end{itemize}

\vfill

By varying these choices one can construct a huge number of different ML models. 
Many ML models follow this construction principle or can be interpreted through 
the lens of RRM.

\end{vbframe}

%-------------------------------------------------------------------------------

\begin{vbframe}{Regularization in Nonlinear Models}

\begin{itemize}
  \item So far we have mainly considered regularization in LMs
  \item Can in general also be applied to non-linear models; vector-norm penalties require numeric params 
%    \item Often, non-linear models can be seen as LMs based on internally transformed features.  
  \item Here, we typically use $L2$ regularization, which
    still results in parameter shrinkage and weight decay
  \item For non-linear models, regularization is even more important / basically required to prevent overfitting
  \item Commonplace in methods such as NNs, SVMs, or boosting
  \item Prediction surfaces / decision boundaries become smoother
\end{itemize}

\end{vbframe}


%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------
\begin{frame}{Regularization in Nonlinear Models}

\begin{center}
\begin{minipage}{0.6\textwidth}
{\small
Classification for \texttt{spirals} data.\\
NN with single hidden layer, size 10, $L2$ penalty:}
\end{minipage}%
\begin{minipage}{0.3\textwidth}
\includegraphics[width=1\textwidth]{figure/nn_size_10.png}
\end{minipage}
\end{center}


\vspace{-0.8cm}
%\vfill

\only<1>{\begin{center}\includegraphics[width=\textwidth]{figure/classifi_nn_w_size_1.png}\end{center}}
\only<2>{\begin{center}\includegraphics[width=\textwidth]{figure/classifi_nn_w_size_2.png}\end{center}}
\only<3>{\begin{center}\includegraphics[width=\textwidth]{figure/classifi_nn_w_size_3.png}\end{center}}
\only<4>{\begin{center}\includegraphics[width=\textwidth]{figure/classifi_nn_w_size_4.png}\end{center}}

%\only<5>{\includegraphics[width=\textwidth]{figure/classifi_nn_w_size_5.png}}
%\only<6>{\includegraphics[width=\textwidth]{figure/classifi_nn_w_size_6.png}}

$\lambda$ affects smoothness of decision boundary and magnitude of weights

\end{frame}

\begin{frame}{Regularization in Nonlinear Models}

Prevention of overfitting can also be seen in CV.\\
Same settings as before, but each $\lambda$ is evaluated with
5x10 REP-CV

\begin{center}\includegraphics[width=1\textwidth]{figure/classifi_nn_err_decay.png}\end{center}

Typical U-shape with sweet spot between overfitting and underfitting
\end{frame}


%-------------------------------------------------------------------------------
\begin{vbframe} {Structural Risk Minimization}

\begin{itemize}
  \item Can also see this as an iterative process;\\
  more a ``discrete'' view on things
  %\item Thus far, we only considered adding a complexity penalty to empirical risk minimization. 
  \item SRM assumes that $\Hspace$ can be decomposed into increasingly complex hypotheses: 
  $\Hspace = \cup_{k \geq 1 }\Hspace_{k}$
  \item Complexity parameters can be, e.g. the degree of polynomials in linear models or the size of hidden layers in neural networks
\end{itemize}

\lz

\begin{center}
\includegraphics[width=0.6\textwidth]{figure_man/fig-regu-srm-1}
% FIGURE SOURCE:https://docs.google.com/drawings/d/1qFoFSyuY4glsNvgYgIZ96yRcznOdA5q3oogI5fVBQ1A/edit?usp=sharing
\end{center}

\framebreak


\begin{itemize}

    \item SRM chooses the smallest $k$ such that the optimal model from $\Hspace_k$ found by ERM or RRM cannot significantly
        be outperformed by a model from a $\Hspace_m$ with $m > k$
  \item Principle of Occam's razor
  \item One challenge might be choosing an adequate complexity measure, as for some models, multiple exist
\end{itemize}

\begin{center}
\includegraphics[width=0.6\textwidth]{figure_man/fig-regu-srm-2}
% FIGURE SOURCE: https://docs.google.com/drawings/d/1mk_qVUbfOYwwmuE0AgmnPiNSMoX--pE_nZsWYND0IhQ/edit?usp=sharing
\end{center}

\end{vbframe}

%-------------------------------------------------------------------------------
\begin{frame} {Structural Risk Minimization}

\small

Again \texttt{spirals}.\\
NN with 1 hidden layer, and fixed (small) L2 penalty. 

\vfill


\only<1>{
\begin{center}
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{figure/classifi_nn_size_1.png}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{figure/nn_size_1.png}
\end{minipage}
\end{center}
}
\only<2>{
\begin{center}
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{figure/classifi_nn_size_2.png}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{figure/nn_size_2.png}
\end{minipage}
\end{center}
}
\only<3>{
\begin{center}
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{figure/classifi_nn_size_3.png}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{figure/nn_size_3.png}
\end{minipage}
\end{center}
}
\only<4>{
\begin{center}
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{figure/classifi_nn_size_4.png}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{figure/nn_size_5.png}
\end{minipage}
\end{center}
}
\only<5>{
\begin{center}
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{figure/classifi_nn_size_5.png}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{figure/nn_size_10.png}
\end{minipage}
\end{center}
}

\only<6>{
\begin{center}
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{figure/classifi_nn_size_6.png}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{figure/nn_size_100.png}
\end{minipage}
\end{center}
}

Size affects complexity and smoothness of decision boundary


\end{frame}

\begin{frame} {Structural Risk Minimization}
Again, complexity vs CV score. 

\begin{center}\includegraphics[width=\textwidth]{figure/classifi_nn_err_size.png}\end{center}

Minimal model with good generalization seems to size=10 

\end{frame}


\begin{frame} {Structural Risk Minimization and RRM}

RRM can also be interpreted through SRM, \\
if we rewrite it in constrained form:

\begin{columns}
\begin{column}{0.5\textwidth}
\begin{eqnarray*}
\min_{\thetav} && \sumin \Lxyit  \\
  \text{s.t. } && \|\thetav\|_2^2  \leq t \\
\end{eqnarray*}
\end{column}
\begin{column}{0.5\textwidth}
\begin{figure}
\includegraphics[width=0.6\textwidth]{figure/ridge_perspectives_04.png}
\end{figure}
\end{column}
\end{columns}

\vspace{0.5cm}

Can interpret going through $\lambda$ from large to small as through $t$ from small to large.
Constructs series of ERM problems with hypothesis spaces $\Hspace_\lambda$, 
where we constrain norm of $\thetav$ to unit balls of growing sizes.
\end{frame}


\endlecture
\end{document}

