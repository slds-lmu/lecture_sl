\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}



\title{Introduction to Machine Learning}


\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Regularization 
  }{% Lecture title  
    Non-Linear Models and Structural Risk Minimization
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/fig-regu-nonlin-2.png
  }{
  \item Understand that regularization and parameter shrinkage can be applied to non-linear models
  \item Know structural risk minimization 
}



%-------------------------------------------------------------------------------

\begin{vbframe}{Summary: Regularized Risk Minimization}

If we should define (supervised) ML in only one line, this might be it:

$$
\min_{\thetab} \riskrt= \min_{\thetab} \left(\sumin \Lxyit + \lambda \cdot J(\thetab) \right)
$$

We can choose for a task at hand:

\begin{itemize}
  \item the \textbf{hypothesis space} of $f$, which determines how features can 
  influence the predicted $y$
  \item the \textbf{loss} function $L$, which measures how errors should be treated
  \item the \textbf{regularization} $J(\thetab)$, which encodes our inductive 
  bias and preference for certain simpler models
\end{itemize}

\vfill

By varying these choices one can construct a huge number of different ML models. 
Many ML models follow this construction principle or can be interpreted through 
the lens of regularized risk minimization.

\end{vbframe}

%-------------------------------------------------------------------------------

\begin{vbframe}{Regularization in Nonlinear Models}

\begin{itemize}
  \item So far we have mainly considered regularization in LMs.
  \item Can also be applied to non-linear models (with numeric parameters), where it is 
  often important to prevent overfitting.
    \item Often, non-linear models can be seen as LMs based on internally transformed features.  
  \item Here, we typically use $L2$ regularization, which
    still results in parameter shrinkage and weight decay.
  \item Adding regularization is commonplace and sometimes crucial in non-linear methods such as NNs, SVMs, or boosting.
  \item By adding regularization, prediction surfaces in regression and 
  classification become smoother. 
\end{itemize}

\end{vbframe}


%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------
\begin{frame}{Regularization in Nonlinear Models}

\begin{center}
\begin{minipage}{0.6\textwidth}
{\small
Classification for the \texttt{spirals} data.
Neural network with single hidden layer containing 10 neurons, regularized with $L2$:}
\end{minipage}%
\begin{minipage}{0.3\textwidth}
\includegraphics[width=1\textwidth]{figure/nn_size_10.png}
\end{minipage}
\end{center}


\vspace{-0.8cm}
%\vfill

\only<1>{\begin{center}\includegraphics[width=\textwidth]{figure/fig-regu-nonlin-1.png}\end{center}}
\only<2>{\begin{center}\includegraphics[width=\textwidth]{figure/fig-regu-nonlin-2.png}\end{center}}
\only<3>{\begin{center}\includegraphics[width=\textwidth]{figure/fig-regu-nonlin-3.png}\end{center}}
\only<4>{\begin{center}\includegraphics[width=\textwidth]{figure/fig-regu-nonlin-4.png}\end{center}}

%\only<5>{\includegraphics[width=\textwidth]{figure/fig-regu-nonlin-5.png}}
%\only<6>{\includegraphics[width=\textwidth]{figure/fig-regu-nonlin-6.png}}

Varying $\lambda$ affects smoothness of the decision boundary and magnitude of network weights

\end{frame}

\begin{frame}{Regularization in Nonlinear Models}

The prevention of overfitting can also be seen in CV.
Same settings as before, but each $\lambda$ is evaluated with
repeated CV (10 folds, 5 reps). 

\begin{center}\includegraphics[width=0.7\textwidth]{figure/fig-regu-nonlin-srm-1.png}\end{center}

We see the typical U-shape with the sweet spot between overfitting (LHS, low $\lambda$) and 
underfitting (RHS, high $\lambda$) in the middle.
\end{frame}


%-------------------------------------------------------------------------------
\begin{vbframe} {Structural Risk Minimization}

\begin{itemize}
  % \item Complex models generalize poorly (overfitting) if merely the empirical risk is optimized. 
  \item Thus far, we only considered adding a complexity penalty to empirical risk minimization. 
  \item Instead,  structural risk minimization (SRM) assumes that the hypothesis space $\Hspace$ can be decomposed into increasingly complex hypotheses (size or capacity): $\Hspace = \cup_{k \geq 1 }\Hspace_{k}$. 
  \item Complexity parameters can be, e.g. the degree of polynomials in linear models or the size of hidden layers in neural networks.  
\end{itemize}

\begin{center}
\includegraphics[width=0.5\textwidth]{figure_man/fig-regu-srm-1}
% FIGURE SOURCE:https://docs.google.com/drawings/d/1qFoFSyuY4glsNvgYgIZ96yRcznOdA5q3oogI5fVBQ1A/edit?usp=sharing
\end{center}

\framebreak


\begin{itemize}

    \item SRM chooses the smallest $k$ such that the optimal model from $\Hspace_k$ found by ERM or RRM cannot significantly
        be outperformed by a model from a $\Hspace_m$ with $m > k$.
  \item By this, the simplest model can be chosen, which minimizes the generalization bound.  
  \item One challenge might be choosing an adequate complexity measure, as for some models, multiple complexity measures exist.
\end{itemize}

\begin{center}
\includegraphics[width=0.6\textwidth]{figure_man/fig-regu-srm-2}
% FIGURE SOURCE: https://docs.google.com/drawings/d/1mk_qVUbfOYwwmuE0AgmnPiNSMoX--pE_nZsWYND0IhQ/edit?usp=sharing
\end{center}

\end{vbframe}

%-------------------------------------------------------------------------------
\begin{frame} {Structural Risk Minimization}

\small

Classification for the \texttt{spirals} data.
NN with 1 hidden layer, and fixed (small) L2 penalty. \\
Varying the size of the hidden layer affects smoothness of the decision boundary:


\vfill


\only<1>{
\begin{center}
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{figure/fig-regu-nonlin-size-1.png}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{figure/nn_size_1.png}
\end{minipage}
\end{center}
}
\only<2>{
\begin{center}
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{figure/fig-regu-nonlin-size-2.png}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{figure/nn_size_2.png}
\end{minipage}
\end{center}
}
\only<3>{
\begin{center}
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{figure/fig-regu-nonlin-size-3.png}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{figure/nn_size_3.png}
\end{minipage}
\end{center}
}
\only<4>{
\begin{center}
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{figure/fig-regu-nonlin-size-4.png}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{figure/nn_size_5.png}
\end{minipage}
\end{center}
}
\only<5>{
\begin{center}
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{figure/fig-regu-nonlin-size-5.png}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{figure/nn_size_10.png}
\end{minipage}
\end{center}
}

\only<6>{
\begin{center}
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{figure/fig-regu-nonlin-size-6.png}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\linewidth]{figure/nn_size_100.png}
\end{minipage}
\end{center}
}



\end{frame}

\begin{frame} {Structural Risk Minimization}
Again, complexity vs CV score. 

\begin{center}\includegraphics[width=0.7\textwidth]{figure/fig-regu-nonlin-srm-2.png}\end{center}

A minimal model with good generalization seems to have ca. 10 hidden neurons.

\end{frame}


\begin{frame} {Structural Risk Minimization and RRM}

Note that normal RRM can also be interpreted through SRM, if we rewrite the penalized ERM as constrained ERM.

\begin{columns}
\begin{column}{0.5\textwidth}
\begin{eqnarray*}
\min_{\thetab} && \sumin \Lxyit  \\
  \text{s.t. } && \|\thetab\|_2^2  \leq t \\
\end{eqnarray*}
\end{column}
\begin{column}{0.5\textwidth}
\begin{figure}
\includegraphics[width=0.6\textwidth]{figure_man/ridge_hat.png}
\end{figure}
\end{column}
\end{columns}

\vspace{0.5cm}

We can interpret going through $\lambda$ from large to small as through $t$ from small to large.
This constructs a series of ERM problems with hypothesis spaces $\Hspace_\lambda$, 
where we constrain the norm of $\thetab$ to unit balls of growing size.
\end{frame}


\endlecture
\end{document}

