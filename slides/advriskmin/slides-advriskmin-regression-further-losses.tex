\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Advanced Risk Minimization
  }{% Lecture title  
    Advanced Regression Losses
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/plot_loss_overview.png
  }{
  \item Know the Huber loss
  \item Know the log-cosh loss
  \item Know the Cauchy loss
  \item Know the log-barrier loss
  \item Know the $\eps$-insensitive loss
  \item Know the quantile loss
}

\begin{vbframe}{Advanced Loss Functions}
Special loss functions can be used to estimate non-standard posterior components, 
    to measure errors in a custom way or are designed to have special properties like robustness.

\vspace{1cm}

Examples:
\begin{itemize}
\item Quantile loss: Overestimating a clinical parameter might not be as bad as underestimating it.
\item Log-barrier loss: Extremely under- or overestimating demand in production would put company profit at risk.
\item $\eps$-insensitive loss: A certain amount of deviation in production does no harm, larger deviations do.
\end{itemize}
% \item Sometimes a custom loss must be designed specifically for the given application.
% \item Some learning algorithms use specific loss functions, e.g., the hinge loss for SVMs.
% \end{itemize}
\end{vbframe}

\begin{vbframe}{Huber Loss}

% Description
\vspace*{-0.5cm}

$$
\Lyf = \begin{cases}
  \frac{1}{2}(y - f)^2  & \text{ if } |y - f| \le \epsilon \\
  \epsilon |y - f|-\frac{1}{2}\epsilon^2 \quad & \text{ otherwise }
  \end{cases}, \quad \epsilon > 0
$$

\begin{itemize}
\item Piece-wise combination of $L1$/$L2$ to have robustness/smoothness
\item Analytic properties: convex, differentiable (once)
% \item Combines advantages of $L1$ and $L2$ loss: differentiable + robust
\end{itemize}

% \vspace*{-1cm}

% \begin{center}
% \includegraphics[width = 0.45\textwidth]{figure/loss_huber_plot.png}~~\includegraphics[width = 0.45\textwidth]{figure_man/loss_huber_plot2.png}\\
% \end{center}

\begin{center}
\includegraphics[width = 0.65\textwidth]{figure/loss_huber_plot.png}
\end{center}

\vspace*{-0.2cm}

\begin{itemize}
\item Risk minimizer and optimal constant do not have a closed-form solution. To fit a model numerical optimization is necessary. 
\item Solution behaves like \textbf{trimmed mean}: a (conditional) mean of two (conditional) quantiles. % (the location depends on the distribution and the value of $\epsilon$).  
\end{itemize}

% \vspace*{0.2cm}

% \textbf{Optimal constant model:}
% \begin{itemize}
% \item Similarly, there is no closed-form solution for the optimal constant model. 
% \item Numerical optimization methods are necessary. 
% \item The \enquote{optimal} solution can only be approached to a certain degree of accuracy via iterative optimization.
% \end{itemize}

\end{vbframe}

\begin{vbframe}{Log-cosh Loss \citelink{SALEH2022STATISTICAL}}

% Confirmed with Bernd: use def from https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0

$$
\Lyf = \log \left( \cosh(\left|y - f\right|) \right) \quad \text{where}\,\text{cosh}(x):=\frac{e^x+e^{-x}}{2}
% \log \left(\tfrac{1}{2} (\left|y - f\right| / c)^2 + 1\right), \quad c \in \R
$$

\normalsize
\begin{itemize}
\item Logarithm of the hyperbolic cosine of the residual.
\item Approximately equal to $0.5 (\left|y - f\right|)^2$ for small residuals and to $\left|y - f\right| - \log 2$ for large residuals, meaning it works a smoothed out $L1$ loss using $L2$ around the origin. %like $L2$ loss for small residuals, but is less outlier-sensitive due to its $L1$ approximation for larger residuals.
\item Has all the advantages of Huber loss and is, moreover, twice differentiable everywhere.
\end{itemize}

\begin{center}
% \includegraphics[width = 0.6\textwidth]{figure_man/loss_cauchy_plot1.png}
\includegraphics[width = 0.5\textwidth]{figure/loss_logcosh.png}
\end{center}

\framebreak

What is the idea behind the log-cosh loss?
\vspace{-0.3cm}
\begin{columns}

\begin{column}{0.4\textwidth}
%\textbf{Left Column}

{\scriptsize
Essentially, we  
\begin{enumerate}\setlength{\itemsep}{0.32em}
    \item take derivative of $L1$ loss w.r.t.~$y-f$, which is the $\text{sign}(y-f)$ function
    \item eliminate discontinuity at $0$ by approximating $\text{sign}(y-f)$ using the cont. differentiable $\text{tanh}(y-f)$
    \item finally integrate the smoothed sign function ``up again'' to obtain smoothed $L1$ loss $\log(\text{cosh}(y-f))=\log(\text{cosh}(|y-f|))$
\end{enumerate}
}
\end{column}

\begin{column}{0.7\textwidth}
%\textbf{Right Column}

\begin{figure}
      \centering
        \scalebox{0.99}{\includegraphics{figure_man/logcosh-derivation.png}}
        %\caption{\footnotesize lasso vs non-convex SCAD and MCP penalties for scalar parameter $\thetab$}
    \end{figure}

\end{column}

\end{columns}

The log-cosh approach to obtain a differentiable approximation of the $L1$ loss can also be extended to  differentiable quantile/pinball losses.

\framebreak

\textbf{The cosh($\theta,\sigma$) distribution}:\\
The (normalized) reciprocal $\text{cosh}(x)$ defines a pdf by its positivity on $\mathbb{R}$ and since $\int_{-\infty}^{\infty} \frac{1}{\pi \text{cosh}(x)} \text{d}x = 1$.\\
\vspace{0.1cm}
We can define a location-scale family of distributions (using $\theta$ and $\sigma$) resembling Gaussians with \textbf{heavier tails}. 

It is easy to check that ERM using the log-cosh loss is equivalent to MLE of the $\text{cosh}(\theta,1)$ distribution.

\begin{columns}

\begin{column}{0.5\textwidth}
%\textbf{Left Column}

{\normalsize 
\begin{itemize}\setlength{\itemsep}{0.32em}
    \item $p(x | \theta, \sigma)=\frac{1}{\pi \sigma \cosh \left(\frac{x-\theta}{\sigma}\right)}$
    \item $\mathbb{E}_{X\sim p}[X]=\theta$
    \item $\text{Var}_{X \sim p}[X]=\frac{1}{4}(\pi^2 \sigma^2)$
    \item {\footnotesize $\thetah^{MLE}=\argmax_{\theta} \prod_{i=1}^n \frac{1}{\pi \cosh \left(x_i-\theta\right)} \equiv$}\\{\footnotesize $\thetah = \argmin_{\theta}\sumin \log(\text{cosh}(x_i-\theta))$}
\end{itemize}
}
\end{column}

\begin{column}{0.7\textwidth}
%\textbf{Right Column}

\begin{figure}
      \centering
        \scalebox{1}{\includegraphics{figure_man/cosh-gaussian-densities.png}}
        %\caption{\footnotesize lasso vs non-convex SCAD and MCP penalties for scalar parameter $\thetab$}
    \end{figure}

\end{column}

\end{columns}


\end{vbframe}

\begin{vbframe}{Cauchy loss}

% Confirmed with Bernd: use def from https://www.user.tu-berlin.de/mtoussai/teaching/15-MachineLearning/15-MachineLearning-script.pdf, p. 24 

$$
\Lyf = \frac{c^2}{2} \log \left( 1 + \left( \frac{\left|y - f\right|}{c}\right)^2 \right), 
\quad c \in \R
% \log \left(\tfrac{1}{2} (\left|y - f\right| / c)^2 + 1\right), \quad c \in \R
$$

\normalsize
\begin{itemize}
\item Particularly robust toward outliers (controllable via $c$).
\item Analytic properties: differentiable, but not convex! 
\end{itemize}

\vfill

\begin{center}
% \includegraphics[width = 0.6\textwidth]{figure_man/loss_cauchy_plot1.png}
\includegraphics[width = 0.6\textwidth]{figure/loss_cauchy.png}
\end{center}

\end{vbframe}

\begin{comment}
\begin{vbframe}{Log-Barrier Loss}

\begin{small}
\[
  \Lyf = \left\{\begin{array}{lr}
        -\epsilon^{2} \cdot \log \Bigl( 1 - \Bigl(\frac{\left|y - f\right|}{\epsilon}\Bigr)^2 \Bigr) & \text{if } \left|y-f\right| \leq \epsilon \\
        \infty & \text{if } \left|y-f\right|  > \epsilon
        \end{array}\right.
  \]
\end{small}

\begin{itemize}
\item Behaves like $L2$ loss for small residuals.
\item We use this if we don't want residuals larger than $\epsilon$ at all.
\item No guarantee that the risk minimization problem has a solution.
\item Plot shows log-barrier loss for $\epsilon=2$:
\end{itemize}

\begin{center}
% \includegraphics[width = 0.8\textwidth]{figure_man/log-barrier01.png}
\includegraphics[width = 0.8\textwidth]{figure/loss_logbarrier_1.png}
\end{center}


\end{vbframe}



\begin{vbframe}{Log-Barrier loss}

\begin{itemize}
 % \item Similarly to the Huber loss, there is no closed-form solution for the optimal constant model $f = \thetab$ w.r.t. the log-barrier loss. Numerical optimization is necessary.
 \item Note that the optimization problem has no (finite) solution if there is no way to fit a constant where all residuals are smaller than $\epsilon$.  
\end{itemize}

\vspace{0.2cm}

\begin{center}
% \includegraphics[width = 9cm ]{figure_man/log_barrier2.png} 
\includegraphics[width = \textwidth]{figure/loss_logbarrier_2.png}
\end{center}

% \framebreak 
% 
% 
% Note that the optimization problem has no (finite) solution if there is no way to fit a constant where all residuals are smaller than $a$. 
% 
% \vspace{- 0.2cm}
% 
% 
% \begin{center}
% \includegraphics[width = 0.8\textwidth]{figure_man/log_barrier_2_1.png} \\
% \end{center}

% We see that the constant model fitted w.r.t. Huber loss in fact lies between L1- and L2-Loss.

\end{vbframe}

\begin{vbframe}{$\eps$-Insensitive loss}

\vspace*{-0.3cm}
$$
\Lyf =  \begin{cases}
  0  & \text{if } |y - f| \le \epsilon \\
  |y - f|-\epsilon & \text{otherwise }
  \end{cases}, \quad \epsilon \in \R_{+}
$$
\begin{itemize}
\item Modification of $L1$ loss, errors below $\epsilon$ accepted without penalty.
\item Used in SVM regression.
\item Properties: convex and not differentiable for $ y - f \in \{-\epsilon, \epsilon\}$.
\end{itemize}

\vfill

\begin{center}
% \includegraphics[width = 10cm, height = 4.7cm]{figure_man/2_6_loss_epsilon_plot1.png} \\
\includegraphics[width = 0.8\textwidth]{figure/loss_eps_insensitive.png}
\end{center}

\end{vbframe}
\end{comment}

%% BB: der beweis hier sieht falsch aus...!
%% \begin{vbframe}{$\epsilon$-insensitive Loss: Optimal Constant}

%% % Derive Constant model and eps-insens loss
%What is the optimal constant model $f = \thetab$ w.r.t. the $\epsilon$-insensitive loss $\Lyf =  |y - f| ~ \mathds{1}_{ \left\{|y - f| > \epsilon \right\}}$?

%\vspace{-0.2cm}
%\begin{eqnarray*}
%\hat \thetab&=& \argmin_{\thetab\in \R}\sumin \Lyfi \\
%&=& \argmin_{\thetab\in \R} \sum_{i \in I_\eps} \left| \yi - \thetab \right| - \eps  \\
%&=& \argmin_{\thetab\in \R} \sum_{i \in I_\eps} \left| \yi - \thetab \right| - \sum_{i \in I_\eps} \eps \\
%&=& \text{median}\left(\left\{\yi ~|~ i \in I_\eps\right\}\right) - |I_\eps| \cdot \eps \\
%\end{eqnarray*}

%with $I_\eps := \left\{i: |\yi - fi| \le \eps \right\}$.

%% \framebreak

%% 
%% < <eps-loss-plot, fig.height = 5.5, include = FALSE>>=
%%   
%%   plotConstantModel(df, c("L2", "L1", "quant25", "quant75", "Huber1", "eps2"))
%% @
%% 
%% \begin{center}
%% \includegraphics[width = 10cm]{figure/eps-loss-plot-1.pdf} \\
%% \end{center}

%\end{vbframe}

\begin{vbframe}{Quantile Loss / Pinball Loss}
\vspace{-0.3cm}

\small
$$
\Lyf = \begin{cases} (1 - \alpha) (f - y) & \text{ if } y < f\\
\alpha (y - f) & \text{ if } y \ge f
\end{cases}, \quad \alpha \in (0, 1)
$$


\normalsize
\begin{itemize}
\item Extension of $L1$ loss (equal to $L1$ for $\alpha = 0.5$).
\item Weights either positive or negative residuals more strongly. 
\item $\alpha<0.5$ $(\alpha>0.5)$ penalty to over-estimation (under-estimation)
\item Risk minimizer is (conditional) 
    $\alpha$-quantile (median for $\alpha=0.5$).
\end{itemize}

\vfill

\begin{center}
% \includegraphics[width = 10cm, height = 4.7cm]{figure_man/2_3_loss_pinball_plot2.png}
\includegraphics[width = 0.8\textwidth]{figure/loss_quantile.png}
\end{center}

% \framebreak

% BB: this proof cannot be true, it simply states what it claimed???
% What is the optimal constant model $f = \thetab$ w.r.t.\ the quantile loss?
% \vspace{-0.2cm}
% \begin{eqnarray*}
% \thetah&=& \argmin_{\thetab\in \R}\sumin \Lxyi \\
% \Leftrightarrow\quad 
% \thetah &=& \argmin_{\thetab \in \R}\left\{ (1 - \alpha) \sum_{\yi<\thetab}  \left|\yi-\thetab\right| + \alpha \sum_{\yi \geq\thetab}  \left|\yi-\thetab\right|\right\} \\
% \Leftrightarrow\quad \thetah &=& Q_\alpha(\{\yi\})
% \end{eqnarray*}

% where $Q_\alpha(\cdot)$ computes the empirical $\alpha$-quantile of $\{\yi\}, i = 1, ..., n$.

\end{vbframe}




\endlecture

\end{document}
