\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}


\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{
Advanced Risk Minimization
}{
Advanced Regression Losses
}{
figure/loss_overview.png
}{
\item Huber loss
\item Log-Cosh loss
\item Cauchy loss
\item $\eps$-Insensitive loss
\item Quantile loss
}

\begin{framei}[sep=M, fs=large]{Advanced Loss Functions \furtherreading{WANG2020COMPRE}}

\item Handle errors in custom fashion
\item Model other error distributions\\
(see section on max. likelihood)
\item Induce properties like robustness
\item Handle other predictive tasks

\vfill


\imageC[0.55]{figure/loss_overview.png}

% Examples:
% \begin{itemize}
% \item Quantile loss: Overestimating a clinical parameter might not be as bad as underestimating it.
% \item Log-barrier loss: Extremely under- or overestimating demand in production would put company profit at risk.
% \item $\eps$-insensitive loss: A certain amount of deviation in production does no harm, larger deviations do.
% \end{itemize}
% % \item Sometimes a custom loss must be designed specifically for the given application.
% % \item Some learning algorithms use specific loss functions, e.g., the hinge loss for SVMs.
% % \end{itemize}
\end{framei}

\begin{frame}{Huber Loss}

% Description

$$
\Lxy = \begin{cases}
  \frac{1}{2}(y - \fx)^2  & \text{ if } |y - \fx| \le \epsilon \\
  \epsilon |y - \fx|-\frac{1}{2}\epsilon^2 \quad & \text{ otherwise }
  \end{cases} \quad \epsilon > 0
$$

\begin{itemize}
\item Piece-wise combination of $L1$/$L2$ to have robustness/smoothness
\item Analytic properties: convex, differentiable (once)
\end{itemize}


\imageC[0.65]{figure/loss_huber.png}


\begin{itemize}
\item No closed-form solution even for constant or linear model
\item Solution behaves like \textbf{trimmed mean}:\\
a (conditional) mean of two (conditional) quantiles  
\end{itemize}

\end{frame}

\begin{frame}{Log-cosh Loss \furtherreading{SALEH2022STATISTICAL}}

% Confirmed with Bernd: use def from https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0

$$
\Lxy = \log ( \cosh(\left|y - \fx\right|) ) \qquad \text{cosh}(x)=\frac{e^x+e^{-x}}{2}
$$

\vfill

\begin{itemize}
%\item Logarithm of the hyperbolic cosine of the residual.
\item Approx. $0.5 (\left|y - \fx\right|)^2$ for small residuals;\\
$\left|y - \fx\right| - \log 2$ for large residuals
\item Smoothed combo of $L1$ / $L2$ loss
\item Similar to Huber, but twice differentiable
\end{itemize}

\vfill


\imageC[0.5]{figure/loss_logcosh.png}


\end{frame}

\begin{frame}{Log-cosh Loss \furtherreading{SALEH2022STATISTICAL}}
%What is the idea behind the log-cosh loss?

\vfill
\splitV[0.4]{
Essential idea:
\begin{enumerate}
    \item Derivative of $L1$ \\
    w.r.t. residual 
    %$\text{sign}(y-\fx)$ function
    \item Approx. $\sign$ with $\tanh$
    %$\text{sign}(y-\fx)$ using the cont. differentiable $\text{tanh}(y-\fx)$
    \item Integrate ``up again'' 
\end{enumerate}
}
{\imageC{figure/logcosh-derivation.png}}

\vfill

Same trick can be used to get differentiable pinball losses

\end{frame}

\begin{frame}{Log-cosh Loss \furtherreading{SALEH2022STATISTICAL}}

\textbf{cosh($\theta,\sigma$) distribution}:\\

\begin{itemizeL}

\item Normalized reciprocal $\text{cosh}(x)$ is pdf: 
positive and $\int_{-\infty}^{\infty} \frac{1}{\pi \text{cosh}(x)} \text{d}x = 1$

\item Location-scale type ($\theta, \sigma$) 
resembling Gaussian with heavy tails

\item ERM using log-cosh 
is equivalent to MLE of $\text{cosh}(\theta,1)$ distribution

\end{itemizeL}

\vfill

\splitV[0.52]{
\begin{itemizeM}[small]\setlength{\itemsep}{0.32em}
    \item $p(x | \theta, \sigma)=\frac{1}{\pi \sigma \cosh (\frac{x-\theta}{\sigma})}$
    \item $\E_{x \sim p}[x] = \theta$
    \item $\var_{x \sim p}[x]=\frac{1}{4} \pi^2 \sigma^2 $
    \item {\footnotesize $\thetah^{MLE}=\argmax_{\theta} \prodin \frac{1}{\pi \cosh (\yi-\theta)} =$}\\{\footnotesize $\thetah = \argmin_{\theta}\sumin \log(\text{cosh}(\yi-\theta))$}
\end{itemizeM}
}
{\imageL[1.1]{figure/logcosh-derivation-cosh-gaussian-densities.png}}


\end{frame}

\begin{frame}{Cauchy loss}

% Confirmed with Bernd: use def from https://www.user.tu-berlin.de/mtoussai/teaching/15-MachineLearning/15-MachineLearning-script.pdf, p. 24 

$$
\Lxy = \frac{c^2}{2} \log ( 1 + ( \frac{\left|y - \fx\right|}{c})^2 ), 
\quad c \in \R
$$

\begin{itemize}
\item Particularly robust toward outliers (controllable via $c$)
\item Analytic properties: differentiable, but not convex
\end{itemize}

\vfill


\imageC[0.6]{figure/loss_cauchy.png}


\end{frame}

\begin{framei}[sep=M]{Telephone data}

\item Illustrate the effect of robust losses on telephone data set 
\item Nr. of calls (in 10mio units) in Belgium 1950-1973
\item Outliers due to a change in measurement without re-calibration 

\vfill

\imageC[0.8]{figure/telephone-data.pdf}


\end{framei}


\begin{frame}{$\eps$-Insensitive loss}

$$
\Lxy =  \begin{cases}
  0  & \text{if } |y - \fx| \le \epsilon \\
  |y - \fx|-\epsilon & \text{otherwise }
  \end{cases}, \quad \epsilon \in \R_{+}
$$

\begin{itemize}
\item Modification of $L1$, errors below $\epsilon$ get no penalty
\item Used in SVM regression
\item Properties: convex, not differentiable for $ y - \fx \in \{-\epsilon, \epsilon\}$
\end{itemize}

\vfill


\imageC[0.8]{figure/loss_eps_insensitive.png}


\end{frame}


\begin{frame}{Quantile Loss / Pinball Loss}

$$
\Lxy = \begin{cases} (1 - \alpha) (\fx - y) & \text{ if } y < \fx\\
\alpha (y - \fx) & \text{ if } y \ge \fx
\end{cases}, \quad \alpha \in (0, 1)
$$

\begin{itemize}
\item Extension of $L1$ loss (equal to $L1$ for $\alpha = 0.5$).
\item Penalizes either over- or under-estimation more
%\item $\alpha<0.5$ $(\alpha>0.5)$ penalty to over-estimation (under-estimation)
\item Risk minimizer is (conditional) 
    $\alpha$-quantile (median for $\alpha=0.5$)
\end{itemize}

\vfill


\imageC[0.8]{figure/loss_quantile.png}


\end{frame}

\begin{framei}[sep=M]{Quantile Loss / Pinball Loss}

\item Simulate $n=200$ samples from heteroskedastic LM 
\item $y = 1+0.2x+\varepsilon$; 
$\quad \varepsilon \sim \mathcal{N}(0, 0.5+0.5x)$; $\quad x \sim \mathcal{U}[0,10]$
\item Fit LM with pinball losses to estimate $\alpha$-quantiles 

\imageC[0.62]{figure/quantile-regression.pdf}


\end{framei}


\endlecture

\end{document}
