\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-hpo}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{
  Advanced Risk Minimization
  }{
  Risk Minimization Basics
  }{
  figure_man/optimization_steps.jpeg
  }{
  \item Risk minimization and ERM recap
  \item Bayes optimal model, Bayes risk
  \item Bayes regret, estimation and approximation error    
  \item Optimal constant model
  \item Consistency
}

\begin{frame}{Empirical Risk Minimization}
To learn a model, we usually do ERM:
$$\riskef = \sumin \Lxyi$$

\begin{itemizeM}[small]
    \item observations $(\xi, \yi) \in  \Xspace \times \Yspace$
%    so from feature and target space
    \item model $f_{\Hspace}:\Xspace \rightarrow \R^g$, 
    %$\fx$ is a model 
    from hypothesis space $\Hspace$;
    maps a feature vector to output score;
    often we omit $\Hspace$ in index 
    \item loss $L:\Yspace\times\R^g\rightarrow\R$ ,
    %$\Lxy$ 
    measures error between label and prediction
    \item data generating process (DGP) $\Pxy$, we assume %$(\xv,y) \sim \Pxy$ and 
    $(\xi, \yi)  \overset{\text{i.i.d.}}{\sim} \Pxy$  
    %\item $f_{\Hspace}^{\ast}$ is the minimizer of the theoretical risk $\riskf$ over $\Hspace$ and $\hat{f}_{\Hspace}$ the minimizer of $\riskef$
\end{itemizeM}

Minimizing theoretical risk, so expected loss over DGP, is major goal:

$$ \risk (f) := \E_{xy} [\Lxy] = \int \Lxy \text{d}\Pxy $$

\end{frame}

\begin{frame}{Two short examples}
\textbf{Regression with linear model:}\\
\begin{itemize}
    \item Model: $f(\xv) = \thetav^\top \xv + \theta_0$
    \item Squared loss:  
    $\Lxy = (y-\fx)^2$
    \item Hypothesis space: $$\Hspace_{\text{lin}} = \left\{ \xv \mapsto \thetav^\top \xv + \theta_0 : \thetav \in \R^d, \theta_0 \in \R \right\}$$
\end{itemize}

\vfill

\textbf{Binary classification with shallow MLP:}\\
\begin{itemize}
    \item Model: $f(\xv) = \pi(\xv)= \sigma(\bm{w}_2^{\top} \text{ReLU}(\bm{W}_1 \xv + \bm{b}_1) + b_2)$
    \item Bernoulli / Log / Cross-Entropy loss: $\Lpixy = -(y\log(\pix)+(1-y)\log(1-\pix))$\\ 
    \item Hypothesis space: {\small $$\Hspace_{\text{MLP}} = \left\{ \xv \mapsto \sigma(\bm{w}_2^{\top} \text{ReLU}(\bm{W}_1 \xv + \bm{b}_1) + b_2): \mathbf{W}_1 \in \R^{h \times d}, \mathbf{b}_1 \in \R^h, \mathbf{w}_2 \in \R^h, b_2 \in \R \right\}$$}
\end{itemize}
  
\end{frame}

\begin{frame}[c]{Hypothesis Spaces and Parametrization}

We often write $\riskf$, but finding an optimal $f$ is operationalized as finding optimal $\thetav \in \Theta$ among a family of parametrized curves:
\vfill

$$\Hspace = \{f_{\thetav}: f_{\thetav}\,\text{from functional family parametrized by}\,\thetav\}$$

\vfill

\begin{itemizeL}
    \item Optimizing numeric vectors is more convenient than functions
    \item For some model classes, some parameters encode the same function (non-injective mapping, non-identifiability). \\
    We don't care here, now. 
\end{itemizeL}

\end{frame}


\begin{frame}{Optimal loss values -- M-Estimators}

\begin{itemize}
\item Assume some RV $z \sim Q, z \in \Yspace$ as target
\item $z$ not the same as $y$, as we want to fiddle with its distribution
\item We now consider $\argmin_c \E_{z \sim Q}[L(z, c)]$\\
What is the constant that approximates $z$ with minimal loss?

\end{itemize}

\vfill

\textbf{3 cases for Q}:\\
\begin{itemize}
\item $Q = P_y$, distribution of labels y, marginal of $\Pxy$\\
optimal theoretical constant prediction

\item $Q = P_n$, the empirical product distribution for data $y^{(1)}, \ldots, y^{(n)}$\\
optimal empirical constant prediction

\item $Q = P_{y | \xv = \xtil}$, conditional label distribution at point $\xv = \xtil$\\
Bayes optimal pointwise prediction / theoretical risk minimizer 

\end{itemize}

%Solving $\argmin_c \E_{z \sim Q}[L(z, c)]$ for any $Q$ provides 
%multiple results.

\end{frame}


\begin{framei}[sep=M]{Optimal unconditional values}

\item Associating such a 
$$c = \argmin_{c \in \R} \E_{z \sim Q}[L(z, c)]$$ with 
a distribution is called a ``statistical functional''

\item Such a loss-minimizing version, and especially its empirical version below, is called an \textbf{M-estimator}\\
\item ``M'' can be read as ``max-likelihood type'', or ``minimizing'',\\
I prefer the latter

\item If we look at the empirical counterpart, with the empirical distribution, this is the so-called ``plug-in'' estimator

$$\argminlim_{c \in \R} \sumin L(\yi,c)$$

\end{framei}


\begin{framei}[sep=M]{Optimal Constant Model}

\item Goal: loss optimal, constant baseline predictor
\item ``constant'': featureless ML model, always predicts same value
\item ``baseline'': more complex model has to be better 
%Can use it as baseline in experiments, if we don't beat this
%with more complex model, that model is useless
\item Also useful as optimal intercept


$$f_{c}^{\ast} = \argmin_{c \in \R} \E_{xy} \left[L(y,c)\right] = \argmin_{c \in \R} \E_{y} \left[L(y,c)\right]$$

\item Estimation via ERM: $\hat{f}_c = \argminlim_{c \in \R} \sumin L(\yi,c)$


\imageC[0.34]{figure/l1_vs_l2.png}

\end{framei}


\begin{framei}[sep=M]{Risk Minimizer}

\item Assume, hypothesis space $\Hspace=\Hall$ is unrestricted;\\
contains any measurable $f: \Xspace \to \R^g$

\item We know $\Pxy$

\item $f$ with minimal risk across $\Hall$
is called\\
\textbf{risk minimizer}, \textbf{population minimizer} or \textbf{Bayes optimal model}

\begin{align*}
\fbayes_{\Hall} &= \argmin_{f \in \Hall} \risk(f) = \argmin_{f \in \Hall}\Exy\left[\Lxy\right]\\ 
&=  \argmin_{f \in \Hall}\int \Lxy \text{d}\Pxy
\end{align*}

\item The resulting risk is called \textbf{Bayes risk}:  $\riskbayes_{} = \risk(\fbayes_{\Hall})$

\item \textbf{Risk minimizer within} $\Hspace \subset \Hall$ is
$\fbayes_{\Hspace} = \argmin_{f \in \Hspace} \riskf$

\end{framei}



% \begin{frame}[t]{optimal point-wise predictions}  

% To derive the risk minimizer we usually make use of the following trick: 

% \begin{itemize}
% 	\item We can choose $\fx$ as we want (unrestricted hypothesis space, no assumed functional form)
% 	\item Consequently, for a fixed value $\xv \in \Xspace$ we can select \textbf{any} value $c$ we want to predict 
%         % (we are not restricted by any functional form, e.g., a linear function)
% 	% \item Instead of looking for the optimal $f$ in function space (which is impossible), we compute the \textbf{point-wise optimizer} for every $\xv \in \Xspace$.
% 	\item So we construct the \textbf{point-wise optimizer} for every $\xv \in \Xspace$.
% \end{itemize}

% \vfill
% 
% \imageC[1]{figure/optimal_pointwise.png}
% 


% \end{frame}

\begin{frame}[t]{optimal point-wise predictions}  

\begin{itemize}

\item To derive the RM, by law of total expectation 
$$    \riskf = \E_{xy} \left[\Lxy\right] 
    = \E_x \left[\E_{y|x}\left[\Lxy~|~\xv\right]\right]$$

	\item We can choose $\fx$ as we want from $\Hall$ %(unrestricted hypothesis space, no assumed functional form)
	\item Hence, for fixed feature vector $\xtil$ we can select \textbf{any} value $c$ to predict. So we construct the \textbf{point-wise optimizer} 
        % (we are not restricted by any functional form, e.g., a linear function)
	% \item Instead of looking for the optimal $f$ in function space (which is impossible), we compute the \textbf{point-wise optimizer} for every $\xv \in \Xspace$.
 $$\fbayes(\xtil) = \argmin_c \E_{y|x}\left[L(y, c)~|~ \xv = \xtil \right] $$
%        \item Under L2 loss, best point-wise prediction is simply cond. mean

%$f^{\ast}(\xtil) = \E_{y|x} \left[y ~|~ \xv = \xtil \right]$.
\end{itemize}

\imageC[0.8]{figure/optimal_pointwise.png}

\end{frame}


\begin{framei}[sep=M]{Theoretical and Empirical Risk}  
 
\item Bayes risk minimizer is mainly a theoretical tool
\item In practice, need to restrict $\Hspace$ for efficient search %so we efficiently search over it. 
\item We don't normally know $\Pxy$. Instead, use ERM. 
$$
{\hat f}_{\Hspace} = \argmin_{f \in \Hspace} \riskef = \argmin_{f \in \Hspace} \sumin \Lxyi
$$
\item Due to \textbf{law of large numbers}, empirical risk for fixed model converges to true risk, so consistent estimator
$$
\riskeb(f) = \frac{1}{n} \sumin \Lxyi \overset{n \to \infty}{\longrightarrow} \riskf 
$$
\item Still, that does not imply that the selected ERM minimizer converges to $f^{\ast}$, due to overfitting or lack of uniform convergence 
\item Would need more assumptions / math. machinery for this, \\
will not pursue this here 

\end{framei}



\begin{framei}[sep=L]{Estimation and Approximation Error} 

    \item Goal: Train model $\hat f_{\Hspace}$ with risk $\risk(\hat f_{\Hspace})$ close to Bayes risk $\riskbayes$ \\
    \item Minimize \textbf{Bayes regret} or \textbf{excess risk}
$$
	\risk(\hat f_{\Hspace}) - \riskbayes
$$ 

\item Decompose: 

\begin{align*}
	\risk(\hat f_{\Hspace}) - \riskbayes &= \underbrace{\left[\risk(\hat f_{\Hspace}) - \inf_{f \in \Hspace} \risk(f)\right]}_{\text{estimation error}} + \underbrace{\left[\inf_{f \in \Hspace} \risk(f) - \riskbayes\right]}_{\text{approximation error}} \\
    &= \left[\risk(\hat{f}_{\Hspace}) - \risk(\fbayes_{\Hspace})\right] + \left[\risk(\fbayes_{\Hspace}) - \risk(\fbayes_{\Hall}) \right] 
\end{align*}

\end{framei}

\begin{frame}{Estimation and Approximation Error} 

% https://docs.google.com/presentation/d/1vXuX8P6p7TFlXnpVcInQM6PR3qHk9LqC2Z75_I4u4Lo/edit#slide=id.p


\imageC[0.5]{figure_man/risk_minimization_diagram.png}




$$
\risk(\hat f_{\Hspace}) \fhspace - \riskbayes = \underbrace{\left[\risk(\hat f_{\Hspace}) - \inf_{f \in \Hspace} \risk(f)\right]}_{\text{estimation error}} + \underbrace{\left[\inf_{f \in \Hspace} \risk(f) - \riskbayes\right]}_{\text{approximation error}}  
$$

\vfill

\begin{itemize}
\item \textbf{Estimation error}:
We fit $\hat f_{\Hspace}$ via ERM on finite data, \\
so we don't find best $f \in \Hspace$
\item \textbf{Approximation error}: 
$\Hspace$ will often not contain Bayes optimal $\fbayes$ 
\end{itemize}

\end{frame}


\begin{frame}{(Universally) Consistent Learners \furtherreading{STNE1977CONS}}
% https://mlweb.loria.fr/book/en/consistency.html

\textbf{Consistency} is an asymptotic property of a learning algorithm, which ensures the algorithm returns \textbf{the correct model} when given \textbf{unlimited data}.

\vfill 

Let $\ind:\mathbb{D} \to \Hspace$ be a learning algorithm that takes a training set $\Dtrain \sim \Pxy$ of size $n_\text{train}$ and estimates a model $\hat f: \Xspace \to \R^g$. 

\vfill 

The learning method $\ind$ is said to be \textbf{consistent} w.r.t. a certain distribution $\Pxy$ if the risk of the estimated model $\hat f$ converges in probability ( \enquote{$\overset{p}{\longrightarrow}$}) to the Bayes risk $\riskbayes$ when $n_\text{train}$ goes to $\infty$: 

$$
	\risk(\ind(\Dtrain)) \overset{p}{\longrightarrow} \riskbayes \quad \text{for } n_\text{train} \to \infty
$$

\end{frame}

\begin{frame}{(Universally) Consistent Learners \furtherreading{STNE1977CONS}}

Consistency is defined w.r.t. a particular distribution $\Pxy$.
But since we usually don't know $\Pxy$, consistency
does not offer much help to choose an algorithm for a specific task. 

\vfill 

More interesting is the stronger concept of \textbf{universal consistency}: An algorithm is universally consistent if it is consistent for \textbf{any} distribution. 

\vfill 

In Stone's famous consistency theorem (1977), the universal consistency of a weighted average estimator such as KNN was proven. Many other ML models have since then been proven to be universally consistent (SVMs, ANNs, etc.).

%\vfill

%\textbf{Note}: universal consistency is a desirable property - however, it does not tell us anything about convergence rates ...

\end{frame}



% \begin{frame}[t]{Risk Minimizer And Optimal Constant}

% Later, we will derive risk minimizers for various losses. 

% \begin{table}[] 
% \footnotesize
% \renewcommand{\arraystretch}{1.5} %<- modify value to suit your needs
%   \begin{tabular}{c|lll}
%   Name & Risk Minimizer & Optimal Constant\\ \hline
%   L2 & $\fxbayes = \E_{y|x} \left[y ~|~ \xv \right]$ & $\fxh = \frac{1}{n} \sumin \yi$ \\
%   L1 & $\fxbayes = \text{med}_{y|x} \left[y ~|~ \xv \right]$ & $\fxh = \text{med}(\yi)$\\
%   0-1 & $\hxbayes = \argmax_{l \in \Yspace} \P(y = l~|~ \xv)$  & $\hxh = \text{mode} \left\{\yi\right\}$ \\
%   Brier & $\pixbayes = \P(y = 1~|~\xv)$ & $\pixh = \frac{1}{n} \sumin \yi$\\
%   Bernoulli (on probs) & $\pixbayes = \P(y = 1~|~\xv)$ & $\pixh = \frac{1}{n} \sumin \yi$\\
%   Bernoulli (on scores) & $\fxbayes = \log(\frac{\P(y = 1 ~|~\xv)}{1 - \P(y = 1 ~|~\xv)})$ & $\fxh = \log \frac{n_{+1}}{n_{-1}}$  
%   \end{tabular}
% \end{table}

% \only<1>{
% We see: For regression, the RMs model the conditional expectation and median of the underlying distribution. This makes intuitive sense, depending on your concept of how to best estimate central location / how robust this location should be.
% }
% \only<2>{
%     For the 0-1 loss, the risk minimizer constructs the \textbf{optimal Bayes decision rule}: We predict the class with maximal posterior probability.
% }
% \only<3>{
%     For Brier and Bernoulli, we predict the posterior probabilities (of the true DGP!). Losses that have this desirable property are called \textbf{proper scoring (rules)}.
% }

% \end{frame}



\endlecture

\end{document} 
