\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Advanced Risk Minimization
  }{% Lecture title  
    L1 Risk Minimizer (Deep-Dive)
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/loss_absolute_1.png
  }{
  \item Derive the risk minimizer of the L1-loss
  \item Derive the optimal constant model for the L1-loss
}

\begin{vbframe}{L1-Loss: Risk Minimizer}

\begin{footnotesize}
\textbf{Proof:} Let $p(y)$ be the density function of $y$. Then: 
  \begin{eqnarray*}
  && \argmin_c \E\left[|y - c|\right] = \argmin_c \int_{-\infty}^\infty |y - c| ~ p(y) \text{d}y \\
  &=& \argmin_c \int_{-\infty}^c -(y - c)~p(y)~\text{d}y + \int_c^\infty (y - c)~p(y)~\text{d}y 
  \end{eqnarray*}

We now compute the derivative of the above term and set it to $0$

\begin{eqnarray*}
0 &=& \frac{\partial}{\partial c} \left(\int_{-\infty}^c -(y - c)~p(y)~\text{d}y + \int_c^\infty (y - c)~p(y)~\text{d}y\right) \\ &\overset{^\ast\text{Leibniz}}{=}& \int_{-\infty}^c  ~p(y)~\text{d}y - \int_c^\infty ~p(y)~\text{d}y =   \P_y (y \le c) - \left(1 - \P_y (y \le c)\right) \\
&=& 2 \cdot \P_y (y \le c) - 1 \\
\Leftrightarrow 0.5 &=& \P_y (y \le c),
\end{eqnarray*}

which yields $c = \text{med}_y(y)$. 

\framebreak 

$^\ast$ \textbf{Note} that since we are computing the derivative w.r.t. the integration boundaries, we need to use Leibniz integration rule 

\begin{eqnarray*}
  \frac{\partial}{\partial c} \left(\int_a^c g(c, y) ~\text{d}y\right) &=& g(c, c) + \int_a^c \frac{\partial}{\partial c} g(c, y) ~\text{d}y \\
  \frac{\partial}{\partial c} \left(\int_c^a g(c, y) ~\text{d}y\right) &=& - g(c, c) + \int_c^a \frac{\partial}{\partial c} g(c, y) ~\text{d}y    
\end{eqnarray*}

We get 

\vspace*{-0.3cm}

\begin{eqnarray*}
&&\frac{\partial}{\partial c} \left(\int_{-\infty}^c -(y - c)~p(y)~\text{d}y + \int_c^\infty (y - c)~p(y)~\text{d}y \right) \\
&=& \frac{\partial}{\partial c} \left(\int_{-\infty}^c \underbrace{-(y - c)~p(y)}_{g_1(c, y)}~\text{d}y\right) + \frac{\partial}{\partial c} \left(\int_c^\infty \underbrace{(y - c)~p(y)}_{g_2(c, y)}~\text{d}y \right) \\
&=& \underbrace{g_1(c, c)}_{=0} + \int_{-\infty}^c \frac{\partial}{\partial c}  \left(-(y - c)\right)~p(y)~\text{d}y - \underbrace{g_2(c, c)}_{= 0} + \int_c^\infty \frac{\partial}{\partial c}  (y - c)~p(y)~\text{d}y \\
&=& \int_{-\infty}^c  ~p(y)~\text{d}y + \int_c^\infty -~p(y)~\text{d}y. 
\end{eqnarray*}

\end{footnotesize}

\end{vbframe}


\begin{vbframe}{L1-Loss: Optimal constant model}

\textbf{Proof}: 

\begin{itemize}
  \item Firstly note that for $n = 1$ the median $\thetah = \text{med}(\yi) = y^{(1)}$ obviously minimizes the emp. risk $\riske$ using the $L1$ loss. 


% Take proof from this page? 
% https://math.stackexchange.com/questions/113270/the-median-minimizes-the-sum-of-absolute-deviations-the-l-1-norm/1024462#1024462

  \item Hence let $n > 1$ in the following For $a,b \in \mathbb{R}$, define 
  $$
    S_{a,b}:\mathbb{R} \rightarrow \mathbb{R}^+_0, \theta \mapsto |a- \theta| + |b-\theta|
  $$
  \vspace*{-0.3cm}
  Any $\thetah \in [a,b]$ minimizes $S_{a,b}(\theta)$, because it holds that
  \vspace*{-0.0cm}
  \begin{eqnarray*}
  S_{a,b}(\theta) &=& \begin{cases}|a-b| ,& \text{ for } \theta \in [a,b]\\ |a-b| + 2\cdot\min\{|a-\theta|,|b-\theta|\}
  ,& \text{ otherwise. }\end{cases}
  \end{eqnarray*}

\vspace*{-0.15cm}

\begin{center}
\includegraphics[width = 0.4\textwidth ]{figure_man/S_function_plot.pdf} \\
\end{center}



  \framebreak

  W.l.o.g. assume now that all $\yi$ are sorted in increasing order.

  Let us define $i_{\max} = n / 2$ for $n$ even and $i_{\max} = (n - 1) / 2$ for $n$ odd and consider the intervals 
  $$
    \mathcal{I}_i := [y^{(i)},y^{(n+1-i)}], i \in \{1, ..., i_{\max}\}. 
  $$

  By construction $\mathcal{I}_{j+1} \subseteq \mathcal{I}_j$ for $j \in \{1,\dots,i_{\max}-1\}$ and $\mathcal{I}_{i_{\max}} \subseteq \mathcal{I}_i$. With this, $\riske$ can be expressed as
  \begin{footnotesize}
  \begin{eqnarray*}
  \riske(\theta) &=& \sumin L(\yi,\theta) = \sumin|\yi - \theta| \\ 
  &=& \underbrace{|\yi[1] - \theta| + |\yi[n] - \theta|}_{= S_{\yi[1], \yi[n]}(\theta)} + \underbrace{|\yi[2] - \theta| + |\yi[n - 1] - \theta|}_{= S_{\yi[2], \yi[n - 1]}(\theta)} + ...  \\
  &=& \begin{cases} \sum\limits_{i = 1}^{i_{\max}} S_{\yi, \yi[n + 1 - i]}(\theta) & \text{ for } n \text{ is even} \\
  \sum\limits_{i = 1}^{i_{\max}} \left(S_{\yi, \yi[n + 1 - i]}(\theta)\right) + |\yi[(n + 1)/2] - \theta| & \text{ for } n \text{ is odd}. \end{cases}
  \end{eqnarray*}
  \end{footnotesize}

  % \begin{eqnarray*}
  % \begin{cases}
  % \sum^{\overbrace{n/2}^{=:i_{\max}}}_{i=1} \overbrace{S_{y^{(i)},y^{(n+1-i)}}(c)}^{=:S_i(c)},&  \text{ for } n \text{ is even}\\
  % (\sum^{\overbrace{(n-1)/2}^{=:i_{\max}}}_{i=1} \underbrace{S_{y^{(i)},y^{(n+1-i)}}(c)}_{=:S_i(c)}) + \underbrace{|y^{(n+1)/2}-c|}_{=:S_0(c)},& \text{ for } n \text{ is odd}. 
  % \end{cases}
  % \end{eqnarray*}
  % \end{footnotesize}
  % Now we define for $i \in \{1,\dots,i_{\max}\} \; \mathcal{I}_i := [y^{(i)},y^{(n+1-i)}].$ \\
  % From construction it follows that for  $j \in \{1,\dots,i_{\max}-1\}$
  % $$\mathcal{I}_{j+1} \subseteq \mathcal{I}_j \Rightarrow \forall i \in \{1,\dots,i_{\max}\}:  \mathcal{I}_{i_{\max}} \subseteq \mathcal{I}_i.$$
  \framebreak 


  From this follows that
  \vspace{0.35cm}
  \begin{itemize}
  \setlength{\itemsep}{1.4em}
  \item for \enquote{$n$ is even}: $\thetah \in  \mathcal{I}_{i_{\max}} = [y^{(n/2)},y^{(n/2+1)}]$ minimizes $S_i$ for all $i \in \{1,\dots, i_{\max}\} \; \Rightarrow$  it minimizes $\riske$,
  \item for \enquote{$n$ is odd}: $\thetah = y^{(n+1)/2} \in \mathcal{I}_{i_{\max}}$ minimizes $S_i$ for all $i \in \{1,\dots, i_{\max}\}$ and it's minimal for
  $|\yi[(n + 1)/2] - \theta|$ \\ $\Rightarrow$
  it minimizes $\riske$.
  \end{itemize}
  \vspace{0.35cm}  
  Since the median fulfills these conditions, we can conclude that it minimizes 
  the $L1$ loss.
\end{itemize}

% \framebreak


% \begin{center}
% \includegraphics[width = 11cm ]{figure_man/L1-loss.png} \\
% \end{center}

% \framebreak 

% We see that the $L1$-Loss is more robust w.r.t. outliers than the $L2$-Loss. 
% \vspace{0.3cm}


% \begin{center}
% \includegraphics[width = 9cm ]{figure_man/L1andL2-loss.png} \\
% \end{center}

\framebreak 

\end{vbframe}


% \begin{vbframe}{Robustness}

% We see that the $L1$-Loss is more robust w.r.t. outliers than the $L2$-Loss. 
% \vspace{0.3cm}

% \begin{center}  
% \includegraphics[width = 9cm ]{figure_man/L1andL2-loss.png} \\
% \end{center}

% \end{vbframe}

\endlecture

\end{document}
