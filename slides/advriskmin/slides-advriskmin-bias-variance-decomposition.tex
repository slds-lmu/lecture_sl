\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-eval}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Advanced Risk Minimization
  }{% Lecture title  
    Advanced Risk Minimization:\\
    Bias-Variance Decomposition
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/bias_variance_decomposition-linear_model_bias.png
  }{
  \item Understand how to decompose the generalization error of a learner into 
  \begin{itemize}
    \item \footnotesize bias of the learner
    \item \footnotesize variance of the learner
    \item \footnotesize inherent noise in the data
  \end{itemize} 
}

\begin{vbframe} {Bias-Variance decomposition}

Let us take a closer look at the generalization error of a learning algorithm $\ind_{L}$.
This is the expected error of an induced model $\fh_{\D_n}$, on training sets of size $n$, when applied to a fresh, random test observation.
  $$GE_n\left(\ind_{L}\right) = \E_{\D_n \sim \Pxy^n, (\xv, y) \sim \Pxy}\left( L\left(y, \fh_{\D_n}(\xv)\right) \right) = \E_{\D_n, xy}\left( L\left(y, \fh_{\D_n}(\xv)\right) \right)  $$
We therefore need to take the expectation over all training sets of size $n$, as well as the independent test observation.\\

For the squared loss, there is a nice additive decomposition of $GE_n\left(\ind_{L}\right)$ into three components.$^{\ast}$

Hence we assume that the data is generated by 
$$
y = \ftrue(\xv) + \epsilon\,,
$$
with zero-mean homoskedastic error $\epsilon \sim (0, \sigma^2)$ independent of $\xv$.
\vspace{0.1cm}

{\scriptsize ${\ast}$ Similar decomp also exist for other losses expressible as Bregman divergences (e.g. cross-entropy). The $0/1$ loss e.g. has no such decomposition (\cite{BROWN2024BIAS})}

\framebreak 


\begin{footnotesize}
$GE_n\left(\ind_{L}\right) =$  
$$
 \underbrace{\sigma^2}_{\text{Variance of the data}} + \E_{xy}\underbrace{\left[\var_{\D_n}\left(\fh_{\D_n}(\xv) ~|~\xv, y\right)\right]}_{\text{Variance of learner at } (\xv, y)} + \E_{xy}\underbrace{\left[\left(\left(\ftrue(\xv)-\E_{\D_n}\left(\fh_{\D_n}(\xv)\right)\right)^2~|~\xv, y\right)\right]}_{\text{Squared bias of learner at } (\xv, y)}  
$$
\end{footnotesize}
\begin{enumerate}
  \item The first term expresses the variance of the data. 
    This is pure \textbf{noise} in the data.
    Also called Bayes, intrinsic or irreducible error.
    No matter what we do, we will never get below this error.
  \item The second term expresses, on average, how much $\fh_{\D_n}(\xv)$ fluctuates around test points if we vary the training data. Expresses also the learner's tendency to learn random things irrespective of the real signal (overfitting).
  \item The third term says how much we are "off" on average at test locations (underfitting).
    Models with high capacity typically have low \textbf{bias} and \textit{vice versa}.
\end{enumerate}


% So for the squared error loss, the generalized prediction error can be decomposed into

% \begin{itemize}
% \item \textbf{Noise}: Intrinsic error, independent from the learner, cannot be avoided
% \item \textbf{Variance}: Learner's tendency to learn random things irrespective of the real signal (overfitting)
% \item \textbf{Bias}: Learner's tendency to \textbf{consistently} misclassify certain instances (underfitting)
% \end{itemize}




% \framebreak

% For the $k$-NN learning algorithm, which outputs models of the shape $$\fh_{\D}(x) = \frac{1}{k}\sum_{i: \xi \in N_k(x)} \yi,$$
% the generalization error becomes 

% \begin{eqnarray*}
% \GE(\mathcal{I}_{L, O}) &=& \sigma^2 + \var\left(\fh_{\D}(\xv)\right) + \text{Bias}\left(\fh_{\D}(\xv)\right)^2 \\
% &=&\sigma^2 + \frac{\sigma^2}{k} + \E_x \left(f(\xv) - \frac{1}{k}\sum_{\xi \in N_k(\xv)}\fxi\right)^2 \\
% \end{eqnarray*}

% where we assumed for simplicity that training inputs $\xv^{(i)}$ are fixed and the randomness arises only from $y$.



\framebreak

\textbf{Illustration}: Let us consider the following example. We will generate a dataset using the following model : 
$$y = x + \frac{x^2}{2} + \epsilon  \ , \ \ \ \ \epsilon \sim 
N (0, 1)$$

The data is then split into a training set and a test set.

\begin{center}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-train_test.png}
\end{center}

\framebreak

To obtain estimates for the bias and variance, we will train several models by sampling with replacement from the training data. This is commonly known as \textbf{bootstrapping}. \\
\vspace{0.3cm}
First, we train several (low capacity) linear models (polynomial of degree $d=1$).

\begin{center}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-bootstrap_1.png}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-bootstrap_2.png}
\end{center}

\framebreak

By creating several models, we obtain the average model over different samples of the training dataset.

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure/bias_variance_decomposition-linear_model.png}
\end{center}

\framebreak

We can now estimate the (squared) bias, by computing the average squared difference between the average model and the true model, at the test point locations.

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure/bias_variance_decomposition-linear_model_bias.png}
\end{center}

\framebreak

We compute the average variance of the predictions of the models we trained at the test point locations.
\vspace{-0.35cm}
\begin{center}
  \includegraphics[width = 0.35\textwidth]{figure/bias_variance_decomposition-linear_model_variance.png}
\end{center}
\vspace{-0.8cm}

$$GE_n\left(\ind_{L}\right) \approx 1 + 1.628 + 0.135 = 2.763 $$


\begin{itemize}
  \item The biggest component of the generalization error is the bias.
  \item Computing the MSE in the usual way for each model, via L2 loss, and then averaging over models gives rise to nearly the same value, as expected
\end{itemize}

\framebreak

\begin{center}
  \includegraphics[width = 0.35\textwidth]{figure/bias_variance_decomposition-linear_model_variance.png}
\end{center}

\begin{footnotesize}

\begin{itemize}
  \item We can now check whether this alternative computation of the GE is correct
  \item So, we simply compute the MSE in the standard fashion for each model
  \item So for each model we compute the L2 loss at each data point, then average
  \item Then we average these MSEs over all models
  \item Result = 2.72, would be closer if we average over more models and test points 
\end{itemize}

\end{footnotesize}


\framebreak


We will repeat the same procedure, but use a high-degree polynomial ($d=7$) with more capacity.

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure/bias_variance_decomposition-complex_model.png}
\end{center}


\framebreak

\begin{center}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-complex_model_bias.png}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-complex_model_variance.png}
\end{center}

$$GE_n\left(\ind_{L}\right) \approx 1 + 0.139 + 1.963 = 3.102 $$

\begin{itemize}
  \item The generalization error is higher than before
  \item Even though the bias is lower, the variance of the learner is higher. 
\end{itemize}



\framebreak

What happens if we use a model with the same complexity as the true model (quadratic polynomial)? 

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure/bias_variance_decomposition-correct_model.png}
\end{center}

\framebreak

\begin{center}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-correct_model_bias.png}
  \includegraphics[width = 0.4\textwidth]{figure/bias_variance_decomposition-correct_model_variance.png}
\end{center}

$$GE_n\left(\ind_{L}\right) \approx 1 + 0.008 + 0.082 = 1.091 $$

\begin{itemize}
  \item The generalization error is the lowest at this complexity.
  \item The variance of the data acts as a lower bound.
\end{itemize}
 

\end{vbframe}


\begin{vbframe}{Capacity and overfitting}

  \begin{figure}
    \centering
    \includegraphics[width = 0.6\textwidth]{figure_man/lcurve_1.png}
    \tiny{\\ Credit: Ian Goodfellow}
  \end{figure}
  

\begin{itemize}
  \item 
    The performance of a learner depends on its ability to 
    \begin{enumerate}
      \item \textbf{fit} the training data well
      \item \textbf{generalize} to new data
    \end{enumerate}  
  \item Failure of the first point is called \textbf{underfitting}
  \item Failure of the second item is called \textbf{overfitting}
\end{itemize}  


\framebreak

\begin{figure}
  \centering
  \includegraphics[width = 0.6\textwidth]{figure_man/lcurve_1.png}
  \tiny{\\ Credit: Ian Goodfellow}
\end{figure}


\begin{itemize}
  \item The tendency of a model to underfit/overfit is a function of its capacity, determined by the type of hypotheses it can learn.
  \item Usually, low bias means high capacity, which in turn means a higher chance of overfitting
  \item Low-bias models usually have 
  also higher variance
  \item For such models, regularization (we discuss later) is essential
  \item Even for correctly specified models, the generalization error is lower-bounded by the irreducible noise $\sigma^2$
\end{itemize}
\end{vbframe}


\begin{vbframe}{Approximation an Estimation \citelink{BROWN2024BIAS} }
The Bias-Variance decomposition is often confused or equated with the related (but different) decompo of \textbf{excess risk} into \textbf{estimation} and \textbf{approximation} error.

\begin{eqnarray*}
    \underbrace{\risk(\hat f_{\Hspace}) - \risk(\fbayes_{\Hspace_{all}})}_{\text{excess risk}} &=& \underbrace{\risk(\hat f_{\Hspace}) - \risk(\fbayes_{\Hspace})}_{\text{estimation error}} + \underbrace{\risk(\fbayes_{\Hspace}) -  \risk(\fbayes_{\Hspace_{all}})}_{\text{approx. error}} 
\end{eqnarray*}

\begin{figure}
    \centering
    \includegraphics[width = 0.75\textwidth]{figure_man/biasvar-vs-estapprox-tradeoff.png}
    \tiny{\\ Credit: \cite{BROWN2024BIAS}}
  \end{figure}

\end{vbframe}

\framebreak

\begin{vbframe}{Approximation/Estimation Error}
    
\begin{itemize}
    \item The approx. error is a structural property of $\Hspace$.
    \item The estimation error is random due to dependence on data in $\fh$
    \item Estimation error arises because we choose $f \in \Hspace$ with limited training data using $\riske$ instead of $\risk$
\end{itemize}

$\fh_{\Hspace}$ assumes we found a global minimizer of $\riske$, which is often impossible. 
\vspace{0.2cm}

In practice, optimizing $\riske$ gives us our 'best guess' $\tilde{f}_{\Hspace} \in \Hspace$ of $\fh_{\Hspace}$. We can now decompose its excess risk as

\begin{eqnarray*}
    \underbrace{\risk(\tilde{f}_{\Hspace}) - \risk(\fbayes_{\Hspace_{all}})}_{\text{excess risk}} &=& \underbrace{\risk(\tilde{f}_{\Hspace}) - \risk(\fh_{\Hspace})}_{\text{optim. error}} + \underbrace{\risk(\hat{f}_{\Hspace}) - \risk(\fbayes_{\Hspace})}_{\text{estimation error}} + \underbrace{\risk(\fbayes_{\Hspace}) -  \risk(\fbayes_{\Hspace_{all}})}_{\text{approx. error}} 
\end{eqnarray*}

Note that the optimization error can be negative, even though $\riske(\tilde{f}_{\Hspace}) \geq \riske(\fh_{\Hspace})$ always holds.


\end{vbframe}


% Note that for special case of a linear model with squared loss (OLS), the bias component becomes equal to the approximation bias and the variance to the estimation bias.

\endlecture
\end{document}


