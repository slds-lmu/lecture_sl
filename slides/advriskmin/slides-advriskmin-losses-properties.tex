\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\usepackage{booktabs}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{
Advanced Risk Minimization
}{
Properties of Loss Functions
}{
figure_man/vgg_example.png
}{
\item Statistical properties 
\item Robustness 
\item Optimization properties
\item Some fundamental terminology
}

\begin{framei}[sep=L]{The role of Loss Functions}

\item Should be designed to measure errors appropriately

\item \textbf{Statistical} properties: choice of loss implies statistical assumptions about the distribution of $y ~|~ \xv = \xtil$ \\
(see \emph{maximum likelihood vs. empirical risk minimization})
\item \textbf{Robustness} properties: \\
some losses more robust towards outliers than others
\item \textbf{Optimization} properties: computational complexity of
$$
\argmint \risket
$$
is influenced by choice of the loss 

\end{framei}

% ------------------------------------------------------------------------------


\begin{framei}[sep=M]{Losses with one argument}

% https://davidrosenberg.github.io/mlcourse/Archive/2017/Lectures/3b.loss-functions.pdf

\item Regr. losses often only depend on  \textbf{residuals} 
$\rx := y - \fx$

\item Classif. losses usually in terms of 
\textbf{margin}: $\nux  := y \cdot \fx$

\vfill

\splitVCC[0.4]{
\imageC{figure/loss_properties_dist_based}
\small \centering
Distance-based: $L1$
}{
\imageC{figure/overview_classif_all.png}
\small \centering
%Margin-based losses
}

\end{framei}


\begin{frame}{Some basic properties}

% https://davidrosenberg.github.io/mlcourse/Archive/2017/Lectures/3b.loss-functions.pdf
A loss is  
\begin{itemize}
\item \textbf{symmetric} if $\Lxy = L(\fx, y)$
\item \textbf{translation-invariant} if $L(y + a, \fx + a) = \Lxy$, 
$a \in \R$
\item \textbf{distance-based} if it can be written in terms of residual 
$\Lxy = \psi (r)$ for some $\psi: \R \to \R$, and 
$\psi(r) = 0 \Leftrightarrow r = 0$
\end{itemize}


\vfill

\splitVThree
{
\imageC{figure/loss_properties_dist_based}
\small \centering
Distance-based: $L1$
}%
{
\imageC{figure/loss_properties_transl_inv.png}
\small \centering
Transl.-invariant.: $L2$
}%
{
\imageC{figure/loss_properties_symmetric}
\small \centering
Symmetric: Brier score
}

\end{frame}

% ------------------------------------------------------------------------------

\begin{framev}[fs=small]{Robustness}

Outliers (in $y$) have large residuals $\rx = y - \fx$. Some losses are more
affected by large residuals than others. If loss goes up superlinearly (e.g. L2) it is not robust, linear (L1) or even sublinear losses are more robust.

\vfill

\splitVCC{
{\footnotesize
\begin{table}[]
\begin{tabular}{r|r|r|r}
\toprule
$y - \fx$ & $L1$ & $L2$ & Huber ($\eps = 5$) \\ \hline
1 & 1 & 1 & 0.5 \\
5 & 5 & 25 & 12.5 \\
10 & 10 & 100 & 37.5 \\
50 & 50 & 2500  & 237.5
\end{tabular}
\end{table}}
}%
{
As a consequence, a model is less influenced by outliers than by ``inliers'' if 
the loss is \textbf{robust}.\\
Outliers e.g.\ strongly influence $L2$.
}%

\vfill

\splitVTT[0.4]{
\imageC[0.9]{figure/loss_l2_l1_huber.png}
}{
\imageC[0.9]{figure/robustness}
}

\end{framev}

% ------------------------------------------------------------------------------

\begin{frame}{Optimization Properties: Smoothness}

% \textcolor{blue}{LW: points 1-3 together w/ plot on subsequent slide; scnd slide w/ concrete examples (e.g., gd failing w/ lasso}

\begin{itemize}
\item Measured by number of continuous derivatives. $f \in \mathcal{C}^k$ for $k$-times cont. differentiable. $f$ and smooth for $f \in \mathcal{C}^{\infty}$
\item Usually want to have at least gradients in optimization of $\risket$
\item If loss is not differentiable, might have to use derivative-free optimization (or worse, in case of 0-1)
\item Smoothness of $\risket$ not only depends on $L$, but also requires smoothness of $\fx$! 
\end{itemize}

\vfill

\splitVCC[0.65]{
\imageC[0.9]{figure/overview_classif_all.png}
}%
{
\raggedright {\footnotesize
Squared, exponential and squared hinge losses are continuously 
differentiable. Hinge loss is continuous but not differentiable. 
0-1 loss is not even continuous.}
}%

% \small
% \textbf{Example: Lasso regression}

% \begin{itemize}
%   \small
%   \item Problem: Lasso has a non-differentiable 
%   objective function $$\riskrt = \| \yv - \Xmat \thetav \|^2_2
%   + \lambda \| \thetav \|_1 ~ \in \mathcal{C}^0,$$
%   but many optimization methods are derivative-based, e.g.,
%   \begin{itemize}
%     \small
%     \item Gradient descent: requires existence of gradient $\nabla \risket$, 
%     \item Newton-Raphson: requires existence of Hessian $\nabla^2 \risket$.
%   \end{itemize}
%   \item We must therefore resort to alternative optimization 
%   techniques -- for instance, coordinate descent with subgradients.
% \end{itemize}

% \vfill

% \begin{minipage}[c]{0.3}
%   \imageC[]{figure/lasso_unpenalized}
% \end{minipage}%
% \begin{minipage}[c]{0.05}
%   \phantom{foo}
% \end{minipage}%
% \begin{minipage}[c]{0.3}
%   \imageC[0.9]{figure/lasso_penalty}
% \end{minipage}%
% \begin{minipage}[c]{0.05}
%   \phantom{foo}
% \end{minipage}%
% \begin{minipage}[c]{0.3}
%   \imageC[0.9]{figure/lasso_penalized}
% \end{minipage}%

% \tiny Example: $y = x_1 + 1.2 x_2 + \epsilon$. \textit{Left:} unpenalized 
% objective, \textit{middle:} $L1$ penalty, \textit{right:} penalized objective 
% (all as functions of $\thetav$). We see how the $L1$ penalty nudges the optimum 
% towards (0, 0) and compromises the original objective's smoothness.

\end{frame}

% ------------------------------------------------------------------------------

\begin{framei}[sep=L]{Optimization Properties: Convexity}

\item $\risket$ is convex if
$$
\riske(t \cdot \thetav + (1 - t) \cdot \tilde \thetav) \le t \cdot
\riske(\thetav) + (1 - t) \cdot \riske(\tilde \thetav )
$$
$\forall$ $t \in [0, 1], ~\thetav, \tilde \thetav \in \Theta$\\
(strictly convex if above holds with strict inequality)

\item In optimization, convex problems have several convenient properties, e.g. all local minima are global 
\item Strictly convex function has at most \textbf{one} global min (uniqueness)

\item For $\riske \in \mathcal{C}^{2}$, $\riske$ is convex iff Hessian $\nabla^2 \risket$ is psd

\item Above holds for arbitrary functions, not only risks

\end{framei}

% ------------------------------------------------------------------------------

\begin{frame}{Optimization properties: Convexity}

\begin{itemizeL}
\item Convexity of $\risket$ depends both on convexity of $L(\cdot)$ (given in most cases) and $\fxt$ (often problematic)
\item If we model our data using an exponential family distribution, we always get convex losses \furtherreading{WEDDERBURN1976MAX}
\item For $\fxt$ linear in $\thetav$, linear/logistic/softmax/poisson/$\ldots$ regression are convex problems (all GLMs)! 
\end{itemizeL} 

\vfill

\splitVCC{
\raggedright {\footnotesize
The problem on the bottom right is convex, the others are not (note that 
very high-dimensional surfaces are coerced into 3D here).  
\phantom{foo}}}
{
\imageC[0.75][LI2018VISUALIZING]{
figure_man/convex-vs-nonconfex-landscape}}%

\end{frame}

% ------------------------------------------------------------------------------

% \begin{framei}[sep=M]{Analytical Properties: Convergence}
% 
% \small
% The choice of the loss function may also impact the convergence behavior of the 
% optimization problem. 
% 
% \vspace{0.2cm}
% 
% \begin{minipage}[b]{0.7}
%   \begin{itemize} 
%     \small
%     \item Example: optimizers, e.g., gradient descent, in logistic regression 
%     will not converge for 
%     linearly separable data (\textbf{complete separation}). 
%     \item This is a direct consequence of the convergence behavior of Bernoulli 
%     loss, which reaches 0 only in the infinite limit of the margin.
%   \end{itemize}  
% \end{minipage}%
% \begin{minipage}[b]{0.05}
%   \phantom{foo}
% \end{minipage}%
% \begin{minipage}[b]{0.25}
%   \imageC[]{figure/bernoulli}
% \end{minipage}%
% 
% \begin{itemize} 
%   \item In the case of complete separation, we have
%   \footnotesize
%   \begin{flalign*}
%     \risket &= \sumin \log ( 1 + \exp ( - \yi \thetav^T \xi )
%     ) \\ &=
%     \sumin \log ( 1 + \exp ( - | \thetav^T \xi| )
%     ),
%   \end{flalign*}
%   \small
%   as every observation is correctly classified (i.e., $\thetav^T \xi < 0$ \\for
%   $\yi = -1$ and $\thetav^T \xi > 0$ for $\yi = 1$).
% \end{itemize} 
% 
% \framebreak
% 
% \begin{itemize} 
%   \small
%   \item $\risket$ thus monotonically decreases in $\thetav$: if a parameter 
%   vector $\thetav^\prime$ is able to classify the samples perfectly, then 
%   $2\thetav^\prime$ also classifies the samples perfectly, and at lower risk.
%   \item Geometrically, this translates to an ever steeper slope of the 
%   logistic/softmax function, leading to increasingly sharp discrimination and 
%   infinitely running optimization.
%   
%   \vspace{0.3cm}
%   \begin{minipage}[b]{0.4}
%     \imageC[0.8]{figure/softmax_1}
%   \end{minipage}%
%   \begin{minipage}[b]{0.4}
%     \imageC[0.8]{figure/softmax_2}
%   \end{minipage}%
% \end{itemize}  
% 
% \vfill
% 
% 
% 
% \end{framei}

% ------------------------------------------------------------------------------

\endlecture

\end{document}