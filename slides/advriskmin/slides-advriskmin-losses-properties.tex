\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@


\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\usepackage{booktabs}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Advanced Risk Minimization
  }{% Lecture title  
    Properties of Loss Functions
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/vgg_example.png
  }{
  % \item Understand why you should care about properties of loss functions
  \item Statistical properties 
  \item Robustness 
  \item Numerical properties
  \item Some fundamental terminology
}

\begin{vbframe}{The role of Loss Functions}

Why should we care about the choice of the loss function $\Lxy$?

\begin{itemize}
% \item For regression, the loss usually only depends on residual $\Lxy = L\left(y - \fx\right) = L(\eps)$, this is a \emph{translation invariant} loss
\item \textbf{Statistical} properties: choice of loss implies statistical assumptions about the distribution of $y ~|~ \xv = \xv$ (see \emph{maximum likelihood estimation vs.\
empirical risk minimization}). 
\item \textbf{Robustness} properties: some loss functions are more robust towards outliers than others. 
\item \textbf{Numerical} properties: the computational complexity of
$$
\argmint \risket
$$
is influenced by the choice of the loss function. 
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------


\begin{vbframe}{Some basic terminology}

% https://davidrosenberg.github.io/mlcourse/Archive/2017/Lectures/3b.loss-functions.pdf

Classification losses are usually expressed in terms of the 
  \textbf{margin}: $\nu := y \cdot \fx.$

\vfill

\begin{minipage}[b]{1\textwidth}
  \includegraphics[width=0.75\textwidth]{figure/plot_loss_overview_classif_2}
  \tiny \centering
  %Margin-based losses
\end{minipage}%


\end{vbframe}


\begin{vbframe}{Some basic terminology}

% https://davidrosenberg.github.io/mlcourse/Archive/2017/Lectures/3b.loss-functions.pdf

\begin{itemize}
  \small
  \item Regression losses often only depend on the \textbf{residuals}
  $r := y - \fx.$
%  \item Classification losses are usually expressed in terms of the 
%  \textbf{margin} $\nu := y \cdot \fx.$
  \item Losses are called \textbf{symmetric} if $\Lxy = L\left(\fx, y\right)$. 
  \item A loss is \textbf{translation-invariant} if $L(y + a, \fx + a) = \Lxy$, 
  $a \in \R$.
  \item A loss is called \textbf{distance-based} if
  \begin{itemize}
    \small
    \item it can be written in terms of the residual, i.e., 
    $\Lxy = \psi (r)$ \\for some $\psi: \R \to \R, ~ \text{and}$
    \item $\psi(r) = 0 \Leftrightarrow r = 0$.
  \end{itemize}
\end{itemize}

\vfill

%\begin{minipage}[b]{0.25\textwidth}
%  \includegraphics[width=\textwidth]{figure/plot_loss_overview_classif_2}
%  \tiny \centering
%  Margin-based losses
%\end{minipage}%
\begin{minipage}[b]{0.32\textwidth}
  \includegraphics[width=\textwidth]{figure/loss_dist_based}
  \tiny \centering
  Distance-based: $L1$
\end{minipage}%
\begin{minipage}[b]{0.265\textwidth}
  \includegraphics[width=\textwidth]{figure/loss_transl_inv.png}
  \tiny \centering
  Translation-invariant: $L2$
\end{minipage}%
\begin{minipage}[b]{0.40\textwidth}
  \includegraphics[width=\textwidth]{figure/loss_symmetric}
  \tiny \centering
  Symmetric: Brier score
\end{minipage}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Robustness}

\small

Outliers (in $y$) have large residuals $r = y - \fx$. Some losses are more
affected by large residuals than others. If loss goes up superlinearly (e.g.\ L2) it is not robust, linear (L1) or even sublinear losses are more robust.

\vspace{0.5cm}

\begin{minipage}[c]{0.5\textwidth}
  \footnotesize
  \begin{table}[]
  \begin{tabular}{r|r|r|r}
  \toprule
  $y - \fxh$ & $L1$ & $L2$ & Huber ($\eps = 5$) \\ \hline
  1 & 1 & 1 & 0.5 \\
  5 & 5 & 25 & 12.5 \\
  10 & 10 & 100 & 37.5 \\
  50 & 50 & 2500  & 237.5
\end{tabular}
\end{table}
\end{minipage}%
\begin{minipage}[c]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[c]{0.45\textwidth}
  \small
  As a consequence, a model is less influenced by outliers than by ``inliers" if 
  the loss is \textbf{robust}.\\
  Outliers e.g.\ strongly influence $L2$.
\end{minipage}%

\vfill


\begin{minipage}[c]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[c]{0.45\textwidth}
  \includegraphics[width=\textwidth]{figure/loss_l1_l2}
  \footnotesize \centering
\end{minipage}%
\begin{minipage}[c]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[c]{0.45\textwidth}
\includegraphics[width=\textwidth]{figure/robustness}
  %\small \raggedright
  %is an example for a loss function that is not very robust towards
  %outliers. It penalizes large residuals more than $L1$ or Huber loss, which are
  %considered robust.
\end{minipage}%

% \framebreak 
% 
% The L2 loss is an example for a loss function that is not very robust towards outliers. It penalizes large residuals more than the L1 or the Huber loss. The L1 and the Huber loss are thus regarded robust. 
% 
% \begin{center}
% \includegraphics[width=0.8\textwidth]{figure/robustness.png}
% \end{center}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Numerical Properties: Smoothness}

% \textcolor{blue}{LW: points 1-3 together w/ plot on subsequent slide; scnd slide w/ concrete examples (e.g., gd failing w/ lasso}

\begin{itemize}
  \small
  \item \textbf{Smoothness} of a function is a property measured by 
  the number of continuous derivatives. 
 % \item A function is said to be $\mathcal{C}^k$ if it is $k$ times continuously differentiable. A function is $\mathcal{C}^\infty$ if it is continuously differentiable for all orders $k$. 
  \item Derivative-based optimization requires smoothness of the 
  risk $\risket$ 
  \begin{itemize}
    \small
    \item If loss is unsmooth, we might have to use derivative-free optimization (or worse, in case of 0-1)
    \item Smoothness of $\risket$ not only depends on $L$, but also requires smoothness of $\fx$! 
  \end{itemize}
\end{itemize}

\vfill

\begin{minipage}[c]{0.4\textwidth}
  \includegraphics[width=0.9\textwidth]{figure/plot_loss_overview_classif}
\end{minipage}%
\begin{minipage}[c]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[c]{0.55\textwidth}
  \footnotesize \raggedright
  Squared loss, exponential loss and squared hinge loss are continuously 
  differentiable. Hinge loss is continuous but not differentiable. 
  0-1 loss is not even continuous.
\end{minipage}%

\framebreak

% \small
% \textbf{Example: Lasso regression}

% \begin{itemize}
%   \small
%   \item Problem: Lasso has a non-differentiable 
%   objective function $$\riskrt = \| \yv - \Xmat \thetab \|^2_2
%   + \lambda \| \thetab \|_1 ~ \in \mathcal{C}^0,$$
%   but many optimization methods are derivative-based, e.g.,
%   \begin{itemize}
%     \small
%     \item Gradient descent: requires existence of gradient $\nabla \risket$, 
%     \item Newton-Raphson: requires existence of Hessian $\nabla^2 \risket$.
%   \end{itemize}
%   \item We must therefore resort to alternative optimization 
%   techniques -- for instance, coordinate descent with subgradients.
% \end{itemize}

% \vfill

% \begin{minipage}[c]{0.3\textwidth}
%   \includegraphics[width=\textwidth]{figure/lasso_unpenalized}
% \end{minipage}%
% \begin{minipage}[c]{0.05\textwidth}
%   \phantom{foo}
% \end{minipage}%
% \begin{minipage}[c]{0.3\textwidth}
%   \includegraphics[width=0.9\textwidth]{figure/lasso_penalty}
% \end{minipage}%
% \begin{minipage}[c]{0.05\textwidth}
%   \phantom{foo}
% \end{minipage}%
% \begin{minipage}[c]{0.3\textwidth}
%   \includegraphics[width=0.9\textwidth]{figure/lasso_penalized}
% \end{minipage}%

% \tiny Example: $y = x_1 + 1.2 x_2 + \epsilon$. \textit{Left:} unpenalized 
% objective, \textit{middle:} $L1$ penalty, \textit{right:} penalized objective 
% (all as functions of $\thetab$). We see how the $L1$ penalty nudges the optimum 
% towards (0, 0) and compromises the original objective's smoothness.

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Numerical Properties: Convexity}

\begin{itemize}
  %\footnotesize
  \setlength\itemsep{1.2em}
  \item A function $\risket$ is convex if
  $$
  \riske\left(t \cdot \thetab + (1 - t) \cdot \tilde \thetab\right) \le t \cdot
  \riske\left(\thetab\right) + (1 - t) \cdot \riske\left(\tilde \thetab \right)
  $$
  $\forall$ $t \in [0, 1], ~\thetab, \tilde \thetab \in \Theta$\\
  (strictly convex if the above holds with strict inequality).
  \item In optimization, convex problems have a number of convenient properties. E.g., all local minima are 
  global. \vspace{0.2cm }\\
  $\rightarrow$ strictly convex function has at most 
  \textbf{one} global min (uniqueness).
  \item For $\riske \in \mathcal{C}^{2}$, $\riske$ is convex iff Hessian $\nabla^2 \risket$ is psd.
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Numerical properties: Convexity}

\begin{itemize}
  \setlength\itemsep{1em}
  \item Convexity of $\risket$ depends both on convexity of $L(\cdot)$ (given in most cases) and $\fxt$ (often problematic).
  \item If we model our data using an exponential family distribution, we always get convex losses
  \begin{itemize} 
    %\footnotesize
    \item For $\fxt$ linear in $\thetab$, linear/logistic/softmax/poisson/$\ldots$ regression are convex problems (all GLMs)! 
  \end{itemize}  
\end{itemize} 

\vfill

\begin{minipage}[b]{0.5\textwidth}
  \footnotesize \raggedright
  \href{https://arxiv.org/pdf/1712.09913.pdf}{Li et al., 2018: 
  \textit{Visualizing the Loss Landscape of Neural Nets}}. 
  The problem on the bottom right is convex, the others are not (note that 
  very high-dimensional surfaces are coerced into 3D here).  
  \\
  \phantom{foo}
\end{minipage}%
\begin{minipage}[b]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[b]{0.45\textwidth}
  \includegraphics[width=0.75\textwidth]{
  figure_man/convex-vs-nonconfex-landscape}
\end{minipage}%

\end{vbframe}

% ------------------------------------------------------------------------------



\begin{vbframe}{Numerical Properties: Convergence}

\footnotesize
%The choice of the loss function may also impact convergence behavior.

In case of \textbf{complete separation}, optimization might even 
fail entirely, e.g.:
%Consider the following scenario:

\vspace{0.5cm}

\begin{minipage}{0.7\textwidth}
  \begin{itemize}
    \item Margin-based loss that is strictly monotonicly decreasing in
    $y \cdot f$, e.g., \textbf{Bernoulli loss}: 
    % \scriptsize
    $$\Lxy = \log \left( 1 + \exp \left( - y  \fx \right)\right)$$
  \end{itemize}
\end{minipage}%
\begin{minipage}{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}{0.25\textwidth}
  \includegraphics[width=\textwidth]{figure/plot_bernoulli}
\end{minipage}%

\begin{itemize}
  \item $f$ linear in $\thetab$, e.g.,
  \textbf{logistic regression} with $\fxt = \thx$
  %\begin{itemize}
    %\small
  \item Data perfectly separable by our learner, so we can find $\thetab$:
   $$ \yi \fxit = \yi \thetab^T \xi > 0 ~~ \forall \xi$$
%    \\ \vspace{0.1cm} \scriptsize 
    %as every $\xi$ is correctly classified: $\fxit < 0$ for
    %$\yi = -1$, $> 0$ for $\yi = 1$ \small
   % \vspace{0.1cm}
 %   \item[$\Rightarrow$] $y \fxt = $ 
  %\end{itemize}
 % \vspace{0.1cm}
  %   \begin{itemize}
  %   \small
  %   \item[$\Rightarrow$] $f \left( \xv ~|~ a \cdot \thetab \right) = 
  %   a \cdot \fxt$ for $a > 1$
  % \end{itemize}
%\end{itemize}

% \vfill

% With optimization, e.g., gradient descent, we can always find a set of 
% parameters $\thetab^\prime$ that fully separates the data.

%\framebreak

%\begin{itemize} 
  %\small
 % \item In optimization, e.g., with gradient descent, we can always find a set 
  %of parameters $\thetab^\prime$ that classifies all samples perfectly.
   
  %But taking a closer look at $\risket$, we find that the same can be 
  %achieved with $2 \cdot \thetab^\prime$ -- and at lower risk:
  
\item Can now a construct a strictly better $\thetab$

$$    \riske(2 \cdot \thetab) = \sumin  
    % \log \left( 1 + \exp \left( - |2 \thetab^T \xi| \right)\right) \\
    L \left( 2 \yi \thetab^T \xi  
    \right) < \risket
$$

\item As ||$\thetab$|| increases, sum strictly decreases, as argument of L is strictly larger

  %  \begin{flalign*
    %\riske(2 \cdot \thetab) &= \sumin  
    % \log \left( 1 + \exp \left( - |2 \thetab^T \xi| \right)\right) \\
   % L \left( \left \rvert f\left( \xi ~|~ 2  \cdot \thetab \right) \right \rvert  
   % \right)
   %5 = \sumin L \left(2 \cdot \left \rvert %\fxit \right \rvert \right) \\
   %5 &< \sumin L \left( \left \rvert \fxit \right \rvert \right) = \risket
 % \end{flalign*}
\item We can iterate that, so there is no local (or global) optimum, and no numerical procedure can converge
%  \item This actually holds true for every $a \cdot \thetab$ with $a > 1$.
%  \begin{itemize}
%    \small
%    \item[$\Rightarrow$] By increasing $\| \thetab \|$, our loss becomes smaller 
%    and smaller, and optimization runs infinitely.
    % \item[$\Rightarrow$] This is a consequence of the above assumptions and 
    % sometimes encountered in logistic regression and linear support vector 
    % machines.
  \end{itemize}
%\end{itemize}  

\framebreak

\begin{itemize}
  \small
  \item
  Geometrically, this translates to an ever steeper slope of the 
  logistic/softmax function, i.e., increasingly sharp discrimination:
  
  \vspace{0.3cm}
  \begin{minipage}[b]{0.5\textwidth}
    \includegraphics[width=0.8\textwidth]{figure/softmax_1}
  \end{minipage}%
  \begin{minipage}[b]{0.5\textwidth}
    \includegraphics[width=0.8\textwidth]{figure/softmax_2}
  \end{minipage}%
  \item In practice, data are seldomly linearly separable and misclassified 
  examples act as counterweights to increasing parameter values.
  \item Besides, we can use \textbf{regularization} to encourage convergence 
  to robust solutions.
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

% \begin{vbframe}{Analytical Properties: Convergence}
% 
% \small
% The choice of the loss function may also impact the convergence behavior of the 
% optimization problem. 
% 
% \vspace{0.2cm}
% 
% \begin{minipage}[b]{0.7\textwidth}
%   \begin{itemize} 
%     \small
%     \item Example: optimizers, e.g., gradient descent, in logistic regression 
%     will not converge for 
%     linearly separable data (\textbf{complete separation}). 
%     \item This is a direct consequence of the convergence behavior of Bernoulli 
%     loss, which reaches 0 only in the infinite limit of the margin.
%   \end{itemize}  
% \end{minipage}%
% \begin{minipage}[b]{0.05\textwidth}
%   \phantom{foo}
% \end{minipage}%
% \begin{minipage}[b]{0.25\textwidth}
%   \includegraphics[width=\textwidth]{figure/plot_bernoulli}
% \end{minipage}%
% 
% % \vfill
% % \includegraphics[width=0.4\textwidth]{figure_man/snap_bernoulli_loss}
% % \vfill
% 
% \begin{itemize} 
%   \item In the case of complete separation, we have
%   \footnotesize
%   \begin{flalign*}
%     \risket &= \sumin \log \left( 1 + \exp \left( - \yi \thetab^T \xi \right)
%     \right) \\ &=
%     \sumin \log \left( 1 + \exp \left( - | \thetab^T \xi| \right)
%     \right),
%   \end{flalign*}
%   \small
%   as every observation is correctly classified (i.e., $\thetab^T \xi < 0$ \\for
%   $\yi = -1$ and $\thetab^T \xi > 0$ for $\yi = 1$).
% \end{itemize} 
% 
% \framebreak
% 
% \begin{itemize} 
%   \small
%   \item $\risket$ thus monotonically decreases in $\thetab$: if a parameter 
%   vector $\thetab^\prime$ is able to classify the samples perfectly, then 
%   $2\thetab^\prime$ also classifies the samples perfectly, and at lower risk.
%   \item Geometrically, this translates to an ever steeper slope of the 
%   logistic/softmax function, leading to increasingly sharp discrimination and 
%   infinitely running optimization.
%   
%   \vspace{0.3cm}
%   \begin{minipage}[b]{0.4\textwidth}
%     \includegraphics[width=0.8\textwidth]{figure/softmax_1}
%   \end{minipage}%
%   \begin{minipage}[b]{0.4\textwidth}
%     \includegraphics[width=0.8\textwidth]{figure/softmax_2}
%   \end{minipage}%
% \end{itemize}  
% 
% \vfill
% 
% 
% 
% \end{vbframe}

% ------------------------------------------------------------------------------

\endlecture

\end{document}