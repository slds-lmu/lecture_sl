\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\argminl}{\mathop{\operatorname{arg\,min}}\limits}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{
    Advanced Risk Minimization
  }{% Lecture title  
    Optimal constant model for the empirical log loss risk (Deep-Dive)
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/bernoulli_prob.png
  }{
  \item Derive the optimal constant model for the binary empirical log loss risk
  \item Derive the optimal constant model for the empirical multiclass log loss risk
}

\begin{frame2}[small]{Binary log loss: Emp. Risk Minimizer}

Given $n \in \mathbb{N}$ observations $y^{(1)}, \cdots, y^{(n)} \in \Yspace = \{0, 1\}$ we want to determine the optimal constant model for the empirical log loss risk.

\vfill

\begin{eqnarray*}
  \argminl_{\theta \in (0, 1)}\riske  & = & 
  \argminl_{\theta \in (0, 1)}-\sum^n_{i=1} y^{(i)}\log(\theta) + (1-y^{(i)})\log(1-\theta).
\end{eqnarray*}

The minimizer can be found by setting the derivative to zero, i.e.,

\vfill

\begin{align*}
  \frac{d}{d\theta}\riske\quad  & = & 
  -\sum^n_{i=1} \frac{y^{(i)}}{\theta} - \frac{1-y^{(i)}}{1-\theta} & \overset{!}{=}  0\\
  \quad&\iff&  -\sum^n_{i=1} y^{(i)}(1-\theta) - \theta(1-y^{(i)})  & \overset{!}{=} 0 \\
   \quad &\iff&  -\sum^n_{i=1} (y^{(i)} - \theta)  & \overset{!}{=} 0 \\
  \Rightarrow \hat{\theta} = \frac{1}{n}\sum^n_{i=1} y^{(i)}\quad &\quad\in& (0,1)\;\checkmark \text{(assuming both labels occur).}
\end{align*}

\end{frame2}

\begin{frame}{Multiclass log loss: Emp. Risk Minimizer}

Given $n \in \mathbb{N}$ observations $y^{(1)}, \cdots, y^{(n)} \in \Yspace = \{1, \dots, g\}$ with $g\in\mathbb{N}_{>1}$ we want to determine the optimal constant model $\theta \in (0,1)^g$ for the empirical log loss risk

\begin{eqnarray*}
  \argminl_{\theta \in (0, 1)^g}\riske   &=& 
  \argminl_{\theta \in (0, 1)^g}-\sum^n_{i=1}\sum^g_{j=1} \mathds{1}_{\{y^{(i)} = j\}}\log(\theta_j) \\
  &\text{s.t.}& \sum^g_{j=1}\theta_j = 1.
\end{eqnarray*}

We can solve this constrained optimization problem by plugging the constraint into the risk (we could also use Lagrange multipliers), i.e., we replace $\theta_g$ (this is an arbitrary choice) such that $
 \theta_g = 1 - \sum^{g-1}_{j=1}\theta_j.$

\end{frame}
\begin{frame}{Multiclass log loss: Emp. Risk Minimizer}
With this, we find the equivalent optimization problem
\begin{eqnarray*}
  \argminl_{\theta \in (0, 1)^{g-1}}\riske  & = & \argminl_{\theta \in (0, 1)^{g-1}}
  -\sum^n_{i=1}\sum^{g-1}_{j=1} \mathds{1}_{\{y^{(i)} = j\}}\log(\theta_j) \\ 
  &&+ \mathds{1}_{\{y^{(i)} = g\}}\log(1 - \sum\nolimits^{g-1}_{j=1}\theta_j) \\
  &\text{s.t.}&  \sum\nolimits^{g-1}_{j=1}\theta_j < 1.
\end{eqnarray*}

For $j \in \{1,\dots,g-1\}$, the $j$-th partial derivative of our objective 
\begin{eqnarray*}
 \frac{\partial}{\partial \theta_j}\riske  & = & 
  -\sum^n_{i=1} \mathds{1}_{\{y^{(i)} = j\}}\frac{1}{\theta_j} - \mathds{1}_{\{y^{(i)} = g\}}\frac{1}{1 - \sum^{g-1}_{j=1}\theta_j} \\
  & = & -\frac{n_j}{\theta_j} + \frac{n_g}{\theta_g}
\end{eqnarray*}
where $n_k$ with $k\in \{1,\dots, g\}$ is the number of label $k$ in $y$ and we assume that $n_k > 0.$

\end{frame}

\begin{frame}{Multiclass log loss: Emp. Risk Minimizer}

For the minimizer, it must hold for $j \in \{1,\dots,g-1\}$ that
\begin{eqnarray*}
 \frac{\partial}{\partial \theta_j}\riske  & \overset{!}{=} & 0 \\
  \iff  -n_j\theta_g + n_g\theta_j & \overset{!}{=} & 0 \\
 \Rightarrow \sum^{g-1}_{j=1}( -n_j\theta_g + n_g\theta_j) & \overset{!}{=} & 0 \\
  \iff -(n-n_g)\theta_g + n_g(1-\theta_g) & \overset{!}{=} & 0 \\
    \iff -n\theta_g + n_g & \overset{!}{=} & 0 \\
    \Rightarrow \hat{\theta}_g  = \frac{n_g}{n}& &  \in (0,1)\; \checkmark \\
    \Rightarrow   \forall j \in \{1,\dots,g-1\}:\quad \hat{\theta}_j =  \frac{\hat{\theta}_gn_j}{n_g} =  \frac{n_j}{n} &  &  \in (0,1)\; \checkmark. \\
    (\Rightarrow \sum\nolimits^{g-1}_{j=1}\hat{\theta}_j = 1- \hat{\theta}_g = 1 - \frac{n_g}{n} < 1 \checkmark
)
\end{eqnarray*}

\end{frame}

\begin{frame}{Convexity}
Finally, we check that we indeed found a minimizer by showing that $\riske$ is convex for the multiclass case (binary is a special case of this):

\vfill

The Hessian of $\riske$
\begin{eqnarray*}
\nabla^2_\theta\riske  & = & \begin{pmatrix}\frac{n_1}{\theta_1^2} & 0 & \dots & 0 \\
0 & \ddots & \ddots & \vdots \\
\vdots & \ddots & \ddots & 0 \\
0 & \dots & 0 & \frac{n_{g-1}}{\theta_{g-1}^2}
\end{pmatrix} \\
\end{eqnarray*}
is positive definite since all its eigenvalues $$\lambda_j = \frac{n_j}{\theta_j^2} > 0 \quad \forall j \in\{1,\dots,g-1\}.$$ 
From this, it follows that $\riske$ is (strictly) convex.
\end{frame}

\endlecture

\end{document}
