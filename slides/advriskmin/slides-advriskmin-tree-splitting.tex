\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-trees} % For the comparison of Brier and Gini index

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{
    Advanced Risk Minimization
  }{% Lecture title  
    Loss functions and tree splitting
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/cart_tree_i2ml.png
  }{
  \item Tree splitting loss vs impurity:
  \item Bernoulli loss $\sim$ entropy splitting
  \item Brier score $\sim$ gini splitting
}

\begin{framei}[sep=M]{Risk minimization and impurity}

%For an introduction on trees, see our \textbf{I2ML} lecture \\%(cf. our section on trees)%\\

    \item Tree fitting: Find best way to split parent node $\Np_0$ into child nodes $\Np_1$ and $\Np_2$, such that $\Np_1 \cup \Np_2 = \Np_0$ and $\Np_1 \cap \Np_2 = \emptyset$
    \item Two options for evaluating how good a split is: Per node $\Np$ compute the following:
\begin{enumerate}
    \item Compute impurity $\text{Imp}(\Np)$ directly from observations in $\Np$
    \item Fit optimal constant using loss function, sum up losses for $\Np$
\end{enumerate}
    \item Summarize on split level:
    \begin{enumerate}
        \item Weighted average ($n_0 = n_1 + n_2$ are number of obs in nodes)
        $$\text{Imp(split)} = \frac{n_1}{n_0} \text{Imp}(\Np_1) + \frac{n_2}{n_0} \text{Imp}(\Np_2)$$
        \item Sum of individual losses
        $$\risk(\text{split}) = \risk(\Np_1) + \risk(\Np_2)$$
    \end{enumerate}

\end{framei}

\begin{frame}{Bernoulli loss min = Entropy splitting}
\textbf{Claim:} Using entropy in (1) 
    %splitting $\text{Imp}(\Np) = -\textstyle\sum_{k = 1}^g \pikN \log \pikN$ 
    is equivalent to using Bernoulli loss in (2) %($\pikN := \frac{1}{n_{\Np}} \textstyle\sum_{(\xv,y) \in \Np} [y = k]$)

\textbf{Proof:} %To prove this we show that the risk related to a subset of observations 
$\Np \subseteq \D$ denotes subset of observations in that node. 

Risk $\risk(\Np)$ of node $\Np$ w.r.t. (multiclass) Bernoulli loss  
$$
  \Lpixy = -\sum_{k = 1}^g [y = k] \log (\pi_k(\xv))
$$
$\Rightarrow$ Optimal constant per node $\pikN = \frac{1}{n_{\Np}} \sum\limits_{(\xv,y) \in \Np} [y = k]$.\\

Entropy of node $\Np$:
$$
\text{Imp}(\Np) = -\textstyle\sum_{k = 1}^g \pikN \log \pikN %, \quad \pikN := \frac{1}{n_{\Np}} \textstyle\sum_{(\xv,y) \in \Np} [y = k]
$$

\end{frame}
\begin{frame2}[footnotesize]{Risk minimization and impurity}

\begin{eqnarray*}
\risk(\Np) &=& \sum_{\xy \in \Np} (- \sum_{k = 1}^g [y = k] \log \pi_k(\xv) ) \\
&=& -\sum_{k = 1}^g \sum_{\xy \in \Np} [y = k]\log \pikN \\
&=& -\sum_{k = 1}^g \log \pikN %\underbrace{\sum_{\xy \in \Np} [y = k]}_{n_{\Np}\cdot \pikN }
\\
 &=& -n_{\Np} \sum_{k = 1}^g \pikN \log \pikN = n_\Np \text{Imp}(\Np)\\
 \Rightarrow \risk(\text{split}) &=& \risk(\Np_1) + \risk(\Np_2)  = n_1 \text{Imp}(\Np_1) + n_2 \text{Imp}(\Np_2)\\
 &=&n_0 (\frac{n_1}{n_0} \text{Imp}(\Np_1) + \frac{n_2}{n_0} \text{Imp}(\Np_2)) = n_0\text{Imp(split)}\\
\
\end{eqnarray*} 
Bernoulli-risk of the split $\risk(\text{split})$ is proportional to its entropy-impurity $\text{Imp(split)}$, i.e., $\argmin_\text{split} \risk(\text{split}) = \argmin_\text{split} \text{Imp(split)}$ \\

\end{frame2}




\begin{frame}{Brier score minimization = Gini splitting}

\textbf{Claim:} Using Gini in (1) 
    %splitting $\text{Imp}(\Np) = -\textstyle\sum_{k = 1}^g \pikN \log \pikN$ 
    is equivalent to using Brier score in (2) %($\pikN := \frac{1}{n_{\Np}} \textstyle\sum_{(\xv,y) \in \Np} [y = k]$)
    

\textbf{Proof:} %To prove this we show that the risk related to a subset of observations 

Risk $\risk(\Np)$ of node $\Np$ w.r.t. (multiclass) Brier score
$$
  \Lpixy = \sum_{k = 1}^g ([y = k] - \pi_k(\xv))^2
$$
$\Rightarrow$ Optimal constant per node: $\pikN = \frac{1}{n_{\Np}} \sum\limits_{(\xv,y) \in \Np} [y = k] = \frac{n_{\Np,k}}{n_{\Np }}$ \\ ($n_{\Np,k}$ is the number of class $k$ observations in node $\Np$) \\

\vfill

Gini index of node $\Np$:
$$
\text{Imp}(\Np) = \sum_{k=1}^g \pikN (1-\pikN ) %, \quad \pikN := \frac{1}{n_{\Np}} \textstyle\sum_{(\xv,y) \in \Np} [y = k]
$$


\end{frame}
\begin{frame2}[footnotesize]{Brier score minimization = Gini splitting}

%\vspace*{-0.5cm}
\begin{eqnarray*}
\risk(\Np) &=& \sum_{\xy \in \Np}  \sum_{k = 1}^g ([y = k] - \pikN)^2 
= \sum_{k = 1}^g \sum_{\xy \in \Np} ([y = k] - \frac{n_{\Np,k}}{n_{\Np }})^2\\
% \end{eqnarray*}
% by plugging in the optimal constant prediction w.r.t. the Brier score ($n_{\Np,k}$ is defined as the number of class $k$ observations in node $\Np$): 
% $$\hat \pi_k(\xv)= \pikN = \frac{1}{n_{\Np}} \sum\limits_{(\xv,y) \in \Np} [y = k] = \frac{n_{\Np,k}}{n_{\Np }}. $$ 
%  We split the inner sum and further simplify the expression
% \begin{eqnarray*}
&=& \sum_{k = 1}^{g} (\sum_{\xy \in \Np: ~ y = k} (1 - \frac{n_{\Np,k}}{n_{\Np }})^2 + \sum_{\xy \in \Np: ~ y \ne k} (0 - \frac{n_{\Np,k}}{n_{\Np }})^2) \\
&=& \sum_{k = 1}^g n_{\Np,k}(1 - \frac{n_{\Np,k}}{n_{\Np }})^2 + (n_{\Np } - n_{\Np,k})(\frac{n_{\Np,k}}{n_{\Np }})^2, 
\end{eqnarray*}

since for $n_{\Np,k}$ observations the condition $y = k$ is met, and for the remaining $(n_\Np - n_{\Np,k})$ observations it is not. 

\end{frame2}

\begin{frame2}[footnotesize]{Brier score minimization = Gini splitting}

We further simplify the expression to

% \begin{footnotesize}
\begin{eqnarray*}
\risk(\Np) &=&  \sum_{k = 1}^g n_{\Np,k}(\frac{n_{\Np } - n_{\Np,k}}{n_{\Np }})^2 + (n_{\Np } - n_{\Np,k})(\frac{n_{\Np,k}}{n_{\Np }})^2 \\
&=& \sum_{k = 1}^g \frac{n_{\Np,k}}{n_{\Np }} \frac{n_{\Np } - n_{\Np,k}}{n_{\Np }} (n_{\Np } - n_{\Np,k } + n_{\Np,k}) \\
&=& n_{\Np } \sum_{k = 1}^g \pikN \cdot (1 - \pikN ) = n_\Np \text{Imp}(\Np)\\
 \Rightarrow \risk(\text{split}) &=& \risk(\Np_1) + \risk(\Np_2)  = n_1 \text{Imp}(\Np_1) + n_2 \text{Imp}(\Np_2)\\
 &=&n_0 (\frac{n_1}{n_0} \text{Imp}(\Np_1) + \frac{n_2}{n_0} \text{Imp}(\Np_2)) = n_0\text{Imp(split)}\\
\
\end{eqnarray*} 
Brier-risk of the split $\risk(\text{split})$ is proportional to its gini-impurity $\text{Imp(split)}$, i.e., $\argmin_\text{split} \risk(\text{split}) = \argmin_\text{split} \text{Imp(split)}$\\
% \end{footnotesize}

\end{frame2}


\endlecture

\end{document}