\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\argminl}{\mathop{\operatorname{arg\,min}}\limits}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Advanced Risk Minimization
  }{% Lecture title  
    Logistic regression (Deep-Dive)
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/plot_bernoulli_prob.png
  }{
  \item Derive the gradient of the logistic regression
  \item Derive the Hessian of the logistic regression
  \item Show that the logistic regression is a convex problem
}

\begin{vbframe}{Logistic regression: Risk Problem}

Given $n \in \mathbb{N}$ observations $\left(\xi, \yi\right) \in \Xspace \times \Yspace$ with  $\Xspace = \R^d, \Yspace = \{0, 1\}$ we want to minimize the following risk 


\vspace*{-0.5cm}

\begin{eqnarray*}
  \riske  & = & 
  -\sum^n_{i=1} \yi\log\left(\pixit\right) + \left(1-\yi\log(1-\pixit\right)
\end{eqnarray*}

with respect to $\thetab$ where the probabilistic classifier

\begin{eqnarray*}
  \pixit  & = & 
 s\left(\fxit\right),
\end{eqnarray*}

the sigmoid function $s(f) = \frac{1}{1 + \exp(-f)}$ and the score $\fxit = \thx.$

\vspace*{0.5cm} 

NB: Note that $\frac{\partial}{\partial f} s(f) = s(f)(1-s(f))$ and $\frac{\partial \fxit}{\partial \thetab} = \left(\xi\right)^\top.$

\end{vbframe}

\begin{vbframe}{Logistic regression: Gradient}

We find the gradient of logistic regression with the chain rule, s.t., 

\vspace*{-0.5cm}

\begin{align*}
  \frac{\partial}{\partial\thetab}\riske  & = & 
 -\sumin \frac{\partial}{\partial\pixit }\yi\log(\pixit)\frac{\partial\pixit}{\partial \thetab} +  \\
 &&  \frac{\partial}{\partial\pixit} (1-\yi)\log(1-\pixit)\frac{\partial\pixit}{\partial \thetab}\\
 & = & 
 -\sumin \frac{\yi}{\pixit}\frac{\partial\pixit}{\partial \thetab} -  \frac{1-\yi}{1-\pixit}\frac{\partial\pixit}{\partial \thetab}\\
 &=&  
  -\sumin \left(\frac{\yi}{\pixit} -  \frac{1-\yi}{1-\pixit}\right)\frac{\partial s(\fxit)}{\partial  \fxit}\frac{\partial  \fxit}{\partial\thetab}\\
  &=&  
  -\sum^n_{i=1} \left(\yi(1-\pixit)  -  (1-\yi)\pixit \right)\left(\xi\right)^\top.\\
\end{align*}



\framebreak
\begin{align*}
  \quad &=& 
  \sumin \left(\pixit - \yi\right)\left(\xi\right)^\top.\\
    \quad &=& 
  \left(\pi(\mathbf{X}\vert\;\thetab) - \mathbf{y}\right)^\top\mathbf{X}\\
\end{align*}

where  $\mathbf{X} = \left(
    \xi[1], \dots, 
    \xi[n]\right)^\top \in \R^{n\times d}, \mathbf{y} = \left(
    \yi[1], \dots,
    \yi[n]
\right)^\top,$ \\ $\pi(\mathbf{X}\vert\;\thetab) = \left(
    \pixit[1], \dots,
    \pixit[n]
\right)^\top \in \R^{n}$.

\vspace*{1cm}

$\Rightarrow$ The gradient $\nabla_{\thetab}\riske = \left(\frac{\partial}{\partial\thetab}\riske\right)^\top =  \mathbf{X}^\top\left(\pi(\mathbf{X}\vert\;\thetab) - \mathbf{y}\right)$ 

\vspace*{1cm}

This formula can now be used in gradient descent and its friends.

\end{vbframe}


\begin{vbframe}{Logistic regression: Hessian}

We find the Hessian via differentiation, s.t.,

\begin{align*}
  \nabla^2_{\thetab}\riske  = \frac{\partial^2}{\partial{\thetab^\top}\partial\thetab}\riske  & = & 
 \frac{\partial}{\partial{\thetab^\top}} \sumin \left(\pixit - \yi\right)\left(\xi\right)^\top\\
 & = & 
  \sum^n_{i=1}\xi \left(\pixit\left(1-\pixit\right)\right)\left(\xi\right)^\top\\
  & = & 
\mathbf{X}^\top \mathbf{D} \mathbf{X}\\
\end{align*}

where $\mathbf{D} \in \mathbb{R}^{n\times n}$ is a diagonal matrix with diagonal 
$$\left(\pixit[1](1-\pixit[1], \dots, \pixit[n](1-\pixit[n]\right).$$

Can now be used in Newton-Raphson and other 2nd order optimizers.


\end{vbframe}

\begin{vbframe}{Logistic regression: Convexity}
Finally, we check that logistic regression is a convex problem:
\vspace*{0.3cm}

We define the diagonal matrix $\bar{\mathbf{D}} \in \mathbb{R}^{n \times n}$ with diagonal 
$$\left(\sqrt{\pixit[1])(1-\pixit[1]}, \dots, \sqrt{\pixit[n](1-\pixit[n]}\right) $$
which is possible since $\pi$ maps into (0, 1). \\
\vspace*{0.3cm}
With this, we get for any $\mathbf{w} \in \mathbb{R}^d$ that

$$\mathbf{w}^\top  \nabla^2_{\thetab}\riske \mathbf{w} =   \mathbf{w}^\top  \mathbf{X}^\top \bar{\mathbf{D}}^\top \bar{\mathbf{D}}\mathbf{X} \mathbf{w} = (\bar{\mathbf{D}}\mathbf{X} \mathbf{w})^\top\bar{\mathbf{D}}\mathbf{X} \mathbf{w} = \Vert \bar{\mathbf{D}}\mathbf{X} \mathbf{w} \Vert^2_2 \geq 0$$

since obviously $\mathbf{D} = \bar{\mathbf{D}}^\top \bar{\mathbf{D}}.$ \\
\vspace*{0.3cm}
$\Rightarrow \nabla^2_{\thetab}\riske$ is positive semi-definite $\Rightarrow \riske$ is convex.

\end{vbframe}

\endlecture

\end{document}
