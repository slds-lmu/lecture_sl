\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\argminl}{\mathop{\operatorname{arg\,min}}\limits}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{
Advanced Risk Minimization
}{
Logistic regression (Deep-Dive)
}{
figure/bernoulli_prob.png
}{
\item Derive the gradient of the logistic regression
\item Derive the Hessian of the logistic regression
\item Show that the logistic regression is a convex problem
}

\begin{frame}{Logistic regression: Risk Problem}

Given $n$ observations $(\xi, \yi) \in \Xspace \times \Yspace$ with  $\Xspace = \R^d, \Yspace = \setzo$ we want to minimize the risk 

$$
\riske   = 
-\sum^n_{i=1} \yi\log(\pixit) + (1-\yi)\log(1-\pixit))
$$

with respect to $\thetav$ where the probabilistic classifier

$$
\pixit = s(\fxit)
$$

the sigmoid function $s(f) = \frac{1}{1 + \exp(-f)}$ and the score $\fxit = \thx$

\vfill

\textbf{NB}: Note that $\frac{\partial}{\partial f} s(f) = s(f)(1-s(f))$ and $\frac{\partial \fxit}{\partial \thetav} = (\xi)^\top$\\

\vfill

From now on we abbreviate $\pixit$ as $\pi^{(i)}_{\thetav}$

\end{frame}

\begin{frame}{Logistic regression: Gradient}

We find the gradient of logistic regression with the chain rule:

\vfill

{\small
\begin{align*}
\frac{\partial}{\partial\thetav}\riske  & =  
-\sumin \frac{\partial}{\partial \pi^{(i)}_{\thetav}  }\yi\log( \pi^{(i)}_{\thetav} )\frac{\partial \pi^{(i)}_{\thetav} }{\partial \thetav} +  \\
& \qquad \frac{\partial}{\partial \pi^{(i)}_{\thetav} } (1-\yi)\log(1- \pi^{(i)}_{\thetav} )\frac{\partial \pi^{(i)}_{\thetav} }{\partial \thetav}\\
& =  
-\sumin \frac{\yi}{ \pi^{(i)}_{\thetav} }\frac{\partial \pi^{(i)}_{\thetav} }{\partial \thetav} -  \frac{1-\yi}{1- \pi^{(i)}_{\thetav} }\frac{\partial \pi^{(i)}_{\thetav} }{\partial \thetav}\\
&=  
-\sumin (\frac{\yi}{ \pi^{(i)}_{\thetav} } -  \frac{1-\yi}{1- \pi^{(i)}_{\thetav} })\frac{\partial s(\fxit)}{\partial  \fxit}\frac{\partial  \fxit}{\partial\thetav}\\
&=  
-\sum^n_{i=1} (\yi(1- \pi^{(i)}_{\thetav} )  -  (1-\yi) \pi^{(i)}_{\thetav}  )(\xi)^\top\\
\end{align*}
}


\end{frame}

\begin{frame}{Logistic regression: Gradient}

\begin{align*}
\quad &=& 
\sumin ( \pi^{(i)}_{\thetav}  - \yi)(\xi)^\top\\
\quad &=& 
(\pi(\mathbf{X}\vert\;\thetav) - \mathbf{y})^\top\mathbf{X}\\
\end{align*}

where

\begin{itemizeM}
\item $\mathbf{X} = (
\xi[1], \dots, 
\xi[n])^\top \in \R^{n\times d}$
\item $\mathbf{y} = (
\yi[1], \dots,\yi[n])^\top$
\item $\pi(\mathbf{X}\vert\;\thetav) = (
 \pi^{(i)}_{\thetav} [1], \dots,
\pi^{(i)}_{\thetav}[n]
)^\top \in \R^{n}$
\end{itemizeM}

\vfill

$\implies$ The gradient $\nabla_{\thetav}\riske = (\frac{\partial}{\partial\thetav}\riske)^\top =  \mathbf{X}^\top(\pi(\mathbf{X}\vert\;\thetav) - \mathbf{y})$ 

\vfill

This formula can now be used in gradient descent and its friends

\end{frame}


\begin{frame}{Logistic regression: Hessian}

We find the Hessian via differentiation:

{\small
\begin{align*}
\nabla^2_{\thetav}\riske  = \frac{\partial^2}{\partial{\thetav^\top}\partial\thetav}\riske  & =  
\frac{\partial}{\partial{\thetav^\top}} \sumin ( \pi^{(i)}_{\thetav} - \yi)(\xi)^\top\\
& =  
\sum^n_{i=1}\xi ( \pi^{(i)}_{\thetav}(1- \pi^{(i)}_{\thetav}))(\xi)^\top\\
& =  
\mathbf{X}^\top \mathbf{D} \mathbf{X}\\
\end{align*}

where $\mathbf{D} \in \mathbb{R}^{n\times n}$ is a diagonal matrix with diagonal 
$$\Big( \pi^{(i)}_{\thetav}[1](1- \pi^{(i)}_{\thetav}[1], \dots,  \pi^{(i)}_{\thetav}[n](1- \pi^{(i)}_{\thetav}[n]\Big)$$
}

Can now be used in Newton-Raphson and other 2nd order optimizers


\end{frame}

\begin{frame}{Logistic regression: Convexity}
Finally, we check that logistic regression is a convex problem:
\vfill

We define the diagonal matrix $\bar{\mathbf{D}} \in \mathbb{R}^{n \times n}$ with diagonal 
$$\Big(\sqrt{ \pi^{(i)}_{\thetav}[1])(1- \pi^{(i)}_{\thetav}[1]}, \dots, \sqrt{ \pi^{(i)}_{\thetav}[n](1- \pi^{(i)}_{\thetav}[n]}\Big) $$
which is possible since $\pi$ maps into (0, 1) \\
\vfill
With this, we get for any $\mathbf{w} \in \mathbb{R}^d$ that

$$\mathbf{w}^\top  \nabla^2_{\thetav}\riske \mathbf{w} =   \mathbf{w}^\top  \mathbf{X}^\top \bar{\mathbf{D}}^\top \bar{\mathbf{D}}\mathbf{X} \mathbf{w} = (\bar{\mathbf{D}}\mathbf{X} \mathbf{w})^\top\bar{\mathbf{D}}\mathbf{X} \mathbf{w} = \Vert \bar{\mathbf{D}}\mathbf{X} \mathbf{w} \Vert^2_2 \geq 0$$

since obviously $\mathbf{D} = \bar{\mathbf{D}}^\top \bar{\mathbf{D}}$ \\
\vfill
$\implies \nabla^2_{\thetav}\riske$ is positive semi-definite $\implies \riske$ is convex

\end{frame}

\endlecture

\end{document}
