\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}


\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-hpo}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{
Advanced Risk Minimization
}{
Proper Scoring Rules
}{
figure/bernoulli_prob.png
}{
\item Scoring rules on prob. predictions
\item First order condition of SRs
\item Log loss and Brier are strictly proper
\item L1 on probs is not
\item 0/1 is proper but not strict
}

\begin{framei}[sep=M]{Prob. Preds / scoring rules \furtherreading{GNEITING2007}}

\item Is specific loss $\Lpixy$ on a prob. classifier ``reasonable''?
\item Assume binary classification with $y \in \setzo$
\item Loss can already be called scoring rule, but we also now take expectation over $y$
\item Scoring rule compares predictive vs. label distrib: 
$$ S(Q, P) = \E_{y \sim Q}[L(Q, P)]$$
\item Simply expected loss, now we write $P$ for $\pix$
\item We have looked at this before
\item As we have seen in the beginning of this chapter:\\ 
Can do this unconditionally or conditionally, all the same anyway
\item Minimizing the above asks then for the risk minimizer!


\end{framei}

\begin{framei}[sep=M]{Proper scoring rule \furtherreading{GNEITING2007}}

\item SR is \textbf{proper} if true label distrib $Q$ is among the optimal 
solutions, when we maximize $S(Q, P)$ in the 2nd argument (for a given $Q$)
$$S(Q,Q) \leq S(Q, P) \,\, \text{for all} \, P,Q $$

\item Translation (now for conditional): \\
$\P(y = 1 ~|~ \xv = \xtil)$ is \textbf{among the solutions} for the RM: 
$$
   \eta(\xtil) = \P(y = 1 ~|~ \xv = \xtil) \in \piastxtil
$$

\item NB: We were never precise before that the RM as ``argmin'' is a set!

\item $S$ is \textbf{strictly proper} when equality holds iff $P=Q$

\item Translation (now for conditional): \\
$\P(y = 1 ~|~ \xv = \xtil)$ is unique RM! 
$$
   \piastxtil = \eta(\xtil) = \P(y = 1 ~|~ \xv = \xtil)
$$

\item (Strictly) proper SR ensure optimization pushes solution $\pix$ to $Q$ 


\end{framei}

\begin{framei}[sep=M]{L1 loss is not proper}


\item Let's look at it unconditionally, so we don't always have to write $x$
    
\item Binary targets $y \sim \text{Bern}(\eta)$ %Scoring rules are often related to loss functions $L$ by taking their negative value (maximization vs. minimization).

%\item We want to find out if using a loss $\Lpixy$  incentivizes honest predictions $\pix=\eta$ for any $\eta \in [0,1]$

\item For any binary prob. loss $L$:
$$\E_{y}[\Lpiy]=\eta \cdot L(1, \pi) + (1-\eta) \cdot L(0, \pi)$$

\item Let's check L1 loss $\Lpiy=|y-\pi|$
$$\E_y[\Lpiy]=\eta |1-\pi| + (1-\eta) \pi = \eta+\pi(1-2\eta)$$

\item Linear in $\pi$, but with box constraints

% \item The expected loss is linear in $\pi$, hence we minimize it by setting $\pi = 1$ for $p>0.5$ and $\pi = 0$ for $p<0.5$. 

\item For $\eta > 0.5$: $\pibayes =1$ 

\item For $\eta < 0.5$: $\pibayes = 0$

\item So $\pibayes$ usually not the same as $\eta$

\item True $\eta$ is worse than RM $\pibayes$, so L1 not proper SR


\end{framei}

\begin{framei}[sep=M]{0/1 loss is proper -- but not strict}

\item \textbf{0/1 loss} $\Lpiy=\mathds{1}_{\{y \neq h_\pi\}}$ using hard labeler  $h_{\pi}=\mathds{1}_{\{\pi\geq0.5\}}$ 
\item Expected loss:
\begin{eqnarray*}
\E_y[\Lpiy] &=& \eta \cdot L(1,\pi) + (1-\eta) \cdot L(0,\pi)\\
&=& \left\{
\begin{array}{ll}
\eta & \text{if } h_{\pi} = 0 \\
1-\eta & \text{if } h_{\pi} = 1
\end{array}
\right.
\end{eqnarray*}

\item So expected loss only takes 2 values

\item For $\eta \geq 0.5$, minimal if $h_{\pi}=1$\\ $\rightarrow$ any $\pi \in [0.5,1]$ minimizes 

\item For $\eta < 0.5$ expected loss is minimal if $h_{\pi}=0$\\ $\rightarrow$ any $\pi \in [0, 0.5)$ minimizes 

\item True $\eta$ among solutions $\rightarrow$ proper
\item True $\eta$ is not unique minimizer $\rightarrow$ not strictly proper

\end{framei}

\begin{frame}{Discover proper scoring rules}

\begin{itemizeL}

\item How define $L$ such that $\E_y[\Lpiy]$ is minimized at $\pibayes=\eta$? 

\item Assume symmetry: 
$L(1,\pi)=L(\pi)$ and $L(0,\pi)=L(1-\pi)$ \\

\end{itemizeL}

\splitV[0.6]{
\begin{itemizeL}
\item Remember that when we plotted such losses, we usually had 2 curves,
symmetric at $\pi=0.5$?
\end{itemizeL}
}
{\imageL[0.9]{figure/bernoulli_prob.png}}

\begin{itemizeL}
\item Then, with $\eta = \P(y = 1)$
$$\E_{y}[\Lpiy]=\eta \cdot L(\pi) + (1-\eta) \cdot L(1-\pi)$$
\item First-order condition: Set $L'(\pi) = 0$, and $\pi=\eta$ at minimum 
$$\eta \cdot L'(\eta) \overset{!}{=} (1-\eta) \cdot L'(1-\eta)$$

\end{itemizeL}
\end{frame}

\begin{framei}[sep=M]{Log loss / Brier are strictly proper}

\item First-order condition: 
$$\eta \cdot L'(\eta) \overset{!}{=} (1-\eta) \cdot L'(1-\eta)$$
\item One solution is $L'(\eta)=-1/\eta$ % resulting in $-p/p=-(1-p)/(1-p)=-1$ and the
\item Antiderivative is $L(\eta)=-\log(\eta)$ 
\item Remember: 
$L(1, \pi) = L(\pi) \ \text{and} \ L(0, \pi) = L(1-\pi)$
\item This is \textbf{log loss} 
$$\Lpiy=-y \cdot \log(\pi) - (1-y) \cdot \log(1-\pi)$$

\item Second solution is $L'(\eta)=-2(1-\eta)$% resulting in $-2p(1-p)=-2(1-p)p$ and the 
\item Antiderivative $L(\eta)=(1-\eta)^2$%=\frac{1}{2}((1-\eta)^2+(0-(1-\eta))^2)$
\item So \textbf{Brier score} 
$$\Lpiy=(y - \pi)^2$$


\item Both strictly proper (check 2nd derivative for strict convexity)


\end{framei}

\begin{framei}[sep=L]{Outlook}

\item Usually SRs are maximimized, I adapted notation a bit here
for us, to get a direct connection ERM

\item Was easier to talk about binary classification here, 
but proper SRs are defined in general

\item There are other proper SRs, like ``generalized entropy score'' or ''continuous ranked probability score'' 

\item We only scratched surface of theory

\item  If you want to know more: start by reading the Gneiting paper

\end{framei}



\endlecture

\end{document} 
