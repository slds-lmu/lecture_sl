\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-hpo}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{
Advanced Risk Minimization
}{
Proper Scoring Rules
}{
figure/bernoulli_prob.png
}{
\item Scoring rules on prob. predictions
\item First order condition of SRs
\item Log loss and Brier are strictly proper
\item L1 on probs is not
\item 0/1 is proper but not strict
}

\begin{vbframe}{Prob. Preds / scoring rules \citelink{GNEITING2007}}

\begin{itemizeM}
\item Is specific loss $\Lpixy$ on a prob. classifier ``reasonable''?
\item Assume binary classification with $y \in \setzo$
\item Loss can already be called scoring rule, but we also now take expectation over $y$
\item Scoring rule compares predictive vs. label distrib: 
$$ S(Q, P) = \E_{y \sim Q}[L(Q, P)]$$
\item Simply expected loss, now we write $P$ for $\pix$
\item We have looked at this before
\item As we have seen in the beginning of this chapter:\\ 
Can do this unconditionally or conditionally, all the same anyway
\item Minimizing the above asks then for the risk minimizer!


\end{itemizeM}

\end{vbframe}

\begin{vbframe}{Proper scoring rule \citelink{GNEITING2007}}

\begin{itemize}

\item SR is \textbf{proper} if true label distrib $Q$ is among the optimal 
solutions, when we maximize $S(Q, P)$ in the 2nd argument (for a given $Q$)
$$S(Q,Q) \leq S(Q, P) \,\, \text{for all} \, P,Q $$

\item Translation (now for conditional): \\
$\P(y = 1 ~|~ \xv = \xtil)$ is \textbf{among the solutions} for the RM: 
$$
   \eta(\xtil) = \P(y = 1 ~|~ \xv = \xtil) \in \piastxtil
$$

\item NB: We were never precise before that the RM as ``argmin'' is a set!

\item $S$ is \textbf{strictly proper} when equality holds iff $P=Q$

\item Translation (now for conditional): \\
$\P(y = 1 ~|~ \xv = \xtil)$ is unique RM! 
$$
   \piastxtil = \eta(\xtil) = \P(y = 1 ~|~ \xv = \xtil)
$$

\item (Strictly) proper SR ensure optimization pushes solution $\pix$ to $Q$ 



\end{itemize}


\end{vbframe}

\begin{vbframe}{L1 loss is not proper}

\begin{itemize}

\item Let's look at it unconditionally, so we don't always have to write $x$
    
\item Binary targets $y \sim \text{Bern}(\eta)$ %Scoring rules are often related to loss functions $L$ by taking their negative value (maximization vs. minimization).

%\item We want to find out if using a loss $\Lpixy$  incentivizes honest predictions $\pix=\eta$ for any $\eta \in [0,1]$

\item For any binary prob. loss $L$:
$$\E_{y}[L(y, \pi)]=\eta \cdot L(1, \pi) + (1-\eta) \cdot L(0, \pi)$$

\item Let's check L1 loss $L(y, \pi)=|y-\pi|$
$$\E_y[L(y, \pi)]=\eta |1-\pi| + (1-\eta) \pi = \eta+\pi(1-2\eta)$$

\item Linear in $\pi$, but with box constraints

% \item The expected loss is linear in $\pi$, hence we minimize it by setting $\pi = 1$ for $p>0.5$ and $\pi = 0$ for $p<0.5$. 

\item For $\eta > 0.5$: $\pi^{\ast} =1$ 

\item For $\eta < 0.5$: $\pi^{\ast} = 0$

\item So $\pi^{\ast}$ usually not the same as $\eta$

\item True $\eta$ is worse than RM $\pi^{\ast}$, so L1 not proper SR

%\item The score $S(\pi,y)=-\Lpiy$ is therefore not proper.

\end{itemize}

\end{vbframe}

\begin{vbframe}{0/1 loss is proper -- but not strict}
\begin{itemize}
\item \textbf{0/1 loss} $L(y, \pi)=\mathds{1}_{\{y \neq h_\pi\}}$ using hard labeler  $h_{\pi}=\mathds{1}_{\{\pi\geq0.5\}}$ 
\item Expected loss:
\begin{eqnarray*}
\E_y[\L(y, \pi)] &=& \eta \cdot L(1,\pi) + (1-\eta) \cdot L(0,\pi)\\
&=& \left\{
\begin{array}{ll}
\eta & \text{if } h_{\pi} = 0 \\
1-\eta & \text{if } h_{\pi} = 1
\end{array}
\right.
\end{eqnarray*}

\item So expected loss only takes 2 values

\item For $\eta \geq 0.5$, minimal if $h_{\pi}=1$\\ $\rightarrow$ any $\pi \in [0.5,1]$ minimizes 

\item For $\eta < 0.5$ expected loss is minimal if $h_{\pi}=0$\\ $\rightarrow$ any $\pi \in [0, 0.5)$ minimizes 

\item True $\eta$ among solutions $\rightarrow$ proper
\item True $\eta$ is not unique minimizer $\rightarrow$ not strictly proper

\end{itemize}


%$$\E_y[L(y, \pi)]=p L(1,\pi) + (1-p) L(0,\pi) = $$
\end{vbframe}

\begin{vbframe}{Discover proper scoring rules}

\begin{itemizeL}
    \item How define $L$ such that $\E_y[L(y, \pi)]$ is minimized at $\pi^{\ast}=\eta$? 
%$$\E_{y}[L(y, \pi)]=\eta \cdot L(1,\pi) + (1-\eta) \cdot L(0,\pi)$$

\item Assume symmetry: 
$L(1,\pi)=L(\pi)$ and $L(0,\pi)=L(1-\pi)$ \\

\item 
\begin{minipage}[t]{0.5\textwidth}
Remember that when we plotted such losses, we usually had 2 curves,
symmetric at $\pi=0.5$?
\end{minipage}
\hfill
\begin{minipage}[t]{0.4\textwidth}
\vspace{-0.1cm}
\begin{center}
\includegraphics[width=2.5cm]{figure/bernoulli_prob.png}
\end{center}
\end{minipage}
\item Then, with $\eta = \P(y = 1)$
$$\E_{y}[L(y, \pi)]=\eta \cdot L(\pi) + (1-\eta) \cdot L(1-\pi)$$
\item First-order condition: Set $L'(\pi) = 0$, and $\pi=\eta$ at minimum 
$$\eta \cdot L'(\eta) \overset{!}{=} (1-\eta) \cdot L'(1-\eta)$$
\end{itemizeL}

\end{vbframe}

\begin{vbframe}{Log loss / Brier are strictly proper}

\begin{itemize}
    \item First-order condition: 
    $$\eta \cdot L'(\eta) \overset{!}{=} (1-\eta) \cdot L'(1-\eta)$$
    \item One solution is $L'(\eta)=-1/\eta$ % resulting in $-p/p=-(1-p)/(1-p)=-1$ and the
    \item Antiderivative is $L(\eta)=-\log(\eta)$ 
    \item Remember: 
    $L(1, \pi) = L(\pi) \ \text{and} \ L(0, \pi) = L(1-\pi)$
    \item This is \textbf{log loss} 
    $$L(y, \pi)=-y \cdot \log(\pi) - (1-y) \cdot \log(1-\pi)$$

    \item Second solution is $L'(\eta)=-2(1-\eta)$% resulting in $-2p(1-p)=-2(1-p)p$ and the 
    \item Antiderivative $L(\eta)=(1-\eta)^2$%=\frac{1}{2}((1-\eta)^2+(0-(1-\eta))^2)$
    \item So \textbf{Brier score} 
    $$L(y, \pi)=(y - \pi)^2$$
  

\item Both strictly proper (check 2nd derivative for strict convexity)
\end{itemize}

\end{vbframe}

\begin{vbframe}{Outlook}

\begin{itemizeL}

    \item Usually SRs are maximimized, I adapted notation a bit here
    for us, to get a direct connection ERM

    \item Was easier to talk about binary classification here, 
    but proper SRs are defined in general

    \item There are other proper SRs, like ``generalized entropy score'' or ''continuous ranked probability score'' 

    \item We only scratched surface of theory

    \item  If you want to know more: start by reading the Gneiting paper
    
\end{itemizeL}


\end{vbframe}



\endlecture

\end{document} 
