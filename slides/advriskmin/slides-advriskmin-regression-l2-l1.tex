\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Advanced Risk Minimization
  }{% Lecture title  
    Regression Losses: L2 and L1 loss
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/loss_quadratic_1.png
  }{
  \item Derive the risk minimizer of the L2-loss
  \item Derive the optimal constant model for the L2-loss
  \item Know risk minimizer and optimal constant model for L1-loss
}

\begin{vbframe}{L2-Loss}

\vspace*{-0.5cm}

$$
\Lyf = \left(y-f\right)^2 \quad \text{or} \quad \Lyf = 0.5 \left(y-f\right)^2
$$

\vspace*{-2mm}

\begin{itemize}
\item Tries to reduce large residuals (if residual is twice as large, loss is 4 times as large), hence outliers in $y$ can become problematic
\item Analytic properties: convex, differentiable $\Rightarrow$ gradient no problem in loss minimization\\ {\small (\textbf{Warning}: $\riskef$ can still be non-smooth/non-convex due to $f(\xv)$)}
%\item Residuals = Pseudo-residuals: \begin{footnotesize} $\tilde r = - \pd{0.5 (y-\fx)^2}{\fx} = y - \fx = r$\end{footnotesize}
\end{itemize}

% <<loss-quadratic-plot, echo=FALSE, results='hide', fig.height=3>>=
% x = seq(-2, 2, by = 0.01); y = x^2
% qplot(x, y, geom = "line", xlab = expression(y-f(x)), ylab = expression(L(y-f(x))))
% @

% \framebreak




\begin{center}
  \includegraphics[width = 9cm]{figure/loss_quadratic_2.png} \\
\end{center}

\end{vbframe}




\begin{vbframe}{L2-loss: Optimal Constant Model}
Let us consider the (true) risk for  $\Yspace = \R$ and $L2$-Loss $\Lyf = \left(y-f\right)^2$ with $\Hspace$ restricted to constants. The optimal constant model $f_{c}^{\ast}$ in terms of the theoretical risk is the expected value over $y$: 

\medskip

  \begin{eqnarray*}
    f_{c}^{\ast} &=& \argmin_{c \in \mathbb{R}} \E_{xy} \left[(y-c)^2\right] = \argmin_{c \in \mathbb{R}} \E_{y} \left[(y-c)^2\right] %= \E_{y~|~\xv}\left[y~|~\xv\right] \overset{\text{drop } \xv}{=} \E_y \left[y\right] 
    \\
    %&=& \argmin_{c \in \mathbb{R}} \E_{y} \left[(y-c)^2\right] \\
    &=& \argmin_{c \in \mathbb{R}} \underbrace{\E_{y}\left[(y - c)^2\right] - \left(\E_{y}[y] - c\right)^2}_{= \var_{y}[y - c] = \var_{y}[y]} + \left(\E_{y}[y] - c\right)^2 \\
    &=& \argmin_{c \in \mathbb{R}} \var_{y}[y] + \left(\E_{y}[y] - c\right)^2  \\
    &=& \E_{y}[y]
  \end{eqnarray*} 

\framebreak 

The optimizer $\hat{f}_{c}$ of the empirical risk is $\bar y$ (the empirical mean over $\yi$), which is the empirical estimate for $\E_y \left[y\right]$. 

\begin{center}
\includegraphics[width = 0.6\textwidth ]{figure/L2-loss.png} \\
\end{center}

\framebreak

\textbf{Proof: }

\vspace{0.2cm}

For the optimal constant model $f_{c}^{\ast}$ for the L2-loss $\Lyf = \left(y - f\right)^2$ we solve the optimization problem 
$$
\argmin_{f \in \Hspace} \riskef = \argmin_{\theta \in \R} \sumin (\yi - \theta)^2. 
$$


We calculate the first derivative of $\riske$ w.r.t. $\theta$ and set it to $0$: 


\begin{eqnarray*}
\frac{\partial \risket}{\partial \theta} = -2 \sumin \left(\yi - \theta\right) &\overset{!}{=}& 0 \\
\sumin \yi - n \theta&=& 0 \\
\hat \theta&=& \frac{1}{n} \sumin \yi =: \bar y.
\end{eqnarray*}


\end{vbframe}

\begin{vbframe}{L2-Loss: Risk Minimizer}

Let us consider the (true) risk for  $\Yspace = \R$ and the $L2$-Loss $\Lyf = \left(y-f\right)^2$ with unrestricted $\Hspace = \{f: \Xspace \to \R^g\}$. 

\begin{itemize}
\item By the law of total expectation
  \begin{eqnarray*}
    \risk_L(f) &=& \E_{xy} \left[\Lxy\right] 
    = \E_x \left[\E_{y|x}\left[\Lxy~|~\xv = \xv\right]\right] \\
  &=& \E_x
  \left[\E_{y|x}\left[(y-\fx)^2~|\xv = \xv\right]\right]. 
  \end{eqnarray*}
% \item As our hypothesis space is not restricted at all, we can proceed quite \enquote{arbitrarily} when constructing our model $\hat f$.  

\item Since $\Hspace$ is unrestricted, at any point $\xv = \xv$, we can predict any value $c$ we want. The best point-wise prediction is the cond. mean
$$
  \fxbayes = \argmin_c \E_{y|x}\left[(y - c)^2 ~|~ \xv = \xv \right]\overset{(*)}{=} \E_{y|x} \left[y ~|~ \xv \right]. 
$$

\begin{footnotesize}
$^{(*)}$ follows from the drivation of $f_{c}^{\ast}$
%  \begin{eqnarray*}
%  && \mbox{argmin}_c \E\left[(y - c)^2\right] = \mbox{argmin}_c \underbrace{\E\left[(y - c)^2\right] - \left(\E[y] - c\right)^2}_{= \var[y - c] = \var[y]} + \left(\E[y] - c\right)^2 \\ &=& \mbox{argmin}_c \var[y] + \left(\E[y] - c\right)^2 = \E[y]. 
%  \end{eqnarray*}
\end{footnotesize}

\end{itemize}


\end{vbframe}

\begin{vbframe}{L2 loss means minimizing variance}

Rethinking what we did in the opt. constant model:
We optimized for a constant whose squared distance to all data points is minimal
(in sum, or on average). This turned out to be the mean.

\vspace{0.1cm}

What if we calculcate the loss of $\hat{\theta} = \bar{y}$? That's ${\textstyle \riske = \sumin (\yi - \bar{y})^2}$. Average this by $\frac{1}{n}$ or $\frac{1}{n-1}$ 
to obtain variance. 

\vspace{0.2cm}


\begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{figure_man/plot_const_var.png}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
    \begin{itemize}
    \small
        \item  Generally, if model yields unbiased predictions, { $\E_{y~|~\xv}\left[y-\fx~|~\xv\right]=0$}, using $L2$-loss means minimizing variance of model residuals
        \item Same holds for the pointwise construction / conditional distribution considered before
     \end{itemize}
\end{minipage}


%\vspace{-0.2cm}
%\begin{center}
%\includegraphics[width=0.4\textwidth]{figure_man/plot_const_var.png} \\
%\end{center}
%\vspace{-0.2cm}
%{\small $\Rightarrow$ Generally, if model yields unbiased predictions, {\scriptsize $\E_{y~|~\xv}\left[y-\fx~|~\xv\right]=0$}, using $L2$-loss $\triangleq$ minimizing variance of model residuals}

%The same holds true for the pointwise construction / conditional distribution as considered in the slides before.

\end{vbframe}

\begin{vbframe}{L1-Loss}

% Description
\vspace*{-0.1cm}
The L1 loss is defined as
$$
\Lyf = \left|y-f\right|
$$

\begin{itemize}
\setlength{\itemsep}{1.5em}
\item More robust than $L2$, outliers in $y$ are less problematic.
\item Analytical properties: convex, not differentiable for $y = f(\bm{x})$ (optimization becomes harder).
\end{itemize}

\vspace*{0.2cm}

\begin{center}  \includegraphics[width = 9cm]{figure/loss_absolute_2.png} \\
\end{center}


\end{vbframe}


\begin{vbframe}{L1-Loss: Risk Minimizer}

We calculate the (true) risk for the $L1$-Loss $\Lyf = \left|y-f\right|$ with unrestricted $\Hspace = \{f: \Xspace \to \Yspace\}$. 

\vspace{0.5cm}

\begin{itemize}
  \setlength{\itemsep}{1.4em}  
  \item We use the law of total expectation
  \vspace{0.3cm}
  $$
    \riskf = \E_x \left[\E_{y|x}\left[|y-\fx|~|\xv = \xv\right]\right]. 
  $$
  \item As the functional form of $f$ is not restricted, we can just optimize point-wise at any point $\xv = \xv$. The best prediction at $\xv = \xv$ is then 
  \vspace{0.3cm}  
  $$
    f^{\ast}(\xv) = \argmin_c \E_{y|x}\left[|y - c|\right]= \text{med}_{y|x} \left[y ~|~ \xv \right]. 
  $$

\end{itemize}

\end{vbframe}

\begin{vbframe}{L1-Loss: Optimal constant model}

The optimal constant model in terms of the theoretical risk for the L1 loss is the median over $y$:

\begin{eqnarray*}
  f_{c}^{\ast} &=& \text{med}_{y|x} \left[y ~|~ \xv \right] \overset{\text{drop } \xv}{=}  \text{med}_{y} \left[y \right]
  \end{eqnarray*} 

The optimizer $\hat{f}_c$ of the empirical risk is $\text{med}(\yi)$ over $\yi$, which is the empirical estimate for $\text{med}_{y} \left[y \right]$. 

\vspace*{-0.3cm}

\begin{center}
\includegraphics[width = 0.5\textwidth ]{figure/l1_vs_l2.png} \\
\end{center}

\end{vbframe}


\endlecture

\end{document}