\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}


\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{
Advanced Risk Minimization
}{
Regression Losses: L2 and L1 loss
}{
figure/loss_quadratic_1.png
}{
\item L2 loss and risk minimizers
\item L1 loss and risk minimizers
}

\begin{frame}{L2-Loss}

$$
\Lxy = (y-\fx)^2 \quad \text{or} \quad \Lxy = 0.5 (y-\fx)^2
$$

\vfill

\begin{itemize}
\item Tries to reduce large residuals \\
If residual is twice as large, loss is 4 times as large\\
Hence, sensitive to outliers in $y$
\item Analytic properties: convex, differentiable 
\end{itemize}

\imageC[0.8]{figure/loss_quadratic_2.png}

\end{frame}


\begin{framei}[sep=L]{L2: Optimal value is expectation}

\item Can derive a general result now for any $z ~ \sim Q$
\item Consider 
$$ \argmin_{c \in \R}  \E_{z} [L(z, c)] = \argmin_{c \in \R}  \E [(z - c)^2] $$
$$\E [(z - c)^2] =  \E [z^2 - 2zc + c^2 ] = \E[z^2] -2c\E[z] + c^2   $$

\item The RHS is minimized by $c = \E[z]$\\
(simple quadratic, or take derivative and set to 0)

\end{framei}


\begin{framei}[sep=S]{L2: Optimal Constant Model}

\item From the previous we immediately get for $Q = P_y$
$$
f_{c}^{\ast} = \argmin_{c \in \R} \E_{y} [ (y - c)^2 ] = \E[y]
$$
\item For the best empirical constant we could minimize   
$$\hat{f}_c = \argminlim_{c \in \R} \sumin L(\yi,c)$$

And later we will proceed like that
\item But we can get the result for free from our previous consideration
  
\item For data $y^{(1)}, \ldots, y^{(n)}$, empirical distribution is $P_n = \meanin \delta_{\yi}$

\item Hence: Optimal constant is sample mean
$$\hat{f}_c = \argminlim_{c \in \R} \sumin L(\yi,c) = 
\E_{z \sim P_n} (z-c)^2 = \E[z] = \meanin \yi = \bar{y}
$$

\end{framei}

\begin{framei}[sep=S]{L2-Loss: Risk Minimizer}

\item Let's minimize true risk for unrestricted hypothesis space and $L2$

\item We know: At any point $\xv = \xtil$, our loss-optimal prediction is 

$$\fbayes(\xtil) = \argmin_{c \in \R} \E_{y|x}\left[L(y, c)~|~ \xv = \xtil \right]$$ 

\item Again from the previous, \\
we know the minimizer for $L2$ is the conditional expectation 

$$f^{\ast}(\xtil) = \E_{y|x} \left[y ~|~ \xv = \xtil \right]. $$


\imageC[0.7]{figure/optimal_pointwise.png}

\end{framei}

\begin{frame}{L2 loss means minimizing variance}

  \vfill

\splitVCC[0.55]{
  \begin{itemizeM}[fs=normal]
      \item Let's reconsider the previous
      \item Optimized for const whose squared dist to points is minimal (on avg)
      \item Result: $\hat{\theta} = \bar{y}$
      \item What is the associated risk?\\
        $\mathcal{R}(\hat{\theta}) = \sum_{i=1}^n (y_i - \bar{y})^2$
      \item Average this by $\frac{1}{n}$ or $\frac{1}{n-1}$\\
        to obtain variance
      \item Same holds for the pointwise construction / conditional distribution considered before
    \end{itemizeM}
  }{
  \imageC[]{figure_man/plot_const_var.png}
  }

\end{frame}

\begin{frame}{L1-Loss}

% Description
$$
\Lxy = \left|y-\fx\right|
$$

\begin{itemize}
\item More robust than $L2$, outliers in $y$ are less problematic
\item Analytical properties: convex, not differentiable for $y = f(\bm{x})$ (optimization becomes harder)
\end{itemize}

\vfill

\imageC[0.8]{figure/loss_absolute_2.png} 

\end{frame}


\begin{framei}[sep=L]{L1-Loss: Optimal predictions}

\item Optimal constant model is median: 
$f_{c}^{\ast} = \text{med} [y]$

\item Empirical version: $\hat{f}_c = \text{med}(y^{(1)}, \ldots, y^{(n)}) $  

\item Derivations slightly harder and in deep-dive

\item Risk minimizer / optimal conditional prediction:
$$f^{\ast}(\xtil) = \argmin_c \E_{y|x}\left[|y - c|\right]= \text{med}_{y|x} \left[y ~|~ \xv = \xtil \right]
$$

\end{framei}



\endlecture

\end{document}