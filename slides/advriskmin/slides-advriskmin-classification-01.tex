\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{
Advanced Risk Minimization
}{
Classification and 0-1-Loss
}{
figure/zero_one.png
}{
\item 0-1 loss 
\item Risk minimizer(s) / Optim. predictions
\item Bayes error rate
\item Generative approach in classification 
}

\begin{vbframe}{0-1-Loss}

\splitV{
\begin{itemize}
\item Discrete classifier $h: \Xspace \to \Yspace$
\item Maybe most ``natural'': 0-1-loss 
$$
\Lhxy = \mathds{1}_{\{y \ne \hx\}} 
$$
\end{itemize}
}
{
\begin{center}
\includegraphics[width = 0.8\textwidth]{figure/zero_one.png}
\end{center}
}

\vfill

\begin{itemize}
\item For $\Yspace \in \setmp$ and scoring classifier $\fx$\\
can write it in terms of margin $\nu = y\fx$
$$
\Lxy = \mathds{1}_{\{\nu < 0\}} = \mathds{1}_{\{y\fx < 0\}}
$$
\item For $\Yspace \in \setzo$ and prob. classifier $\pix$
$$
\Lpixy = y \mathds{1}_{\{\pix < 0.5\}} + (1-y) \mathds{1}_{\{\pix \geq 0.5\}} = \mathds{1}_{\{(2y-1)(\pix - 0.5) < 0 \}} 
$$
\item Analytical properties:  Not continuous, even for linear $\fx$ optim. problem NP-hard = close to intractable \citelink{FELDMAN2012AGNOSTIC}
\end{itemize}

\end{vbframe}


\begin{vbframe}{Risk Minimizer for discrete classifiers}

\begin{itemize}

\item Again, unravel with law of total expectation (works for multiclass)

\vspace*{-0.5cm}

\begin{eqnarray*}
  \riskf  & = & \Exy\left[\Lxy\right] = \E_x \left[ \E_{y|x} [ \Lxy ~|~ \xv ] \right] \\
          & = & \E_x \left[\sum_{k \in \Yspace} L(k, \fx) \P(y = k~|~ \xv)\right] 
          % & = & E_x \sum_{k \in \Yspace} L(k, \fx) \pikx,
\end{eqnarray*}

\item $\eta_k(x) := \P(y = k| \xv)$ is true posterior probability for class $k$

\item For binary case, we denote $\eta(\xv) := \P(y = 1 ~|~ \xv)$ and get: 
$$
\riskf = \E_x \left[L(1, \pix) \cdot \eta(\xv) + L(0, \pix) \cdot (1 - \eta(\xv)) \right]
$$

\item Above formulas work for any loss, not only 0-1; and hard labelers, scorers or prob. classifiers

\item Especially for hard labelers and arbitrary misclassif. costs, we see: produces cost-optimal decision, weighted by posterior probs; \\
(we see this again in cost-senslearning)

% The risk minimizer for a general loss function $\Lxy$ is

% \vspace*{-0.3cm}

% \begin{eqnarray*}
%   \fxbayes &=& \argmin_{f: \Xspace \to \R^g} \E_x \left[\sum_{k \in \Yspace} L(k, f(\bm{x})) \P(y = k| \xv = \xtil)\right]\,.  \\
% \end{eqnarray*}


\end{itemize}

\end{vbframe}


\begin{vbframe}{0-1-loss: Optimal predictions}

\begin{itemize}

\item For multiclass and hard labeler $\hx$
\item Optimal constant
\begin{eqnarray*}   h_{c}^{\ast}  &=& \argmin_{k \in \Yspace} \sum_{l \in \Yspace} L(l, k) \cdot \P(y = l) \\
&=& \argmin_{k \in \Yspace} \sum_{k \ne l} \P(y = l) \\ 
  &=& \argmin_{k \in \Yspace} 1 - \P(y = k) \\
  &=& \argmax_{k \in \Yspace} \P(y = k)
  \end{eqnarray*}


\item Translation: Predict most probable class

\item Empirical version: $\hat{h}_c = \argmaxlim_{k \in \Yspace} \pikh$

\item Risk minimizer / optim. cond. prediction / Bayes optim. classifier:

 $h^{\ast}(\xtil) = \argmaxlim_{k \in \Yspace} \P(y = k~|~ \xv = \xtil)$ 

\end{itemize}

\end{vbframe}


\begin{vbframe}{Bayes risk  / Bayes error rate}

$$
  \riskbayes = 1 - \E_{x} \left[\max_{k \in \Yspace} \P(y = k~|~ \xv)\right]
$$

\vfill

For binary case, can write risk minimizer and Bayes risk as:  

$$
  \hxbayes = \begin{cases} 1 & \eta(\xv) \ge \frac{1}{2} \\ 0 & \eta(\xv) < \frac{1}{2}\end{cases} 
$$

$$
\riskbayes = \E_x\left[\min(\eta(\xv), 1 - \eta(\xv))\right] = 1 - \E_x\left[\max(\eta(\xv), 1 - \eta(\xv))\right] 
$$

 
\end{vbframe}


\begin{vbframe}{Generative classifiers}

\begin{itemize}

\item So, $\argmaxlim_{k \in \Yspace} \P(y = k~|~ \xv = \xtil)$ is what we want to do

\item Assume we can model densities given classes and use Bayes:

$$
\P(y = k~|~ \xv = \xtil)  = \frac{p(\xtil | y = k) \P(y = k)} {p(\xtil)} 
$$

\item Then
$$
\argmax_{k \in \Yspace} \P(y = k~|~ \xv = \xtil) = \argmax_{k \in \Yspace}  p(\xtil | y = k) \P(y = k)
$$
\item Then we can estimate these conditional densities and the prior probs, and classify via them

\item This idea we will see in so-called ``generative approaches'' for classification, so in LDA, QDA, etc.


\end{itemize}

\end{vbframe}


\begin{vbframe}{Example}

\begin{itemize}

\item  Assume $\P(y = 1) = \frac{1}{2}$ 
\item And conditional densities of $x$ per class as normal $
\begin{cases}
\phi_{\mu_1, \sigma^2}(x) & \text{for } y = 0\\ 
\phi_{\mu_2, \sigma^2}(x) & \text{for } y = 1
\end{cases}$

\item Bayes optimal classifier = orange; Bayes error = red 

\end{itemize}


\begin{center}
\includegraphics[width = 0.8\textwidth]{figure/bayes_error_5.png} 
\end{center}


\end{vbframe}

\endlecture

\end{document}
