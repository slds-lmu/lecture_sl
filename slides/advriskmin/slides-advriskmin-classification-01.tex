\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Advanced Risk Minimization
  }{% Lecture title  
    0-1-Loss
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/plot_loss_01.png
  }{
  \item Derive the risk minimizer of the 0-1-loss
  \item Derive the optimal constant model for the 0-1-loss
}

\begin{vbframe}{0-1-Loss}

\begin{itemize}
  \item Let us first consider a discrete classifier $\hx: \Xspace \to \Yspace$. 
  \item The most natural choice for $\Lhxy$ is the 0-1-loss 
  \vspace*{-0.2cm}
  $$
  \Lhxy = \mathds{1}_{\{y \ne \hx\}} =
     \footnotesize \begin{cases} 1 \quad \text{ if } y \ne \hx \\ 0 \quad    \text{ if } y = \hx  \end{cases}
  $$
  \item For the binary case ($g = 2$) we can express the 0-1-loss for a scoring classifier $\fx$ based on the margin $\nu := y\fx$
  \vspace*{-0.2cm}
  $$
  \Lxy = \mathds{1}_{\{\nu < 0\}} = \mathds{1}_{\{y\fx < 0\}}. 
  $$
  % and for a probabilistic classifier $\Lxy = \mathds{1}_{\{\pix \ge 0.5 \}}$. 
  \item Analytic properties:  Not continuous, even for linear $f$ the optimization problem is NP-hard and close to intractable.
\end{itemize}

\begin{center}
% \includegraphics[width = 11cm ]{figure_man/0-1-loss.png} \\
\includegraphics[width = 0.5\textwidth]{figure/plot_loss_01.png}
\end{center}

\end{vbframe}


\begin{vbframe}{0-1-loss: Risk Minimizer}

By the law of total expection we can in general rewrite the risk as\\
(this all works for the multiclass case with 0-1)

\vspace*{-0.5cm}

\begin{eqnarray*}
  \riskf  & = & \Exy\left[\Lxy\right] = \E_x \left[ \E_{y|x} [ L(y, \fx) ] \right] \\
          & = & \E_x \left[\sum_{k \in \Yspace} L(k, \fx) \P(y = k~|~ \xv)\right]\,, 
          % & = & E_x \sum_{k \in \Yspace} L(k, \fx) \pikx,
\end{eqnarray*}

with $\P(y = k| \xv)$ the posterior probability for class $k$. For the binary case we denote $\eta(\xv) := \P(y = 1 ~|~ \xv)$ and the expression becomes 

\vspace*{-0.3cm}


\begin{eqnarray*}
  \riskf &=& \E_x \left[L(1, \pix) \cdot \eta(\xv) + L(0, \pix) \cdot (1 - \eta(\xv)) \right]. 
\end{eqnarray*}

% The risk minimizer for a general loss function $\Lxy$ is

% \vspace*{-0.3cm}

% \begin{eqnarray*}
%   \fxbayes &=& \argmin_{f: \Xspace \to \R^g} \E_x \left[\sum_{k \in \Yspace} L(k, f(\bm{x})) \P(y = k| \xv = \xv)\right]\,.  \\
% \end{eqnarray*}




\framebreak 

We compute the point-wise optimizer of the above term for the 0-1-loss (defined on a discrete classifier $\hx$): 

  \begin{eqnarray*}  
  \hxbayes &=& \argmin_{l \in \Yspace} \sum_{k \in \Yspace} L(k, l) \cdot \P(y = k~|~\xv = \xv) \\
  &=& \argmin_{l \in \Yspace} \sum_{k \ne l} \P(y = k~|~\xv = \xv) \\ 
  &=& \argmin_{l \in \Yspace} 1 - \P(y = l~|~\xv = \xv) \\
  &=& \argmax_{l \in \Yspace} \P(y = l~|~ \xv = \xv),
  \end{eqnarray*}

which corresponds to predicting the most probable class. 

\vspace*{0.2cm} 

Note that sometimes $\hxbayes = \argmax_{l \in \Yspace} \P(y = l~|~ \xv = \xv)$ is referred to as the \textbf{Bayes optimal classifier} (without closer specification of the the loss function used). 

\lz 

The Bayes risk for the 0-1-loss (also: Bayes error rate) is 

\begin{eqnarray*}  
  \riskbayes &=& 1 - \E_{x} \left[\max_{l \in \Yspace} \P(y = l~|~ \xv)\right]\,.
\end{eqnarray*}

\lz 

In the binary case ($g = 2$) we can write risk minimizer and Bayes risk as follows:  

\begin{eqnarray*}
  \hxbayes &=& \begin{cases} 1 & \eta(\xv) \ge \frac{1}{2} \\ 0 & \eta(\xv) < \frac{1}{2}\end{cases} \\
\end{eqnarray*}

\vspace*{-0.7cm}

\begin{eqnarray*}
  \riskbayes &=& \E_x\left[\min(\eta(\xv), 1 - \eta(\xv))\right] = 1 - \E_x\left[\max(\eta(\xv), 1 - \eta(\xv))\right] . 
\end{eqnarray*}


% \framebreak 


% If we can estimate $\Pxy$ very well via $\pikx$ through a stochastic model, we can compute the loss-optimal classifications point-wise. 

% \lz

% \textbf{Example}: Assume that our data is generated by a Mixture of Gaussian distributions. 

% \begin{center}
% \includegraphics[width = 9cm ]{figure_man/bayes_error_1.png} \\
% \end{center}

% \framebreak 

% We could try to approximate the $\P(y = k ~|~ \xv = \xv)$ via a stochastic model $\pix$ (shown as contour lines): 

% \begin{center}
% \includegraphics[width = 9cm ]{figure_man/bayes_error_2.png} \\
% \end{center}

% For each new $\xv$, we estimate the class probabilities directly with the stochastic model $\pix$, and our best point-wise prediction is 

% \begin{eqnarray*}
%   \fxh &=& \argmin_{f \in \Hspace} \sum_{k \in \Yspace} L(k, f(\bm{x})) \pix\,.  \\
% \end{eqnarray*}


% \lz 

% But usually we directly adapt to the loss via \textbf{empirical risk minimization}. 

% $$
% \fh = \argmin_{f \in \Hspace} \riske(f) = \argmin_{f \in \Hspace} \sumin \Lxyi.
% $$
  

\framebreak 

\textbf{Example: } Assume that $\P(y = 1) = \frac{1}{2}$ and $
\P(x ~|~ y) = \begin{cases}
\phi_{\mu_1, \sigma^2}(x) & \text{for } y = 0\\ 
\phi_{\mu_2, \sigma^2}(x) & \text{for } y = 1
\end{cases}$

\vfill


The decision boundary of the Bayes optimal classifier is shown in orange and the Bayes error rate is highlighted as red area. 

\vfill

\begin{center}
\includegraphics[width = 0.8\textwidth]{figure/bayes_error_5.png} \\
\end{center}

% \framebreak 
% 
% The Bayes error rate is highlighted as red area. 
% 
% \begin{center}
% \includegraphics[width = 9cm ]{figure_man/bayes_error_5.png} \\
% \end{center}

\end{vbframe}

\endlecture

\end{document}
