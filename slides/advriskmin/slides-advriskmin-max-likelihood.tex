\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}


\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{
Advanced Risk Minimization
}{
Maximum Likelihood vs. ERM
}{
figure/residuals_plot_L2_title.png
}{
\item Max. lik. and ERM are the same
\item Gaussian errors = L2 loss
\item Laplace errors = L1 loss
\item Bernoulli targets vs. log loss
}

\begin{framei}[sep=M]{Maximum Likelihood}

\item Regression from a maximum likelihood perspective
\item Assume data comes from $\Pxy$ 
\item Conditional perspective: 
$$
y~|~ \xv \sim p(y~|~\xv, \thetav)
$$
\item Common case: true underlying relationship $\ftrue$ with additive noise (surface plus noise model): 

\splitVCC{
$$
y = \ftrue(\xv) + \eps
$$
}
{
\imageL[0.85]{figure/ftrue.pdf}
}

\item $\ftrue$ has params $\thetav$ and $\eps \sim\P_\eps$, with $\E[\eps] = 0, \epsilon \perp \!\!\! \perp \xv$
\item We now want to learn $\ftrue$ (or its params)

\end{framei}


\begin{framei}[sep=M]{Maximum Likelihood}

\item Given i.i.d data $\D = \Dset$ from $\Pxy$ 
\item Max. likelihood maximizes \textbf{likelihood} of data under params

\splitVCC{
$$ \LLt = \prod_{i=1}^n \pdfyigxit $$
}
{
\imageL[0.9]{figure/log_reg_ml.pdf}
}

\item Equivalent: minimize \textbf{negative log-likelihood (NLL)}

\splitVCC{
$$ -\loglt = -\sumin \lpdfyigxit $$
}
{
\imageL[0.9]{figure/log_reg_erm.pdf}
}

\end{framei}


\begin{framei}[sep=M]{Risk Minimization} 

\item In ML / ERM: instead of conditional distribution, pick a loss 
\item Our admissible functions $\fx$ come from hypothesis space $\Hspace$
\item But in stats, must assume some form of $\ftrue$, no difference
\item Simply define neg. log-likelihood as \textbf{loss function} 
$$ \Lxyt := - \lpdfygxt $$
\item Then, maximum-likelihood 
 = ERM
$$- \loglt = \risket = \sumin \Lxyit$$
\item NB: When only interested in minimizer, we use $\propto$ as \enquote{proportional up to pos. multiplicative and general additive constants}

\end{framei}


\begin{framei}[sep=M]{Gaussian Errors - L2-Loss} 

\item Assume $y = \ftrue(\xv) + \eps$ with $\eps \sim \normal(0, \sigma^2)$ 
\item Then $y~|~\xv \sim \normal(\ftrue(\xv), \sigma^2)$ and likelihood is 
{\small
\begin{align*}
\LL(\thetav) =& \prodin \pdf(\yi ~\bigg|~ \fxit, \sigma^2) \\ 
\propto& \prodin \exp(-\frac{1}{2\sigma^2} (\yi - \fxit)^2)
\end{align*}
}
\item Minimizing Gaussian NLL is ERM with $L2$-loss
\begin{align*}
- \loglt =& - \log(\LL(\thetav)) \\
\propto& - \log(\prodin \exp(-\frac{1}{2\sigma^2} (\yi - \fxit)^2)) \\
\propto& \sumin (\yi - \fxit)^2
\end{align*}

\end{framei}


\begin{framei}[sep=M]{Gaussian Errors - L2-Loss} 

\item Simulate data $y ~|~x \sim \mathcal{N}(\ftrue(x), 1)$ with $\ftrue = 0.2 \cdot x$ 
\item Plot residuals as histogram, after fitting LM with $L2$-loss (blue)
\item Compare emp. residuals vs. theor. quantiles via Q-Q-plot

\vfill

\imageC[0.8]{figure/residuals_plot_L2.pdf}

\vfill

\item Residuals are approximately Gaussian!

\end{framei}

\begin{framei}[sep=M]{Laplace Errors - L1-Loss}

\item Consider Laplacian errors $\eps$, with density 

\vfill

\splitVCC[0.4]{
$$
\frac{1}{2\sigma} \exp(-\frac{|\eps|}{\sigma})\,, \sigma > 0
$$
}{
{\small $\qquad$ Laplace density, $x \sim \text{LP}(0,1)$}
\imageL[0.9]{figure/laplace-plot.png}
}
\item Then
$$
y = \ftrue(\xv) + \eps 
$$
also follows Laplace distribution with mean $\fxit$ and scale  $\sigma$ 

\end{framei}


\begin{framei}[sep=M]{Laplace Errors - L1-Loss}

\item The likelihood is then 
\begin{align*}
\LL(\thetav) =& \prod_{i=1}^n \pdf(\yi ~\bigg|~ \fxit, \sigma) \\ 
\propto& \exp(-\frac{1}{\sigma}\sumin \left|\yi - \fxit\right|)\,
\end{align*}
\item The negative log-likelihood is
$$
- \loglt \propto \sumin \left|\yi - \fxit\right|
$$
\item MLE for Laplacian errors = ERM with L1-loss 
\item Some losses correspond to more complex or less known error densities, like the Huber loss \furtherreading{MEYER2021ALTERNATIVE}
\item Huber density is (unsurprisingly) a hybrid of Gaussian and Laplace

\end{framei}
\begin{framei}[sep=M]{Laplace Errors - L1-Loss}

\item Same setup, now with $y ~|~x \sim \text{LP}(\ftrue(x), 1)$ 
\item Now fit LM with L1 loss

\vfill

\imageC{figure/residuals_plot_L1.pdf}

\item Again, residuals approximately match quantiles!

\end{framei}


\begin{framei}[sep=M]{Maximum Likelihood in Classification}

\item Now binary classification
\item $y \in \setzo$ is Bernoulli, $y ~|~ \xv \sim \text{Bern}(\pi_\text{true}(\xv))$
\item  NLL:
\begin{align*}
- \loglt =& -\sumin \lpdfyigxit \\ 
=& - \sumin \log \big[\pi(\xi)^{y^{(i)}} \cdot (1 - \pi(\xi))^{(1 - y^{(i)})} \big]\\
=& \sumin -\yi \log[\pi(\xi)] - (1-\yi) \log [1 - \pi(\xi)]
\end{align*}
\item Results in Bernoulli / log loss:
$$
\Lpixy = -y\log(\pix)-(1-y)\log(1-\pix)
$$

\end{framei}


\begin{framei}[sep=L]{Distributions and losses}

\item For \textbf{every} error distribution $\P_\eps$, can derive an equivalent loss
\item Leads to same point estimator for $\thetav$ as maximum-likelihood:

$$
\thetah \in \argmax_{\thetav} \LL(\thetav) \Leftrightarrow \thetah \in \argmin_{\thetav}-\log(\LL(\thetav))
$$
    
\item But \textbf{cannot} derive a pdf/error distrib. for every loss, e.g., Hinge loss; some prob. interpretation still possible \furtherreading{SOLLICH1999NINTH}

\item For dist.-based loss on residual $\Lxy=L_{\mathbb{P}}(r)$, ERM is fully equiv. to max. conditional log-likelihood $\log(p(r))$ if
\begin{enumerate}
\item $\log(p(r))$ is affine trafo of $L_{\mathbb{P}}$ (undoing the $\propto$):
$\log(p(r)) = a - bL_{\mathbb{P}}(r),\,\,a \in \mathbb{R}, b>0$
\item  $p$ is a pdf (non-negative and integrates to one)
\end{enumerate}

\end{framei}


\endlecture
\end{document}
