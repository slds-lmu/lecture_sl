\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{
Advanced Risk Minimization
}{
Maximum Likelihood vs. ERM
}{
figure/residuals_plot_L2_title.png
}{
\item Max. lik. and ERM are the same
\item Gaussian errors = L2 loss
\item Laplace errors = L1 loss
\item Bernoulli targets vs. log loss
}

\begin{vbframe}{Maximum Likelihood}

\begin{itemize}
\item Regression from a maximum likelihood perspective
\item Assume data comes from $\Pxy$ 
\item Conditional perspective: 

$$
	y~|~ \xv \sim p(y~|~\xv, \thetav)
$$

\item
Common case:
true underlying relationship $\ftrue$ with additive noise (surface plus noise model): 

\begin{minipage}{0.5\textwidth}
$$
y = \ftrue(\xv) + \eps
$$
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\includegraphics[width = 0.75\textwidth]{figure/ftrue.pdf}
\end{minipage}

\item $\ftrue$ has params $\thetav$ and $\eps \sim\P_\eps$, with $\E[\eps] = 0, \epsilon \perp \!\!\! \perp \xv$

\item We now want to learn $\ftrue$ (or its params)

\end{itemize}


\framebreak 

\begin{itemize}
\item Given i.i.d data $\D = \Dset$ from $\Pxy$ 
\item Max. likelihood maximizes \textbf{likelihood} of data under params

\begin{minipage}{0.5\textwidth}
$$ \LLt = \prod_{i=1}^n \pdfyigxit $$
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\includegraphics[width = 0.75\textwidth]{figure/log_reg_ml.pdf}
\end{minipage}

\item Equivalent: minimize \textbf{negative log-likelihood (NLL)}

\begin{minipage}{0.5\textwidth}
$$ -\loglt = -\sumin \lpdfyigxit. $$
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\includegraphics[width = 0.75\textwidth]{figure/log_reg_erm.pdf}
\end{minipage}
\end{itemize}

\end{vbframe}


\begin{vbframe}{Risk Minimization} 

\begin{itemize}
\item In ML / ERM: instead of conditional distribution, pick a loss 
\item Our admissible functions come from hypothesis space 
\item But in stats, must assume some form of $\ftrue$, no difference

\item Simply define neg. log-likelihood as \textbf{loss function} 
$$ \Lxyt := - \lpdfygxt $$
\item Then, maximum-likelihood 
 = ERM
$$- \loglt = \risket = \sumin \Lxyit$$

\item NB: When only interested in minimizer, we use $\propto$ as \enquote{proportional up to pos. multiplicative and general additive constants}

\end{itemize}

\end{vbframe}


\begin{vbframe}{Gaussian Errors - L2-Loss} 

\begin{itemize}

\item Assume $y = \ftrue(\xv) + \eps$ with $\eps \sim \normal(0, \sigma^2)$ 
\item Then $y~|~\xv \sim \normal\left(\ftrue(\xv), \sigma^2\right)$ and likelihood is 
{\small
\begin{eqnarray*}
\LL(\thetav) &=& \prodin \pdf\left(\yi ~\bigg|~ \fxit, \sigma^2\right) \\ &\propto& \prodin \exp\left(-\frac{1}{2\sigma^2} \left(\yi - \fxit\right)^2\right)
\end{eqnarray*}
}

%\framebreak 

\item Minimizing Gaussian NLL is ERM with $L2$-loss
{\small
\begin{eqnarray*}
- \loglt &=& - \log\left(\LL(\thetav)\right) \\
&\propto& - \log\left(\prodin \exp\left(-\frac{1}{2\sigma^2} \left(\yi - \fxit\right)^2\right)\right) \\
&\propto& \sumin \left(\yi - \fxit\right)^2
\end{eqnarray*}
}

\end{itemize}
\framebreak 

\begin{itemize}
	\item Simulate data $y ~|~x \sim \mathcal{N}\left(\ftrue(x), 1\right)$ with $\ftrue = 0.2 \cdot x$ 
\item Plot residuals as histogram, after fitting LM with $L2$-loss (blue)
\item Compare emp. residuals vs. theor. quantiles via Q-Q-plot

\begin{center}
\includegraphics[width = 0.8\textwidth]{figure/residuals_plot_L2.pdf}
\end{center}

\item Residuals are approximately Gaussian!
\end{itemize}

\end{vbframe}

\begin{vbframe}{Laplace Errors - L1-Loss}

\begin{itemize}
    \item Consider Laplacian errors $\eps$, with density 
\end{itemize}
\lz

\splitVCC[0.5]{
%\begin{minipage}{0.5\textwidth}
$$
 \frac{1}{2\sigma} \exp\left(-\frac{|\eps|}{\sigma}\right)\,, \sigma > 0
$$
}{
%\end{minipage}%
%\begin{minipage}{0.5\textwidth}
\small{$\qquad$ Laplace density, $x \sim \text{LP}(0,1)$}
\includegraphics[width = \textwidth]{figure/laplace-plot.png}
%\end{minipage}
}
\begin{itemize}
\item Then
$$
y = \ftrue(\xv) + \eps 
$$
also follows Laplace distribution with mean $\fxit$ and scale  $\sigma$ 
\end{itemize}
\framebreak 

\begin{itemize}
    \item The likelihood is then 
\begin{eqnarray*}
\LL(\thetav) &=& \prod_{i=1}^n \pdf\left(\yi ~\bigg|~ \fxit, \sigma\right) \\ &\propto& \exp\left(-\frac{1}{\sigma}\sumin \left|\yi - \fxit\right|\right)\,
\end{eqnarray*}
\item The negative log-likelihood is
$$
- \loglt \propto \sumin \left|\yi - \fxit\right|
$$

\item MLE for Laplacian errors = ERM with L1-loss 

\item Some losses correspond to more complex or less known error densities, like the Huber loss \citelink{MEYER2021ALTERNATIVE}

\item Huber density is (unsurprisingly) a hybrid of Gaussian and Laplace

\end{itemize}

\framebreak 

\begin{itemize}
	\item Same setup, now with $y ~|~x \sim \text{LP}\left(\ftrue(x), 1\right)$ 
\item Now fit LM with L1 loss

\vfill

\includegraphics{figure/residuals_plot_L1.pdf}


\item Again, residuals approximately match quantiles!


\end{itemize}


\end{vbframe}



\begin{vbframe}{Maximum Likelihood in Classification}


\begin{itemize}

\item Now binary classification
\item $y \in \setzo$ is Bernoulli, $y ~|~ \xv \sim \text{Bern}(\pi_\text{true}(\xv))$
\item  NLL:
\begin{eqnarray*}
- \loglt &=& -\sumin \lpdfyigxit \\ 
&=& - \sumin \log \big[\pi(\xi)^{y^{(i)}} \cdot (1 - \pi(\xi))^{(1 - y^{(i)})} \big]\\
&=& \sumin -\yi \log[\pi(\xi)] - (1-\yi) \log [1 - \pi(\xi)]
\end{eqnarray*}
\item Results in Bernoulli / log loss:
$$
  L(y, \pix) = -y\log(\pix)-(1-y)\log(1-\pix)
$$


\end{itemize}

\end{vbframe}




\begin{vbframe}{Distributions and losses}

\begin{itemizeL}

\item For \textbf{every} error distribution $\P_\eps$, can derive an equivalent loss
\item Leads to same point estimator for $\thetav$ as maximum-likelihood:

$$
\thetah \in \argmax_{\thetav} \LL(\thetav) \Leftrightarrow \thetah \in \argmin_{\thetav}-\log(\LL(\thetav))
$$
    
\item But \textbf{cannot} derive a pdf/error distrib. for every loss, e.g., Hinge loss; some prob. interpretation still possible \citelink{SOLLICH1999NINTH}
%\framebreak

\item For dist.-based loss on residual $L(y,\fx)=L_{\mathbb{P}}(r)$, ERM is fully equiv. to max. conditional log-likelihood $\log(p(r))$ if
\begin{enumerate}
    \item $\log(p(r))$ is affine trafo of $L_{\mathbb{P}}$ (undoing the $\propto$):
    $\log(p(r)) = a - bL_{\mathbb{P}}(r),\,\,a \in \mathbb{R}, b>0$
    \item  $p$ is a pdf (non-negative and integrates to one)
\end{enumerate}


\end{itemizeL}


\end{vbframe}


\endlecture
\end{document}
