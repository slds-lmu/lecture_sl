\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-eval}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{
    Advanced Risk Minimization
  }{% Lecture title  
    Bias-Variance Decomposition (Deep-Dive)
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/bias_variance_decomposition-linear_model_bias.png
  }{
  \item Understand how to decompose the generalization error of a learner into 
  \begin{itemize}
    \item \footnotesize Bias of the learner
    \item \footnotesize Variance of the learner
    \item \footnotesize Inherent noise in the data
  \end{itemize} 
}

\begin{vbframe} {Bias-Variance decomposition}

Let us take a closer look at the generalization error of a learning algorithm $\ind$.
This is the expected error of an induced model $\fh_{\D_n}$, on training sets of size $n$, when applied to a fresh, random test observation.
  $$GE_n\left(\ind\right) = \E_{\D_n \sim \Pxy^n, (\xv, y) \sim \Pxy}\left( L\left(y, \fh_{\D_n}(\xv)\right) \right) = \E_{\D_n, xy}\left( L\left(y, \fh_{\D_n}(\xv)\right) \right)  $$
We therefore need to take the expectation over all training sets of size $n$, as well as the independent test observation.

\lz 

We assume that the data is generated by 
$$
y = \ftrue(\xv) + \epsilon
$$
with zero-mean homoskedastic error $\epsilon \sim (0, \sigma^2)$ independent of $\xv$.  

\framebreak 

By plugging in the $L2$ loss $L(y, f(\xv)) = (y - f(\xv))^2$ we get

\begin{footnotesize}
\begin{eqnarray*}
GE_n\left(\ind\right) &=& \E_{\D_n, xy}\left( L\left(y, \fh_{\D_n}(\xv)\right)\right) = \E_{\D_n, xy}\left(\left(y - \fh_{\D_n}(\xv)\right)^2\right) \\
&\stackrel{\text{LIE}}{=}& \E_{xy}\Big[\underbrace{\E_{\D_n}\left(\left(y - \fh_{\D_n}(\xv)\right)^2~|~ \xv, y\right)}_{(*)}\Big] 
\end{eqnarray*}
\end{footnotesize}

Let us consider the error $(*)$ conditioned on one fixed test observation $(\xv, y)$ first. (We omit the $~|~\xv, y$ for better readability for now.)

\begin{footnotesize}
\begin{eqnarray*}
(*) &=& \E_{\D_n}\left(\left(y - \fh_{\D_n}(\xv)\right)^2\right)\\
&=& \underbrace{\E_{\D_n}\left(y^2\right)}_{= y^2} + \underbrace{\E_{\D_n}\left(\fh_{\D_n}(\xv)^2\right)}_{(1)}  - 2\underbrace{\E_{\D_n}\left(y\fh_{\D_n}(\xv)\right)}_{(2)} 
% &=& y^2 + \underbrace{\E_{xy}^2\left(y\right)}_{(1)} + \underbrace{\E_{\D_n, xy}\left(\fh_{\D_n}(\xv)^2\right)}_{(2)}  - 2\underbrace{\E_{\D_n, xy}\left(y\fh_{\D_n}(\xv)\right)}_{(3)} \\
\end{eqnarray*}
\end{footnotesize}

by using the linearity of the expectation.  %and $\var(y) = \E(y^2) - \E^2(y)$. 

\framebreak
\begin{footnotesize}
$$
(*) = \E_{\D_n}\left(\left(y - \fh_{\D_n}(\xv)\right)^2\right) = 
y^2 + \underbrace{\E_{\D_n}\left(\fh_{\D_n}(\xv)^2\right)}_{(1)}  - 2\underbrace{\E_{\D_n}\left(y\fh_{\D_n}(\xv)\right)}_{(2)} =
$$
\end{footnotesize}
\vspace{0.2cm}
\begin{footnotesize}
Using that $\E(z^2) =\var(z) + \E^2(z)$, we see that
% \begin{eqnarray*}
$$
= y^2 + \var_{\D_n}\left(\fh_{\D_n}(\xv)\right) + \E^2_{\D_n}\left(\fh_{\D_n}(\xv)\right) - 2y \E_{\D_n}\left(\fh_{\D_n}(\xv))\right) 
$$
\vspace{0.2cm}
Plug in the definition of $y$
$$
 = \ftrue(\xv)^2 +2\epsilon\ftrue(\xv) + \epsilon^2 + \var_{\D_n}\left(\fh_{\D_n}(\xv)\right) + \E^2_{\D_n}\left(\fh_{\D_n}(\xv)\right) - 2 (\ftrue(\xv)+\epsilon)\E_{\D_n}\left(\fh_{\D_n}(\xv))\right) 
$$
\vspace{0.25cm}
Reorder terms and use the binomial formula
$$
= \epsilon^2 + \text{Var}_{\D_n}\left(\fh_{\D_n}(\xv)\right) + 
  \left(\ftrue(\xv) - \E_{\D_n}\left(\fh_{\D_n}(\xv)\right)\right)^2
 +2 \epsilon \left(\ftrue(\xv) - \E_{\D_n}\left(\fh_{\D_n}(\xv)\right)\right)
$$
% \end{eqnarray*}
\end{footnotesize}


\framebreak 

\begin{footnotesize}
$$
(*) = \epsilon^2 + \text{Var}_{\D_n}\left(\fh_{\D_n}(\xv)\right) + 
  \left(\ftrue(\xv) - \E_{\D_n}\left(\fh_{\D_n}(\xv)\right)\right)^2
 +2 \epsilon \left(\ftrue(\xv) - \E_{\D_n}\left(\fh_{\D_n}(\xv)\right)\right)
$$
\end{footnotesize}
\vspace{0.2cm}
Let us come back to the generalization error by taking the expectation over all fresh test observations $(\xv, y) \sim \Pxy$: 
\vspace{0.8cm}
\begin{footnotesize}
\begin{eqnarray*}
GE_n\left(\ind\right) 
% &=& \var_{xy}(y) + \E_{xy}^2\left(\ftrue(\xv)\right) + \E_{xy}\left[\var_{\D_n}\left(\fh_{\D_n}(\xv) ~|~\xv, y\right)\right]  \\ &+& \E_{xy}\left[\E^2_{\D_n}\left(\fh_{\D_n}(\xv) ~|~\xv, y\right)\right]  - 2\E_{xy}\left[\E_{\D_n}\left(\ftrue(\xv)~|~\xv, y\right) \E_{\D_n}\left(\fh_{\D_n}(\xv)~|~\xv, y\right)\right] \\
% &=& \underbrace{\E_{xy}(y^2 - \ftrue(\xv)^2~|~\xv, y)}_{= \sigma^2} + \E_{xy}\left[\var_{\D_n}\left(\fh_{\D_n}(\xv) ~|~\xv, y\right)\right] \\ &+& \E_{xy}\left[\E^2_{\D_n}\left(\ftrue(\xv)-\fh_{\D_n}(\xv)~|~\xv, y\right)\right] \\
  &=& \underbrace{\sigma^2}_{\text{Variance of the data}} + \E_{xy}\underbrace{\left[\var_{\D_n}\left(\fh_{\D_n}(\xv) ~|~\xv, y\right)\right]}_{\text{Variance of learner at } (\xv, y)} \\ &+& \E_{xy}\underbrace{\left[\left(\left(\ftrue(\xv)-\E_{\D_n}\left(\fh_{\D_n}(\xv)\right)\right)^2~|~\xv, y\right)\right]}_{\text{Squared bias of learner at } (\xv, y)} + \underbrace{0}_{\text{As $\epsilon$ is zero-mean and independent}}
\end{eqnarray*}
\end{footnotesize}



\end{vbframe}



\endlecture
\end{document}


