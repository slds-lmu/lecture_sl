\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-eval}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{
Advanced Risk Minimization
}{% Lecture title  
Bias-Variance Decomposition (Deep-Dive)
}{% Relative path to title page image: Can be empty but must not start with slides/
figure/bias_variance_decomposition-linear_model_bias.png
}{
\item Understand how to decompose the generalization error of a learner under L2 loss into 
\begin{itemize}
\item \footnotesize Bias of the learner
\item \footnotesize Variance
\item \footnotesize Inherent noise in the data
\end{itemize} 
}

\begin{frame}{Bias-Variance decomposition}

Generalization error of learner  $\ind$: 
Expected error of model $\fh_{\D_n}$, on training sets of size $n$, evaluated on a fresh, random test sample.

$$GE_n(\ind) = \E_{\D_n \sim \Pxy^n, (\xv, y) \sim \Pxy}( L(y, \fh_{\D_n}(\xv)) ) = \E_{\D_n, xy}( L(y, \fh_{\D_n}(\xv)) )  $$

\vfill 

  Expectation is taken over all training sets \textbf{and} independent test sample.\\

\vfill 

We assume that the data is generated by 
$$
y = \ftrue(\xv) + \epsilon
$$
with zero-mean homoskedastic error $\epsilon \sim (0, \sigma^2)$ independent of $\xv$.  

\end{frame}

\begin{frame}{Bias-Variance decomposition}

By plugging in the $L2$ loss $L(y, f(\xv)) = (y - f(\xv))^2$ we get

{\footnotesize
\begin{eqnarray*}
GE_n(\ind) &=& \E_{\D_n, xy}( L(y, \fh_{\D_n}(\xv))) = \E_{\D_n, xy}((y - \fh_{\D_n}(\xv))^2) \\
&\stackrel{\text{LIE}}{=}& \E_{xy}\Big[\underbrace{\E_{\D_n}((y - \fh_{\D_n}(\xv))^2~|~ \xv, y)}_{(*)}\Big] 
\end{eqnarray*}
}

Let us consider the error $(*)$ conditioned on one fixed test observation $(\xv, y)$ first. (We omit the $~|~\xv, y$ for better readability for now.)

{\footnotesize
\begin{eqnarray*}
(*) &=& \E_{\D_n}((y - \fh_{\D_n}(\xv))^2)\\
&=& \underbrace{\E_{\D_n}(y^2)}_{= y^2} + \underbrace{\E_{\D_n}(\fh_{\D_n}(\xv)^2)}_{(1)}  - 2\underbrace{\E_{\D_n}(y\fh_{\D_n}(\xv))}_{(2)} 
% &=& y^2 + \underbrace{\E_{xy}^2(y)}_{(1)} + \underbrace{\E_{\D_n, xy}(\fh_{\D_n}(\xv)^2)}_{(2)}  - 2\underbrace{\E_{\D_n, xy}(y\fh_{\D_n}(\xv))}_{(3)} \\
\end{eqnarray*}
}

by using the linearity of the expectation.  %and $\var(y) = \E(y^2) - \E^2(y)$. 

\end{frame}

\begin{frame2}[footnotesize]{Bias-Variance decomposition}

$$
(*) = \E_{\D_n}((y - \fh_{\D_n}(\xv))^2) = 
y^2 + \underbrace{\E_{\D_n}(\fh_{\D_n}(\xv)^2)}_{(1)}  - 2\underbrace{\E_{\D_n}(y\fh_{\D_n}(\xv))}_{(2)} =
$$

\vfill

Using that $\E(z^2) =\var(z) + \E^2(z)$, we see that
% \begin{eqnarray*}
$$
= y^2 + \var_{\D_n}(\fh_{\D_n}(\xv)) + \E^2_{\D_n}(\fh_{\D_n}(\xv)) - 2y \E_{\D_n}(\fh_{\D_n}(\xv))) 
$$

\vfill

Plug in the definition of $y$
$$
 = \ftrue(\xv)^2 +2\epsilon\ftrue(\xv) + \epsilon^2 + \var_{\D_n}(\fh_{\D_n}(\xv)) + \E^2_{\D_n}(\fh_{\D_n}(\xv)) - 2 (\ftrue(\xv)+\epsilon)\E_{\D_n}(\fh_{\D_n}(\xv))) 
$$

\vfill

Reorder terms and use the binomial formula
$$
= \epsilon^2 + \text{Var}_{\D_n}(\fh_{\D_n}(\xv)) + 
  (\ftrue(\xv) - \E_{\D_n}(\fh_{\D_n}(\xv)))^2
 +2 \epsilon (\ftrue(\xv) - \E_{\D_n}(\fh_{\D_n}(\xv)))
$$
% \end{eqnarray*}

\end{frame2}

\begin{frame2}[footnotesize]{Bias-Variance decomposition}

$$
(*) = \epsilon^2 + \text{Var}_{\D_n}(\fh_{\D_n}(\xv)) + 
  (\ftrue(\xv) - \E_{\D_n}(\fh_{\D_n}(\xv)))^2
 +2 \epsilon (\ftrue(\xv) - \E_{\D_n}(\fh_{\D_n}(\xv)))
$$

\vfill 

Let us come back to the generalization error by taking the expectation over all fresh test observations $(\xv, y) \sim \Pxy$: 
\vfill

\begin{align*}
GE_n(\ind) 
% =& \var_{xy}(y) + \E_{xy}^2(\ftrue(\xv)) + \E_{xy}\left[\var_{\D_n}(\fh_{\D_n}(\xv) ~|~\xv, y)\right]  \\ &+& \E_{xy}\left[\E^2_{\D_n}(\fh_{\D_n}(\xv) ~|~\xv, y)\right]  - 2\E_{xy}\left[\E_{\D_n}(\ftrue(\xv)~|~\xv, y) \E_{\D_n}(\fh_{\D_n}(\xv)~|~\xv, y)\right] \\
% &=& \underbrace{\E_{xy}(y^2 - \ftrue(\xv)^2~|~\xv, y)}_{= \sigma^2} + \E_{xy}\left[\var_{\D_n}(\fh_{\D_n}(\xv) ~|~\xv, y)\right] \\ &+& \E_{xy}\left[\E^2_{\D_n}(\ftrue(\xv)-\fh_{\D_n}(\xv)~|~\xv, y)\right] \\
=& \underbrace{\sigma^2}_{\text{Variance of the data}} + \E_{xy}\underbrace{\left[\var_{\D_n}(\fh_{\D_n}(\xv) ~|~\xv, y)\right]}_{\text{Variance of learner at } (\xv, y)} \\ 
+& \E_{xy}\underbrace{\left[((\ftrue(\xv)-\E_{\D_n}(\fh_{\D_n}(\xv)))^2~|~\xv, y)\right]}_{\text{Squared bias of learner at } (\xv, y)} + \underbrace{0}_{\text{As $\epsilon$ is zero-mean and independent}}
\end{align*}



\end{frame2}



\endlecture
\end{document}


