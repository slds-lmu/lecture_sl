\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-eval}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{
Advanced Risk Minimization
}{
Bias-Variance 1: \\
Bias-Variance Decomposition
}{
figure/bias_variance_decomposition-linear_model_bias.png
}{
\item Decompose GE of learner into 
\begin{itemize}
\item \footnotesize bias of learner
\item \footnotesize variance of learner
\item \footnotesize inherent noise of data
\end{itemize} 
\item Simulation study demo
\item Capacity and overfitting
}

\begin{framei}[sep=M]{Bias-variance decomposition}

\item Generalization error of learner  $\ind$: 
Expected error of model $\ind(\D_n) = \fh_{\D_n}$, trained on set of size $n$, evaled on fresh test sample
$$
GE_n(\ind) = \E_{\D_n \sim \Pxy^n, (\xv, y) \sim \Pxy}\left[ L(y, \fh_{\D_n}(\xv)) \right] = \E_{\D_n, xy}\left[L(y, \fh_{\D_n}(\xv)) \right]  
$$

\item $\E$ taken over all train sets \textbf{and} independent test sample. Could also frame this as expected risk (expectation over $\D_n$)
$$GE_n(\ind) = \E_{\D_n}\left[\E_{xy}\left[ L(y, \fh_{\D_n}(\xv))\right]\right] =  \E_{\D_n}\left[\risk(\fh_{\D_n})\right]$$

\item For L2 loss, can additively decompose  $GE_n(\ind)$ into 3 components

\item Assume data is generated by 
$$
y = \ftrue(\xv) + \epsilon
$$
with 0-mean homoskedastic error $\epsilon \sim (0, \sigma^2)$; independent of $\xv$

\item Similar decomps exist for other losses expressable as Bregman divergences (e.g. log-loss). One exception is $0/1$ \furtherreading{BROWN2024BIAS}

\end{framei}

\begin{frame}{Bias-variance decomposition}


$GE_n(\ind) =$  
$$
 \underbrace{\sigma^2}_{\text{Var. of $\epsilon$}} + \E_{x}\underbrace{\left[\var_{\D_n}(\fh_{\D_n}(\xv) ~|~\xv)\right]}_{\text{Variance of learner at } \xv} + \E_{x}\underbrace{\left[(\ftrue(\xv)-\E_{\D_n}(\fh_{\D_n}(\xv)))^2~|~\xv\right]}_{\text{Squared bias of learner at } \xv}  
$$

\begin{enumerate}
  \item First: variance of ``pure''
     \textbf{noise} $\epsilon$; aka Bayes, intrinsic or irreducible error; 
    whatever we we do, will never be better
  \item Second: how much \textbf{$\fh_{\D_n}(\xv)$ fluctuates} at test $\xv$ if we vary training data, averaged over feature space; = learner's tendency to learn random things irrespective of real signal (overfitting)
  
  \item Third: how ``off'' are we on average at test locations (underfitting); uses ``average model integrated out over all $\D_n$''; \\
  models with high capacity have low \textbf{bias} and vice versa
\end{enumerate}


\end{frame} 

\begin{framei}[sep=L]{Simulation Example}

\item True model:
$$y = x + \frac{x^2}{2} + \epsilon  \qquad \epsilon \sim 
N (0, 1)$$
\item Split in train and test sets 

\imageC[0.42]{figure/bias_variance_decomposition-train_test.png}

\end{framei} 


\begin{framei}[sep=M]{Simulation Example}

\item Let's estimate bias and variance via bootstrapping

\item (Could have also used Monte Carlo integration of the above quantities,
BS slightly easier to visually explain)

\item First, train several (low capacity) LMs
\item These are the $\fh_{\D_n}(\xv)$, seen as a RV, based on the random
data $\D_n$

\splitVCC
  {\imageC[0.8]{figure/bias_variance_decomposition-bootstrap_1.png}}
  {\imageC[0.8]{figure/bias_variance_decomposition-bootstrap_2.png}}

\end{framei} 

\begin{framei}[sep=L]{Average Model}

\item Average model over different training datasets
\item This is $\E_{\D_n}[\fh_{\D_n}(\xv)]$ in the decomp

  \imageC[0.5]{figure/bias_variance_decomposition-linear_model.png}

\end{framei} 


\begin{framei}[sep=M]{Squared Bias Computation / Estimation}

\item Compute sq. diff. between avg. and true model at each test $x$
\item Then average over all test points
\item This is 
$ 
\E_x[ ( \ftrue(\xv)-\E_{\D_n} (\fh_{\D_n}(\xv) ))^2~|~\xv ]
$


  \imageC[0.5]{figure/bias_variance_decomposition-linear_model_bias.png}


\end{framei} 


\begin{framei}[sep=M]{Variance Computation}

\item Compute variance of model predictions at each test $x$
\item Then average over all test points
\item This is 
$
\E_{x} [\var_{\D_n} (\fh_{\D_n}(\xv) ~|~\xv ) ]
$

  \imageC[0.4]{figure/bias_variance_decomposition-linear_model_variance.png}

\item Here, we know data variance $\sigma^2=1$;\\
could also estimate it from residuals

\end{framei} 


\begin{framei}[fs=footnotesize,sep=M]{Decomp Result and Comparison with MSE}

\item Decomp result; here bias is largest:
$$GE_n(\ind) \approx 1 + 1.628 + 0.135 = 2.763 $$

  \imageC[0.35]{figure/bias_variance_decomposition-linear_model_mse.png}

\item Regular MSE: For each model, compute MSE on test set
\item Then we average these MSEs over all models
\item Result = 2.72; checks out; \\
better if we avg. over more models and test points 
\item In general: Error quite high as we underfitted

\end{framei}


\begin{framei}[sep=M]{Higher complexity learner}

\item Same procedure, but using a high-degree polynomial ($d=7$)

  \imageC[0.55]{figure/bias_variance_decomposition-complex_model.png}

\end{framei} 

\begin{frame}{Higher complexity learner}

  \splitVThree
  {\imageC{figure/bias_variance_decomposition-complex_model_bias.png}}
  {\imageC{figure/bias_variance_decomposition-complex_model_variance.png}}
  {\imageC{figure/bias_variance_decomposition-complex_model_mse.png}}


$$GE_n(\ind) \approx 1 + 0.139 + 1.963 \approx 3.103 $$

\vfill

\begin{itemize}
\item GE higher than before, although hypo space now contains $\ftrue$
\item Bias is lower, and variance higher 
\item Higher capacity learner overfits (here).\\
   We also do not regularize, that would be better
   
\item NB: There is an ``edge effect'' on LHS, Runge effect,\\
  leads to higher bias as ``artifact'' here (ignore this)
\end{itemize}

\end{frame}

\begin{framei}[sep=M]{Higher complexity learner}

\item What happens if we use a model with the same complexity as the true model (quadratic polynomial)? 

\imageC[0.55]{figure/bias_variance_decomposition-correct_model.png}

\end{framei}

\begin{frame}{Higher complexity learner}

  \splitVThree
  {\imageC{figure/bias_variance_decomposition-correct_model_bias.png}}
  {\imageC{figure/bias_variance_decomposition-correct_model_variance.png}}
  {\imageC{figure/bias_variance_decomposition-correct_model_mse.png}}

\vfill

$$GE_n(\ind) \approx 1 + 0.008 + 0.082 = 1.09 $$

\begin{itemize}
  \item Naturally: better result
  \item Low bias, low variance
  \item Bias should not be that much lower than
  high degree polynomial; but see comment there
  \item In any case, variance of the data is lower bound
\end{itemize}

\end{frame}

\begin{framei}[sep=L]{Capacity and overfitting}

  \item Performance of a learner depends on its ability to 
    \begin{enumerate}
      \item \textbf{fit} the training data well
      \item \textbf{generalize} to new data
    \end{enumerate}  
  \item Failure of the first point is called \textbf{underfitting}
  \item Failure of the second point is called \textbf{overfitting}
  
  \imageC[0.6]{figure_man/lcurve_1.png}
  \tiny \centering
  Credit: Ian Goodfellow

\end{framei}

\begin{framei}[sep=M]{Capacity and overfitting}

  \item The tendency of a learner to underfit/overfit is a function of its capacity, determined by the type of hypotheses it can learn
  \item Usually: high capacity $\rightarrow$ low bias $\rightarrow$ better fit on train
  \item But: high capacity $\rightarrow$ high variance $\rightarrow$ high chance of overfitting
  \item For such models, regularization (discussed later) is essential
  \item Even for correctly specified models, the generalization error is lower-bounded by the irreducible noise $\sigma^2$

  \imageC[0.6]{figure_man/lcurve_1.png}
  \tiny \centering
  Credit: Ian Goodfellow
  
\end{framei}

\endlecture
\end{document}


