\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-eval}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{
Advanced Risk Minimization
}{
Bias-Variance 1: \\
Bias-Variance Decomposition
}{
figure/plots_bias_var_deg1_bias.png
}{
\item Decompose GE of learner into 
\begin{itemize}
\item \footnotesize bias of learner
\item \footnotesize variance of learner
\item \footnotesize inherent noise of data
\end{itemize} 
\item Simulation study demo
\item Capacity and overfitting
}

\begin{framei}[sep=M]{Bias-variance decomposition}

\item Generalization error of learner  $\ind$: 
Expected error of model $\ind(\D_n) = \fh_{\D_n}$, trained on set of size $n$, evaled on fresh test sample
$$
GE_n(\ind) = \E_{\D_n \sim \Pxy^n, (\xv, y) \sim \Pxy}\left[ L(y, \fh_{\D_n}(\xv)) \right] = \E_{\D_n, xy}\left[L(y, \fh_{\D_n}(\xv)) \right]  
$$

\item $\E$ taken over all train sets \textbf{and} independent test sample. Could also frame this as expected risk (expectation over $\D_n$)
$$GE_n(\ind) = \E_{\D_n}\left[\E_{xy}\left[ L(y, \fh_{\D_n}(\xv))\right]\right] =  \E_{\D_n}\left[\risk(\fh_{\D_n})\right]$$

\item For L2 loss, can additively decompose  $GE_n(\ind)$ into 3 components

\item Assume data is generated by 
$$
y = \ftrue(\xv) + \epsilon
$$
with 0-mean homoskedastic error $\epsilon \sim (0, \sigma^2)$; independent of $\xv$

\item Similar decomps exist for other losses expressable as Bregman divergences (e.g. log-loss). One exception is $0/1$ \furtherreading{BROWN2024BIAS}

\end{framei}

\begin{frame}{Bias-variance decomposition}


$GE_n(\ind) =$  
$$
\underbrace{\sigma^2}_{\text{Var. of $\epsilon$}} + \E_{x}\underbrace{\left[\var_{\D_n}(\fh_{\D_n}(\xv) ~|~\xv)\right]}_{\text{Variance of learner at } \xv} + \E_{x}\underbrace{\left[(\ftrue(\xv)-\E_{\D_n}(\fh_{\D_n}(\xv)~|~\xv))^2\right]}_{\text{Squared bias of learner at } \xv}  
$$

\begin{enumerate}
\item First: variance of ``pure''
\textbf{noise} $\epsilon$; aka Bayes, intrinsic or irreducible error; 
whatever we we do, will never be better
\item Second: how much \textbf{$\fh_{\D_n}(\xv)$ fluctuates} at test $\xv$ if we vary training data, averaged over feature space; = learner's tendency to learn random things irrespective of real signal (overfitting)

\item Third: how ``off'' are we on average at test locations (underfitting); uses ``average model integrated out over all $\D_n$''; \\
models with high capacity have low \textbf{bias} and vice versa
\end{enumerate}


\end{frame} 

\begin{framei}[sep=L]{Simulation Example}

\item True model:
$$y = x + \frac{x^2}{2} + \epsilon  \qquad \epsilon \sim 
N (0, 1)$$
\item Split each draw of samples into train and test sets 

\imageC[0.7]{figure/plots_bias_var_aux_3.png}

\end{framei} 


\begin{framei}[sep=M]{Simulation Example}

\item Let's estimate bias and variance via by drawing independent data sets from the DGP and averaging.


\item First, we train several (low capacity) LMs
\item These are the $\fh_{\D_n}(\xv)$, seen as a RV, based on the random
data $\D_n$

\hfill

\splitVCC
{\imageC[0.99]{figure/plots_bias_var_aux_1.png}}
{\imageC[0.99]{figure/plots_bias_var_aux_2.png}}

\end{framei} 

\begin{framei}[sep=L]{Average Model}

\item Average model over different training datasets
\item This is $\E_{\D_n}[\fh_{\D_n}(\xv)]$ in the decomp

\hfill

\imageC[0.7]{figure/plots_bias_var_deg1_fits.png}

\end{framei} 


\begin{framei}[sep=M]{Squared Bias Computation / Estimation}

\item Compute sq. diff. between avg. and true model at each test $x$
\item Then average over all test points
\item This is 
$ 
\E_x[ ( \ftrue(\xv)-\E_{\D_n} (\fh_{\D_n}(\xv) )~|~\xv)^2]
$


\imageC[0.7]{figure/plots_bias_var_deg1_bias.png}


\end{framei} 


\begin{framei}[sep=M]{Variance Computation}

\item Compute variance of model predictions at each test $x$
\item Then average over all test points
\item This is 
$
\E_{x} [\var_{\D_n} (\fh_{\D_n}(\xv) ~|~\xv ) ]
$

\imageC[0.7]{figure/plots_bias_var_deg1_variance.png}

\item For irreducible noise component, we know data variance $\sigma^2=1$;\\
could also estimate it from residuals

\end{framei} 


\begin{framei}[fs=small,sep=M]{Decomp Result and Comparison with MSE}

\item Decomp result; here bias is largest:
$$GE_n(\ind) \approx 1 + 1.572 + 0.288 = 2.86 $$

\imageC[0.6]{figure/plots_bias_var_deg1_mse.png}

\item Regular MSE: For each model, compute MSE on test set
\item Then we average these MSEs over all models
\item Result = 2.86; checks out; \\
\item In general: Error is quite high as we underfitted

\end{framei}


\begin{framei}[sep=M]{Higher complexity learner}

\item Same procedure, but using a high-degree polynomial ($d=7$). Average model looks good now (low bias)

\hfill

\imageC[0.7]{figure/plots_bias_var_deg7_fits.png}

\end{framei} 

\begin{frame}{Higher complexity learner}

\splitVThree
{\imageC{figure/plots_bias_var_deg7_bias.png}}
{\imageC{figure/plots_bias_var_deg7_variance.png}}
{\imageC{figure/plots_bias_var_deg7_mse.png}}


$$GE_n(\ind) \approx 1 + 9.422 + 0.037 \approx 10.458 $$

\vfill

\begin{itemize}
\item GE higher than before, although hypo space now contains $\ftrue$
\item Bias is lower, and variance higher 
\item Higher capacity learner overfits (here).\\
We also do not regularize, that would be better

\item NB: There is an ``edge effect'' on LHS, Runge effect,\\
leads to higher bias as ``artifact'' here (ignore this)
\end{itemize}

\end{frame}

\begin{framei}[sep=M]{Correct complexity learner}

\item What happens if we use a model with the same complexity as the true model (quadratic polynomial)? 

\hfill

\imageC[0.7]{figure/plots_bias_var_deg2_fits.png}

\end{framei}

\begin{frame}{Correct complexity learner}

\splitVThree
{\imageC{figure/plots_bias_var_deg2_bias.png}}
{\imageC{figure/plots_bias_var_deg2_variance.png}}
{\imageC{figure/plots_bias_var_deg2_mse.png}}

\vfill

$$GE_n(\ind) \approx 1 + 0.004 + 0.081 = 1.085 $$

\begin{itemize}
\item Naturally: better result
\item Lowest bias, low variance
\item In any case, variance of the data (irreducible noise) is lower bound
\end{itemize}

\end{frame}

\begin{framei}[sep=L]{Capacity and overfitting}

\item Performance of a learner depends on its ability to 
\begin{enumerate}
\item \textbf{fit} the training data well
\item \textbf{generalize} to new data
\end{enumerate}  
\item Failure of the first point is called \textbf{underfitting}
\item Failure of the second point is called \textbf{overfitting}

\imageC[0.6]{figure_man/lcurve_1.png}
\tiny \centering
Credit: Ian Goodfellow

\end{framei}

\begin{framei}[sep=M]{Capacity and overfitting}

\item Tendency of a learner to underfit/overfit is function of its capacity, determined by the type of hypotheses it can learn
\item Usually: high capacity $\rightarrow$ low bias $\rightarrow$ better fit on train
\item But: high capacity $\rightarrow$ high variance $\rightarrow$ high chance of overfitting
\item For such models, regularization (discussed later) is essential
\item Even for correctly specified models, generalization error is lower-bounded by irreducible noise $\sigma^2$

\imageC[0.6]{figure_man/lcurve_1.png}
\tiny \centering
Credit: Ian Goodfellow

\end{framei}

\endlecture
\end{document}


