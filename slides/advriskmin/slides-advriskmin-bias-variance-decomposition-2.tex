\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-eval}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{
Advanced Risk Minimization
}{
Bias-Variance 2: \\
Approximation and Estimation error
}{
figure/bias_variance_decomposition-linear_model_bias.png
}{
\item Decomposing excess risk
\item Into estimation, approx. and optim. error
}






\begin{vbframe}{Approx./Estimation Error \citelink{BROWN2024BIAS}}

\begin{itemize}

\item BV decomp often confused with related (but different) decomp: 
%\textbf{excess risk} into \textbf{estimation} and \textbf{approximation} error

$$
    \underbrace{\risk(\hat f_{\Hspace}) - \risk(\fbayes_{\Hspace_{all}})}_{\text{excess risk}} = \underbrace{\risk(\hat f_{\Hspace}) - \risk(\fbayes_{\Hspace})}_{\text{estimation error}} + \underbrace{\risk(\fbayes_{\Hspace}) -  \risk(\fbayes_{\Hspace_{all}})}_{\text{approx. error}} 
$$

\item Both commonly described using same figure and analogies

\vfill

\begin{figure}
    \centering
    \includegraphics[width = 0.7\textwidth]{figure_man/biasvar-vs-estapprox-tradeoff.png}
    \tiny{\\ Credit: \cite{BROWN2024BIAS}}
  \end{figure}

\vfill


\item BV decomp. only holds for certain losses, above is universal

\end{itemize}

\end{vbframe}
\begin{vbframe}{Approx./Estimation Error \citelink{BROWN2024BIAS}}

   
\begin{itemize}
    \item Approx. error is a structural property of $\Hspace$
    \item Estimation error is random due to dependence on data in $\fh$
    \item Estimation error occurs as we choose $f \in \Hspace$ with limited train data minimizing $\riske$ instead of $\risk$

\item Knowing $\fh_{\Hspace} \in \arg\inf_{f \in \Hspace} \riske(f)$ assumes we found a global minimizer of $\riske$, which is often impossible (e.g. ANNs) 

\item In practice, optimizing $\riske$ gives us ``best guess'' $\tilde{f}_{\Hspace} \in \Hspace$ of $\fh_{\Hspace}$ 

\item Can now decompose its excess risk finer as

\begin{eqnarray*}
    \underbrace{\risk(\tilde{f}_{\Hspace}) - \risk(\fbayes_{\Hspace_{all}})}_{\text{excess risk}} &=& \underbrace{\risk(\tilde{f}_{\Hspace}) - \risk(\fh_{\Hspace})}_{\text{optim. error}} + \underbrace{\risk(\hat{f}_{\Hspace}) - \risk(\fbayes_{\Hspace})}_{\text{estimation error}} + \underbrace{\risk(\fbayes_{\Hspace}) -  \risk(\fbayes_{\Hspace_{all}})}_{\text{approx. error}} 
\end{eqnarray*}

\item NB: Optim err. can be $< 0$, but $\riske(\tilde{f}_{\Hspace}) \geq \riske(\fh_{\Hspace})$ always

\end{itemize}

\end{vbframe}
\begin{vbframe}{Approx./Estimation Error \citelink{BROWN2024BIAS}}

\begin{itemizeM}

\item We can further decompose estimation error more finely by defining the \textit{centroid} model or ``systematic'' model part

\item For $\fh_{\Hspace} \in  \arg\min_{f \in \Hspace} \riske(f)$ centroid model under L2 loss is mean prediction at each $x$ over all $\D_n$, $f^{\circ}_{\Hspace} := \E_{\D_n \sim \Pxy^n}[\fh_{\Hspace}] $

\item With $f^{\circ}_{\Hspace}$, can decompose expected estimation error as
$$
 \underbrace{\E_{\D_n \sim \Pxy^n}\left[\risk(\hat{f}_{\Hspace}) - \risk(f ^{\ast}_{\Hspace})\right]}_{\text{expected estimation error}} = \underbrace{\E_{\D_n \sim \Pxy^n}\left[\risk(\hat{f}_{\Hspace}) - \risk(f^{\circ}_{\Hspace}) \right]}_{\text{estimation variance}} + \underbrace{\risk(f^{\circ}_{\Hspace})-\risk(f^{\ast}_{\Hspace})}_{\text{estimation bias}}   
$$


\item Estimation bias measures distance of centroid model to risk minimizer over $\Hspace$
\item Estimation var. spread of ERM around centroid model induced by randomness due to $\D_n$

\end{itemizeM}

\end{vbframe}
\begin{vbframe}{Approx./Estimation Error \citelink{BROWN2024BIAS}}

\begin{itemizeM}

\item Can now connect derived quantities back to bias and variance 
\item Bias is not only approx. error and variance is not estimation error
\item Many details skipped here, see paper!

$$ \text{bias} = \text{approximation error} + \text{estimation bias} $$
$$\text{variance} = \text{optimization error} + \text{estimation variance}$$

\begin{figure}
    \centering
    \includegraphics[width = 0.45\textwidth]{figure_man/expected-risk-decomp.png}
    \tiny{\\ Credit: \cite{BROWN2024BIAS}}
  \end{figure}

\item \textbf{NB}: For special case of LM and L2 loss, we have very small optim / numerical error and estimation bias; so both decomps agree

\end{itemizeM}

\end{vbframe}

\endlecture
\end{document}


