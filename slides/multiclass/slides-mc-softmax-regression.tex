\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Introduction to Machine Learning}

\begin{document}
    
\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Multiclass Classification
  }{% Lecture title  
    Softmax Regression
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/softmax1.png
  }{
  \item Know softmax regression
  \item Understand that softmax regression is a generalization of logistic regression
}

\begin{vbframe}{From logistic regression ...}

Remember \textbf{logistic regression} ($\Yspace = \{0, 1\}$): We combined the hypothesis space of linear functions, transformed by the logistic function $s(z) = \frac{1}{1 + \exp(- z)}$, i.e.\

\vspace*{-0.3cm}

\begin{eqnarray*}
  \Hspace = \left\{\pi: \Xspace \to \R ~|~\pix = s(\thetab^\top \xv)\right\}\,,
\end{eqnarray*}

with the Bernoulli (logarithmic) loss: 

\begin{eqnarray*}
  L(y, \pix) = -y \log \left(\pix\right) - (1 - y) \log \left(1 - \pix\right).
\end{eqnarray*}

\vfill

\begin{footnotesize}
  \textbf{Remark:} We suppress the intercept term for better readability. The intercept term can be easily included via $\thetab^\top \tilde\xv$, $\thetab \in \R^{p + 1}$, $\tilde\xv = (1, \xv)$.
\end{footnotesize}

\end{vbframe}

\begin{vbframe}{... to softmax regression} 

There is a straightforward generalization to the multiclass case: 

\begin{itemize}
  \item Instead of a single linear discriminant function we have $g$ linear discriminant functions
    $$
      f_k(\xv) = \thetab_k^\top \xv, \quad k = 1, 2, ..., g,
    $$
  each indicating the confidence in class $k$.
  \item The $g$ score functions are transformed into $g$ probability functions by the \textbf{softmax} function $s:\R^g \to \R^g$ 

  $$
    \pi_k(\xv) = s(\fx)_k = \frac{\exp(\thetab_k^\top \xv)}{\sum_{j = 1}^g \exp(\thetab_j^\top \xv) }\,,
  $$
  instead of the \textbf{logistic} function for $g = 2$. The probabilities are well-defined: $\sum \pi_k(\xv) = 1$ and $\pi_k(\xv) \in [0, 1]$ for all $k$. 

  \item The softmax function is a generalization of the logistic function. For $g = 2$, the logistic function and the softmax function are equivalent. 

  \item Instead of the \textbf{Bernoulli} loss, we use the multiclass \textbf{logarithmic loss}
   $$
    L(y, \pix) = - \sum_{k = 1}^g \mathds{1}_{\{y = k\}} \log\left(\pi_k(\xv)\right).
  $$ 
    \item Note that the softmax function is a \enquote{smooth} approximation of the arg max operation,
        so $s((1, 1000, 2)^T) \approx (0, 1, 0)^T$ (picks out 2nd element!).  
    \item Furthermore, it is invariant to constant offsets in the input:  
      \end{itemize}
    $$ 
    s(\fx + \mathbf{c}) = \frac{\exp(\thetab_k^\top \xv + c)}{\sum_{j = 1}^g \exp(\thetab_j^\top \xv + c)} = 
    \frac{\exp(\thetab_k^\top \xv)\cdot \exp(c)}{\sum_{j = 1}^g \exp(\thetab_j^\top \xv) \cdot \exp(c)} = 
    s(\fx)
    $$  


\end{vbframe}

\begin{vbframe}{Logistic vs. softmax regression} 

\begin{scriptsize}
\begin{table}[]
\bgroup
\def\arraystretch{2}%  1 is the default, change whatever you need
\begin{tabular}{ccc}
& Logistic Regression & Softmax Regression \\ \hline
$\Yspace$ & $\{0, 1\}$ & $\{1, 2, ..., g\}$ \\[0.5cm]
Discriminant fun. & $f(\xv) = \thetab^\top \xv$ & $f_k(\xv) = \thetab_{k}^{\top} \xv, k = 1, 2, ..., g$ \\[0.5cm]
Probabilities & $\pi(\xv) = \frac{1}{1 + \exp\left(-\thetab^\top \xv\right)}$ & $\pi_k(\xv) = \frac{\exp(\thetab_k^\top \xv)}{\sum_{j = 1}^g \exp(\thetab_j^\top \xv) }$ \\[0.5cm]
$L(y, \pix)$ & Bernoulli / logarithmic loss & Multiclass logarithmic loss\\[-0.3cm]
& $-y \log \left(\pix\right) - (1 - y) \log \left(1 - \pix\right)$  & $ - \sum_{k = 1}^g [y = k] \log\left(\pi_k(\xv)\right)$ \\
\end{tabular}
\egroup
\end{table}
\end{scriptsize}

\end{vbframe}

\frame{
\frametitle{Logistic vs. softmax regression}
We can schematically depict softmax regression as follows:

\only<1>{
\begin{center}
  \includegraphics[width=0.6\textwidth]{figure_man/softmax2.png}
\end{center}
}

\only<2>{
\begin{center}
  \includegraphics[width=0.6\textwidth]{figure_man/softmax1.png}
\end{center}
}
}

\begin{vbframe}{Logistic vs. softmax regression}

Further comments:

\begin{itemize}

\item We can now, for instance, calculate gradients and optimize this with standard numerical optimization software.

% \item For linear $\fxt = \theta^T \xv$ this is also called \emph{softmax regression}.

\item Softmax regression has an unusual property in that it has a \enquote{redundant} set of parameters. If we subtract a fixed vector
  from all $\thetab_k$, the predictions do not change at all.
  Hence,  our model is \enquote{over-parameterized}. For any hypothesis we might fit,
  there are multiple parameter vectors that give rise to exactly the same hypothesis function.
  This also implies that the minimizer of $\risket$ above is not unique!
  Hence, a numerical trick is to set $\thetab_g = 0$ and only optimize the other $\thetab_k$. This does not restrict our hypothesis space, but the constrained problem is now convex, i.e., there exists exactly one parameter vector for every hypothesis.

\item A similar approach is used in many ML models: multiclass LDA, naive Bayes, neural networks and boosting.

\end{itemize}

\end{vbframe} 

\begin{vbframe}{Softmax: Linear discriminant functions}

Softmax regression gives us a \textbf{linear classifier}. 

\begin{itemize}
  \item The softmax function $s(\bm{z})_k = \frac{\exp(\bm{z}_k)}{\sum_{j = 1}^g \exp \left(\bm{z}_j\right)}$ is 

\begin{itemize}
  \item a rank-preserving function, i.e.  the ranks among the elements of the vector $\bm{z}$ are the same as among the elements of $s(\bm{z})$. This is because softmax transforms all scores by taking the $\exp(\cdot)$ (rank-preserving) and divides each element by \textbf{the same} normalizing constant. 
\end{itemize}

Thus, the softmax function has a unique inverse function $s^{-1}: \R^g \to \R^g$ that is also monotonic and rank-preserving. Applying $s_k^{-1}$ to $\pi_k(\xv) = \frac{\exp(\thetab_k^\top \xv)}{\sum_{j = 1}^g \exp(\thetab_j^\top \xv)}$ gives us $f_k(\xv) = \thetab_k^\top \xv$. Thus, softmax regression is a linear classifier. 
\end{itemize}

\end{vbframe}

\begin{vbframe}{Generalizing softmax regression} 

Instead of simple linear discriminant functions we could use \textbf{any} model that outputs $g$ scores 
$$
  f_k(\xv) \in \R, k = 1, 2, ..., g
$$
We can choose a multiclass loss and optimize the score functions $f_k, k \in \{1, ..., g\}$ by multivariate minimization. 
The scores can be transformed to probabilities by the \textbf{softmax} function.

\begin{center}
  \includegraphics[width=0.3\textwidth]{figure_man/score_model.png}
\end{center}

\framebreak 

For example for a \textbf{neural network} (note that softmax regression is also a neural network with no hidden layers):

\begin{center}
  % % FIGURE SOURCE: https://docs.google.com/presentation/d/1uzV18OvqNRJb0ihilEb-_RLYbPQJVyxl6vnrAqRv0fc/edit?usp=sharing
  \includegraphics[width=0.6\textwidth]{figure_man/neural_net.png}
\end{center}

\textbf{Remark:} For more details about neural networks please refer to the lecture \textbf{Deep Learning}. 


\end{vbframe}


\endlecture
\end{document}