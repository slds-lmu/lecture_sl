\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-svm}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Nonlinear Support Vector Machines
  }{% Lecture title  
    The Gaussian RBF Kernel
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/svm_rbf_as_basis.png
  }{
  \item Know the Gaussian (RBF) kernel
  \item Understand that all data sets are separable with this kernel
  \item Understand the effect of the kernel hyperparameter $\sigma$
}

\begin{vbframe}{RBF Kernel}

The \enquote{radial} \textbf{Gaussian kernel} is defined as
$$k(\xv, \tilde \xv) = \exp(-\frac{\|\xv - \tilde \xv\|^2}{2\sigma^2}) \quad \text{ or } \quad k(\xv, \tilde \xv) = \exp(-\gamma \|\xv - \tilde \xv\|^2)$$

\begin{center}
\includegraphics[width = 11cm ]{figure/svm_rbf_kernel.png}
\end{center}

A straightforward extension is
$$k(\xv,  \tilde \xv) = \exp\big(- (\xv -  \tilde \xv)^T C (\xv -  \tilde \xv)\big)$$
for a symmetric, positive definite matrix $C$.

\framebreak

  \begin{itemize}

    \item With a Gaussian kernel, all RKHS basis functions 
      $\phix = k(\xv, \cdot)$ are linearly independent - which we will not prove here.
    \item This means that all (finite) data sets are linearly separable!
    \item Do we then need soft-margin machines? 
      The answer is \enquote{yes}. The roles of the nonlinear feature map and the
    soft-margin constraints are very different:
    \begin{itemize}
      \item The purpose of the kernel (and its feature map) is to make learning
      \enquote{easy}.
\item Even in an infinite-dimensional feature space we may want some
      margin violators because we should not trust noisy data. A hard-margin
      SVM with Gaussian kernels may be able to separate any dataset but will usually overfit.
    \end{itemize}
  \end{itemize}
\end{vbframe}

\begin{frame}{Weighted mixture of Gaussians}

 Via the RKHS / basis function intuition we can understand the effect of the RBF kernel much better as a local model.
$$ \fx = \sumin \alpha_i \yi k(\xi, \xv)  + \theta_0 $$
\vspace{-1cm}
\begin{columns}
\begin{column}{0.7\textwidth}

\begin{center}
\includegraphics<1>[width=0.9\textwidth,trim={2.5cm 5cm 2.5cm 5cm},clip]{figure/svm_rbf_as_basis_1.png}%
\includegraphics<2>[width=0.9\textwidth,trim={2.5cm 5cm 2.5cm 5cm},clip]{figure/svm_rbf_as_basis_2.png}%
\includegraphics<3>[width=0.9\textwidth,trim={2.5cm 5cm 2.5cm 5cm},clip]{figure/svm_rbf_as_basis_3.png}%
\includegraphics<4>[width=0.9\textwidth,trim={2.5cm 5cm 2.5cm 5cm},clip]{figure/svm_rbf_as_basis_4.png}%
\includegraphics<5>[width=0.9\textwidth,trim={2.5cm 5cm 2.5cm 5cm},clip]{figure/svm_rbf_as_basis_5.png}%
\includegraphics<6>[width=0.9\textwidth,trim={2.5cm 5cm 2.5cm 5cm},clip]{figure/svm_rbf_as_basis_6.png}%
\includegraphics<7>[width=0.9\textwidth,trim={2.5cm 5cm 2.5cm 5cm},clip]{figure/svm_rbf_as_basis.png}%
\end{center}
\end{column}
\begin{column}{0.4\textwidth}

\footnotesize

\lz

All support vectors are assigned RBF "bumps", these are weighted with the dual variables / Lagrange multipliers $\alpha_i$ and labels $\yi$.
We then "mix" these bumps together to form the decision score function.
Which becomes a bumpy surface.


\end{column}
\end{columns}

\end{frame}

\begin{vbframe}{RBF Kernel Width}

A large $\sigma$ (or a small $\gamma$) will make the decision boundary very smooth and in the limit almost linear. 

\begin{center}
\includegraphics[width = 11cm ]{figure/svm_rbf_kernel_gamma_1.png}
\end{center}

\framebreak

A small $\sigma$ parameter makes the function more \enquote{wiggly}, in the limit we totally over fit the data by basically modelling each training data point - and maximal uncertainty at all other test points.

\begin{center}
\includegraphics[width = 11cm ]{figure/svm_rbf_kernel_gamma_2.png}
\end{center}

\end{vbframe}
\endlecture
\end{document}
