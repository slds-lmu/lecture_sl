\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-svm}

\newcommand{\titlefigure}{figure/svm_poly_kernel_deg_9_coef0_1.png}
\newcommand{\learninggoals}{
  \item Know the homogeneous and non-homogeneous polynomial kernel 
  \item Understand the influence of the choice of the degree on the decision boundary
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{The Polynomial Kernel}
\lecture{Introduction to Machine Learning}



\begin{vbframe}{Homogeneous Polynomial Kernel}
$$ k(\xv, \xtil) = (\xv^T \xtil)^d, \text{ for } d \in \N$$
The feature map contains all monomials of exactly order $d$.
  $$\phix = \left( \sqrt{\mat{d \\ k_1, \ldots, k_p}} x_1^{k_1} \ldots x_p^{k_p}\right)_{k_i \geq 0, \sum_i k_i = d}$$
That $\scp{\phix}{\phixt} = k(\xv, \xtil)$ holds can easily be checked by simple calculation
and using the multinomial formula
$$ (x_1 + \ldots + x_p)^d = \sum_{k_i \geq 0, \sum_i k_i = d} \mat{d \\ k_1, \ldots, k_p} x_1^{k_1} \ldots x_p^{k_p}$$
The map $\phix$ has $\mat{p + d - 1 \\ d}$ dimensions.
We see that $\phix$ contains no terms of "lesser" order, so, e.g., linear effects.
As an example for $p=d=2$: $\phix = (x_1^2, x_2^2, \sqrt{2} x_1 x_2)$.
% From the sum-product rules it directly follows that this is a kernel.
\end{vbframe}

\begin{vbframe}{NonHomogeneous Polynomial Kernel}
 $$k(\xv, \xtil) = (\xv^T \xtil + b)^d, \text{ for } b\geq 0, d \in \N$$
The maths is very similar as before, we kind of add a further constant term in the original space, with
$$ (\xv^T \xtil + b)^d = (x_1 \tilde{x}_1 + \ldots + x_p \tilde{x_p} + \sqrt{b} \sqrt{b})^d$$
The feature map contains all monomials up to order $d$.
  $$\phix = \left( \sqrt{\mat{d \\ k_1, \ldots, k_{p+1}}} x_1^{k_1} \ldots x_p^{k_p} b^{k_{p+1}/2} \right)_{k_i \geq 0, \sum_i k_i = d}$$
The map $\phix$ has $\mat{p + d\\ d}$ dimensions. For $p=d=2$: 
$$\phix = (x_1^2, x_2^2, \sqrt{2} x_1 x_2, \sqrt{2b} x_1, \sqrt{2b} x_2, b)$$
% From the sum-product rules it directly follows that this is a kernel.

\framebreak
  The relationship between the kernel and the feature map can be shown by unraveling the polynomial formula. For p=d=2:
  
  \begin{equation*}
    \begin{aligned}
        \left ( \xv^T \xtil + b  \right)^2 &=       \left ( \mat{ x_1 \\ x_2} \cdot (\xt_1 \ \xt_2) + b  \right) \cdot      \left ( \mat{ x_1 \\ x_2} \cdot (\xt_1 \ \xt_2) + b  \right) \\[10pt]
        &= (x_1 \xt_1 + x_1 \xt_2 + x_2 \xt_1 + x_2 \xt_2 + b) \cdot \\
         & \ \ \ \ \ \ (x_1 \xt_1 + x_1 \xt_2 + x_2 \xt_1 + x_2 \xt_2 + b) \\[10pt]
        &= x_1^2 \xt_1^2 + x_2^2 \xt_2^2  +  2  x_1 \xt_1 x_2 \xt_2  + 2 b x_1 \xt_1 + 2 b x_2 \xt_2  + b^2 \\[10pt]
        &= ( x_1^2 , x_2^2 , \sqrt{2} x_1 x_2 , \sqrt{2b} x_1 , \sqrt{2b} x_2 , b )^T \cdot \\
        & \ \ \ \ \ \  ( \xt_1^2 , \xt_2^2 ,\sqrt{2} \xt_1 \xt_2 , \sqrt{2b} \xt_1 , \sqrt{2b} \xt_2 ,  b ) \\[10pt]
        &= \phix^T \cdot \phixt
    \end{aligned}
  \end{equation*}
\end{vbframe}



\begin{vbframe}{Polynomial Kernel}


Degree $d = 1$ yields a linear decision boundary. 

\vspace*{0.1cm} 
\begin{center}
\includegraphics[width = 11cm]{figure/svm_poly_kernel_deg_1_coef0_1.png}
\end{center}

\framebreak

The higher the degree, the more nonlinearity in the decision boundary. 

\vspace*{0.1cm}
\begin{center}
\includegraphics[width = 11cm]{figure/svm_poly_kernel_deg_3_coef0_1.png}
\end{center}

\framebreak

The higher the degree, the more nonlinearity in the decision boundary. 
\vspace*{0.1cm}
\begin{center}
\includegraphics[width = 11cm]{figure/svm_poly_kernel_deg_9_coef0_1.png}
\end{center}

\framebreak 

For $k(\xv, \tilde \xv) = (\xv^\top \tilde \xv + 0)^d$ we get no lower order effects. 

\vspace*{0.1cm}
\begin{center}
\includegraphics[width = 11cm]{figure/svm_poly_kernel_deg_3_coef0_0.png}
\end{center}

\end{vbframe}


\endlecture
\end{document}
