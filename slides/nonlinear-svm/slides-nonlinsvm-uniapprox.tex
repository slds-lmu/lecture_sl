\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-svm}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Nonlinear Support Vector Machines
  }{% Lecture title  
    Details on Support Vector Machines
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/circles_ds.png
  }{
  \item Know that SVMs are non-parameteric models
  \item Understand the concept of universal consistency
  \item Know that SVMs with an universal kernel (e.g. Gaussian kernel) are universally consistent 
}

\section{SVMs as Non-Parametric Models}

\begin{vbframe}{SVMs as Non-Parametric Models}

  \begin{itemize}
    \item In contrast to linear models, for an SVM
    we do not have to decide the number of coefficients of the
    decision function before training.
    \item The number of coefficients depends on the
    size of the dataset, or on the number of support vectors.
    \item Such models are called \textbf{non-parametric}.
    \item The big advantage of non-parametric models is that their
    modeling capacity is not \textit{a priori} restricted
    to a finite-dimensional subspace of a function space.
    \item It turns out that SVMs do even better: There exist kernels so
    that an SVM can model all continuous functions arbitrarily well.
    It is also known that the SVM learning algorithm can approximate the Bayes optimal decision
    function arbitrarily well in the limit of infinite data.
    \item This property is known as \textbf{universal~consistency}.
  \end{itemize}

  \framebreak

  \textbf{Definition} [Steinwart, 2002]:
  Let $\Xspace \subset \R^p$ be compact. A continuous kernel
  $k : \Xspace \times \Xspace \to \R$ is called universal if the set of all
  induced functions $\sum_i \beta_i k\left(\xi, \cdot\right)$ is dense in
  $\continuous(\Xspace)$; i.e., for all $g \in \continuous(\Xspace)$ and all
  $\varepsilon > 0$ there exists a function $f$ induced by $k$ with
  $\|f-g\|_\infty \leq \varepsilon$.\\[1em]

  \textbf{Example:} Gaussian kernels are universal.\\[1em]

  \textbf{Theorem} [simplified from Steinwart, 2002]:
  For compact $\Xspace \subset \R^p$ define $C(n) = C_0 \cdot n^{q - 1}$
  for some $C_0 > 0$ and $0 < q < 1/p$. Fix any distribution $\mathbb{P}$ on
  $\Xspace \times \{\pm 1\}$ from which i.i.d.\ datasets $\D_n$ of size
  $n$ are drawn. Let $h_n$ denote the soft-margin SVM model,
  trained with a universal kernel and regularization constant $C(n)$
  on the data $\D_n$. Then it holds
  $$
    \lim_{n \to \infty} \E[\risk(h_n)] = \risk^*
    \enspace,
  $$
  where $\risk^*$ denotes the Bayes risk.

\end{vbframe}

\begin{vbframe}{Asymptotic performance}

  \begin{itemize}
    \item Convergence of the risk to the Bayes risk for all distributions
    is called \textbf{universal consistency}.
    \item A universally consistent learning machine can solve all problems
    optimally, provided enough data.
    \item Parametric models are too inflexible for this property. They
    can model only a finite-dimensional subspace (manifold) of
    decision functions.
    \item Thus, in the limit of infinite data, they will systematically
    underfit.
    \item Universal consistency requires more than infinite-dimensional
    modeling power: We also need a learning rule that uses the
    flexibility wisely and avoids overfitting.
    \item The existence of universally consistent learners is one of the
    most exciting facts from non-parametric statistics.
  \end{itemize}

  \framebreak

  \begin{itemize}
    \item Note the arbitrary positive constant $C_0$ in the definition
    of $C(n) = C_0 \cdot n^{q - 1}$.
    \item This means that for a single fixed $n$, $C(n)$ can have
    any positive value.
    \item This is not a problem for the theorem since all it requires is that
    $C$ changes at the right rate with~$n$:

    \begin{itemize}
      \item $n \cdot C(n)$ tends to infinity, which means that the
      relative impact of the regularizer compared to the empirical
      risk decays to zero, so, the risk term takes over for
      large~$n$;
      \item The convergence of $n \cdot C(n)$ to infinity is slow
      enough to avoid overfitting (this is far from obvious, but
      it is in the details of the proof of the theorem).
    \end{itemize}
    \item Importantly, since $C$ can be arbitrary for fixed $n$, this
    theorem does not tell us which $C$ to use for a given problem
    size.
    % \item The universal consistency theorem is purely asymptotic in nature
    % and of absolutely no practical relevance.
  \end{itemize}

\end{vbframe}

\begin{vbframe}{SVM -- Pro's \& Con's}

\begin{columns}[T, totalwidth=\textwidth]
  \begin{column}{0.5\textwidth}
    \textbf{Advantages}
    \normalsize
    \begin{itemize}
      % \item High \textbf{accuracy}
      \item Often \textbf{sparse} solution (w.r.t. observations)
      \item Robust against overfitting (\textbf{regularized}); especially in 
      high-dimensional space
      \item \textbf{Stable} solutions (w.r.t. changes in train data)\\
      $\rightarrow$ Non-SV do not affect decision boundary
      \item Convex optimization problem \\
      $\rightarrow$ local minimum $\hat{=}$ global minimum
      %\item \textbf{memory efficient} (only use non-SVs)
    \end{itemize}
    
    % \textbf{Advantages (nonlinear SVM)}
    % \begin{itemize}
    %    \item Can learn \textbf{nonlinear decision boundaries}
    %    \item \textbf{Very flexible} due to custom kernels \\
    %    $\rightarrow$ RBF kernel yields local model \\
    %    $\rightarrow$ kernel for time series, strings etc.
    % \end{itemize}
  \end{column}

  \begin{column}{0.5\textwidth}
    \textbf{Disadvantages}
    \normalsize
    \begin{itemize}
      \item \textbf{Long} training times $\rightarrow O(n^2 p + n^3)$
      %\item \textbf{Limited scalability} to larger data sets 
      %\textcolor{blue}{\textbf{??}}
      \item Confined to \textbf{linear model}
      \item Restricted to \textbf{continuous features}
      \item Optimization can also fail or get stuck
      % \item Poor \textbf{interpretability}
      %\item No handling of \textbf{missing} data
    \end{itemize}

    %     \textbf{Disadvantages (nonlinear SVM)}
    % \begin{itemize}
    %    \item Poor \textbf{interpretability} due to complex kernel
    %    \item \textbf{Not easy tunable} as it is highly important to choose the right kernel (which also introduces further hyperparameters)
    % \end{itemize}
  \end{column}
\end{columns}

\framebreak
\lz 

\begin{columns}[t, totalwidth=\textwidth]
  \begin{column}{0.5\textwidth}    
    \textbf{Advantages (nonlinear SVM)}
    \begin{itemize}
       \item Can learn \textbf{nonlin. decision boundaries}
       \item \textbf{Very flexible} due to custom kernels \\
       $\rightarrow$ RBF kernel yields local model \\
       $\rightarrow$ kernel for time series, strings etc.
    \end{itemize}
  \end{column}

  \begin{column}{0.5\textwidth}
    \textbf{Disadvantages (nonlin. SVM)}
    \begin{itemize}
       \item Poor \textbf{interpretability} due to complex kernel
       \item \textbf{Not easy tunable} as it is highly important to choose the right kernel (which also introduces further hyperparameters)
    \end{itemize}
  \end{column}
\end{columns}

% \conclbox{Very accurate solution for high-dimensional data that is linearly 
% separable}

\end{vbframe}



\section{Kernels on Infinite-Dimensional Vector Spaces}

\begin{vbframe}{Kernels on infinite-dimensional vector spaces}

  \begin{itemize}
    \item Note that the input space $\Xspace$ does not need to be a finite-dimensional
    vector space.
    \item $\Xspace$ could be the set of all character strings (of unlimited length) or 
       of graphs, or of trees.
    \item Such data structures are natural representations for, e.g, HTML documents.
    \item There are many examples of data that do not naturally come in vector
    form.
    \item Most often meaningful and cheap-to-compute kernels can be defined
    directly on the input data structures -- they simply define a similarity measure over these data.
    \item SVMs (and other kernel methods) allow to learn and predict
    directly on these spaces.
  \end{itemize}


\end{vbframe}


\endlecture
\end{document}