\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-svm}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
    Nonlinear Support Vector Machines
  }{% Lecture title  
    SVM Model Selection
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/rbf_sigma.png
  }{
  \item Know that the SVM is sensitive to hyperparameter choices
  \item Understand the effect of different (kernel) hyperparameters
}

\begin{vbframe}{Model Selection for Kernel SVMs}
    \begin{itemize}
    \item \enquote{Kernelizing} a linear algorithm effectively turns this
    algorithm into a family of algorithms --- one for each
    kernel. There are infinitely many kernels, and many
    efficiently computable kernels.
    \item However, the choice of $C$, the choice of the kernel, the kernel parameters
      are all up to the user.
    \item On the one hand this allows very flexible modelling, and also to 
      incorporate prior knowledge into the learning process. 
    \item On the other hand this puts a huge burden on the user. 
      The machine has no mechanism for identifying a good
    kernel by itself.
    \item SVMs are somewhat sensitive to its hyperparameters and should always 
      be tuned.
  \item Gaussian processes are very related kernel methods, with the big advantage that kernel parameters are directly estimated during training.
  \end{itemize}
\end{vbframe}


\begin{vbframe}{SVM hyperparameters}

Small $C$ \enquote{allows} for margin-violating points in favor of a large margin. 


\begin{center}
\includegraphics[width=0.9\textwidth]{figure/svm_rbf_cost_1.png}
\end{center}


Large $C$ penalizes margin violators, decision boundary is more \enquote{wiggly}. 


\begin{center}
\includegraphics[width=0.9\textwidth]{figure/svm_rbf_cost_2.png}
\end{center}


\framebreak

Hyperparameters strongly influence the model: RBF kernel.

\begin{center}
\includegraphics[width=0.9\textwidth]{figure/svm_rbf_hyperparams.png}
\end{center}


\framebreak

Hyperparameters strongly influence the model: Polynomial kernel.

\begin{center}
\includegraphics[width=0.9\textwidth]{figure/svm_poly_hyperparams.png}
\end{center}

\end{vbframe}


\begin{vbframe}{RBF Sigma Heuristic}

For the RBF kernel $k(\xv, \tilde \xv) = \exp(-\frac{\|\xv - \tilde \xv \|^2}{2\sigma^2})$ a simple heuristic exists for the width hyperparameter $\sigma^2$. 

\lz


\begin{center}
  \includegraphics[width=5cm]{figure/svm_rbf_sigma_est_narrow.pdf}
  \includegraphics[width=5cm]{figure/svm_rbf_sigma_est_wide.pdf}
\end{center}  
\begin{footnotesize} 

\end{footnotesize}


\framebreak


\begin{itemize}
  \item Draw a random subset of the data.
  \item Compute pairwise distances $\|\xv - \tilde \xv \|$.
  \item Take a "central quantile" from their distribution, e.g., the median.
  \item This relates the kernel width to the "average distance" between points,
    which does make intuitive sense.
\end{itemize}

\begin{center}
\includegraphics[width = 11cm ]{figure_man/rbf_sigma.png}
\end{center}


\framebreak

\end{vbframe}

\begin{vbframe}{SVM hyperparameters}
\begin{itemize}
\item RBF-SVM parameters are often optimized on log-scale, as we want to explore
  large values and values close to 0.
\item E.g.: $C \in [2^{-15}, 2^{15}], \gamma \in [2^{-15}, 2^{15}]$
\item The cross-validated performance landscape often forms
  a characteristic "ridge" with a larger area of equally good values.
\end{itemize}

\begin{center}
  \includegraphics[width=7cm]{figure/svm_rbf_hyperparams_tuning_1.pdf}
\end{center}

\end{vbframe}

\endlecture
\end{document}
