{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d2f7806",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "label: colab_R_link\n",
    "https://colab.research.google.com/github/slds-lmu/lecture_sl/blob/main/exercises/svm-quarto/inserted/sol_svm_1_R.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe6a11",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "label: colab_python_link\n",
    "https://colab.research.google.com/github/slds-lmu/lecture_sl/blob/main/exercises/svm-quarto/inserted/sol_svm_1_py.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f33a1",
   "metadata": {},
   "source": [
    "label: exercise\n",
    "# Exercise\n",
    "Write your own stochastic subgradient descent routine to solve the soft-margin SVM in the primal formulation.\n",
    "\n",
    "Hints:\n",
    "\n",
    "- Use the regularized-empirical-risk-minimization formulation, i.e., an optimization criterion without constraints.\n",
    "- No kernels, just a linear SVM.\n",
    "- Compare your implementation with an existing implementation (e.g., `kernlab` in R or `sklearn.svm.SVC` in Python). Are your results similar? Note that you might have to switch off the automatic data scaling in the already existing implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e6bf1",
   "metadata": {},
   "source": [
    "label: import_and_globals\n",
    "## Imports and global variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67f24bd",
   "metadata": {},
   "source": [
    "label: algorithm_explanation\n",
    "\n",
    "# PEGASOS Algorithm Explanation\n",
    "\n",
    "The PEGASOS algorithm is a stochastic gradient descent method for training linear SVMs. It works by:\n",
    "\n",
    "1. **Random Sampling**: At each iteration, randomly select one training example\n",
    "2. **Weight Decay**: Apply regularization by shrinking the weight vector: `θ ← (1 - λα)θ`\n",
    "3. **Margin Check**: If the selected example is within the margin (i.e., `y_i * f(x_i) < 1`), update the weights: `θ ← θ + α * y_i * x_i`\n",
    "4. **Repeat**: Continue until convergence or maximum iterations\n",
    "\n",
    "More details can be found in the [i2ml chapter on linear SVMs](https://slds-lmu.github.io/i2ml/chapters/16_linear_svm/16-05-optimization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba374fc",
   "metadata": {},
   "source": [
    "label: data_setup\n",
    "\n",
    "## Data Generation and Setup\n",
    "\n",
    "For R we'll use the `mlbench.twonorm` dataset, which generates a two-class problem with two features. This is a classic benchmark dataset for binary classification.\n",
    "\n",
    "For Python, we can use `sklearn.datasets.make_classification` to generate a similar dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45135490",
   "metadata": {},
   "source": [
    "label: visual_inspection\n",
    "# Visual Inspection of the Data\n",
    "\n",
    "We can see that the data is mostly linearly separable, but there are some points that are close to the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bfa541",
   "metadata": {},
   "source": [
    "label: train_pegasos\n",
    "# Training the PEGASOS Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa538f54",
   "metadata": {},
   "source": [
    "label: decision_boundaries\n",
    "# Decision Boundaries Visualization\n",
    "\n",
    "Now we'll use the trained model to visualize the decision boundaries. Additionally, for comparison we will also fit a Logistic Regression model and visualize its decision boundary as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eea83b",
   "metadata": {},
   "source": [
    "label: eval_logistic_regression\n",
    "\n",
    "We can see that the decision boundaries are quite similar, let's also check the predictive performances of the logistic regression and Pegasos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7251c4",
   "metadata": {},
   "source": [
    "label: evaluation\n",
    "\n",
    "# Evaluating the PEGASOS Model\n",
    "\n",
    "We'll compute the accuracy and the confusion matrix. In practice, other metrics should be also considered, e.g. precision, recall, F1-score (threshold dependent); ROC AUC, PR AUC (threshold independent)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffe9368",
   "metadata": {},
   "source": [
    "label: kernellab_or_sklearn\n",
    "# Using `kernlab` (R) or `sklearn.svm.SVC` (Python) for Comparison\n",
    "\n",
    "Accuracy is identical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577a9393",
   "metadata": {},
   "source": [
    "label: comparison_coefs\n",
    "## Comparison of Model Coefficients\n",
    "\n",
    "We can see that both the predictions and coefficients are almost identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081af009",
   "metadata": {},
   "source": [
    "label: emp_risk_comparions\n",
    "## Comparison of empirical risks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a37c29",
   "metadata": {},
   "source": [
    "label: plotting_everything\n",
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e295d775",
   "metadata": {},
   "source": [
    "label: complete_visualization\n",
    "## Complete visualization\n",
    "\n",
    "- We can see that all the models produce almost identical decision boundary\n",
    "- The margins are plotted for the `kernlab`'s (R) / `sklearn.svm.SVC` (Python) model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
