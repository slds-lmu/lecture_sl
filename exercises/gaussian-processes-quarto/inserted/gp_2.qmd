---
title: "Exercise 2 -- Gaussian Processes"
subtitle: "[Supervised Learning](https://slds-lmu.github.io/i2ml/)"
notebook-view:  false
---

::: {.content-hidden when-format="pdf"}
::: {.hidden}
{{< include ../_quarto/latex-math.qmd >}}
:::
:::

::: {.content-visible when-profile="solution"}
::: callout-tip
You can run the notebooks in Google Colab here: [R](https://colab.research.google.com/github/slds-lmu/lecture_sl/blob/main/exercises/gaussian-processes-quarto/inserted/sol_gp_2_R.ipynb), [Python](https://colab.research.google.com/github/slds-lmu/lecture_sl/blob/main/exercises/gaussian-processes-quarto/inserted/sol_gp_2_py.ipynb).
:::
:::

# Exercise 1 (d)

Implement the GP with squared exponential kernel, zero mean function and $\ell = 1$ from scratch for $n=2$ observations $(\bm{y},\bm{x})$. 
Do this as efficiently as possible by explicitly calculating all expensive computations by hand. Do the same for the posterior predictive distribution of $y_*$. Test your implementation using simulated data.


::: {.content-visible when-profile="solution"}
# Solution: Explicit Calculations

To implement a GP with squared exponential kernel and $\ell = 1$, we need the inverse of $\bm{K}$. $\bm{x}$ being a vector implies that we have only one feature and thus the entries of our matrix $\bm{K}$ are 
$$
\bm{K} = \begin{pmatrix} 1 & \exp(-0.5 (x^{(1)} - x^{(2)})^2) \\ \exp(-0.5 (x^{(2)} - x^{(1)})^2) & 1 \end{pmatrix}.
$$

The inverse of $\bm{K}$ is then given by:
$$
\frac{1}{1-\exp(-(x^{(1)} - x^{(2)})^2)} \begin{pmatrix} 1 & -\exp(-0.5 (x^{(1)} - x^{(2)})^2) \\ -\exp(-0.5 (x^{(2)} - x^{(1)})^2) & 1 \end{pmatrix}.
$$

If we have a noisy GP, we would have to add $\sigma^2 \bm{I}_2$ to $\bm{K}$ with resulting inverse:

$$
\bm{K}_y^{-1} = \frac{1}{(1+\sigma^2)^2-\exp(-(x^{(1)} - x^{(2)})^2)} \begin{pmatrix} 1+\sigma^2 & -\exp(-0.5 (x^{(1)} - x^{(2)})^2) \\ -\exp(-0.5 (x^{(2)} - x^{(1)})^2) & 1+\sigma^2 \end{pmatrix}.
$$


Assuming a zero mean GP, we can derive $\frac{\partial \bm{K}_y}{\partial \theta}$ with $\theta = \sigma^2$, which gives us the identity matrix. We can thus maximize the marginal likelihood (slide on [*Gaussian Process Training*](https://slds-lmu.github.io/i2ml/chapters/19_gaussian_processes/19-05-training/)), by finding $\sigma^2$ that yields:
$$\text{tr}\left( \bm{K}_y^{-1} \bm{y} \bm{y}^\top \bm{K}_y^{-1} - \bm{K}_y^{-1} \right) = 0.$$

This can be solved analytically (though quite tedious). We will use a root-finding function for this. For the posterior predictive distribution we can make use of the results from the previous exercise.

# Code Setup

::: {.panel-tabset}
### R
{{< embed sol_gp_2_R.ipynb#setup echo=true >}}
### Python
{{< embed sol_gp_2_py.ipynb#setup echo=true >}}
:::

# Kernel Function Definition

We define the squared exponential kernel functions with length scale $l = 1$.

::: {.panel-tabset}
### R
{{< embed sol_gp_2_R.ipynb#kernel_functions echo=true >}}
### Python
{{< embed sol_gp_2_py.ipynb#kernel_functions echo=true >}}
:::

## Data Generation

We generate synthetic data according to the GP generating process. First, we sample input points $x$, then construct the kernel matrix $K$ and add noise to get $K_y$, and finally sample observations $y$ from the multivariate normal distribution.

::: {.panel-tabset}
### R
{{< embed sol_gp_2_R.ipynb#data_gen echo=true >}}
### Python
{{< embed sol_gp_2_py.ipynb#data_gen echo=true >}}
:::

# Hyperparameter Optimization

We implement the root-finding function to optimize the noise parameter $\sigma^2$ by setting the derivative of the marginal likelihood to zero. This corresponds to finding the root of the trace expression derived earlier.

::: {.panel-tabset}
### R
{{< embed sol_gp_2_R.ipynb#optimize_hyperparameters echo=true >}}
### Python
{{< embed sol_gp_2_py.ipynb#optimize_hyperparameters echo=true >}}
:::

## Visualization of Optimization

We plot the marginal likelihood derivative as a function of $\sigma^2$ to visualize the optimization problem and show where the optimal value is found (where the derivative equals zero).

::: {.panel-tabset}
### R
{{< embed sol_gp_2_R.ipynb#plot_optimization echo=true >}}
### Python
{{< embed sol_gp_2_py.ipynb#plot_optimization echo=true >}}
:::

# Posterior Predictive Distribution

We implement a function to draw samples from the posterior predictive distribution. This uses the GP posterior mean and variance formulas to generate predictions at new input points $x^*$.

::: {.panel-tabset}
### R
{{< embed sol_gp_2_R.ipynb#posterior_predictive_function echo=true >}}
### Python
{{< embed sol_gp_2_py.ipynb#posterior_predictive_function echo=true >}}
:::

## Sampling and Visualization

Finally, we draw samples from the posterior predictive distribution at a test point $x^* = 0$ using the optimized noise parameter, and visualize the distribution with a histogram.

::: {.panel-tabset}
### R
{{< embed sol_gp_2_R.ipynb#sample_and_plot echo=true >}}
### Python
{{< embed sol_gp_2_py.ipynb#sample_and_plot echo=true >}}
:::
:::
