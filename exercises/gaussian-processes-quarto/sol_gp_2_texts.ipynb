{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d4d09da",
   "metadata": {},
   "source": [
    "label: colab_R_link\n",
    "https://colab.research.google.com/github/slds-lmu/lecture_sl/blob/main/exercises/gaussian-processes-quarto/inserted/sol_gp_2_R.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8f28e8",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "label: colab_python_link\n",
    "https://colab.research.google.com/github/slds-lmu/lecture_sl/blob/main/exercises/gaussian-processes-quarto/inserted/sol_gp_2_py.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db22f8b0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "label: exercise\n",
    "# Exercise 1 (d)\n",
    "\n",
    "Implement the GP with squared exponential kernel, zero mean function and $\\ell = 1$ from scratch for $n=2$ observations $(\\boldsymbol{y},\\boldsymbol{x})$. \n",
    "Do this as efficiently as possible by explicitly calculating all expensive computations by hand. Do the same for the posterior predictive distribution of $y_*$. Test your implementation using simulated data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa271d1",
   "metadata": {},
   "source": [
    "label: math_solution\n",
    "# Solution: Explicit Calculations\n",
    "\n",
    "To implement a GP with squared exponential kernel and $\\ell = 1$, we need the inverse of $\\boldsymbol{K}$. $\\boldsymbol{x}$ being a vector implies that we have only one feature and thus the entries of our matrix $\\boldsymbol{K}$ are \n",
    "$$\n",
    "\\boldsymbol{K} = \\begin{pmatrix} 1 & \\exp(-0.5 (x^{(1)} - x^{(2)})^2) \\\\ \\exp(-0.5 (x^{(2)} - x^{(1)})^2) & 1 \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "The inverse of $\\boldsymbol{K}$ is then given by:\n",
    "$$\n",
    "\\frac{1}{1-\\exp(-(x^{(1)} - x^{(2)})^2)} \\begin{pmatrix} 1 & -\\exp(-0.5 (x^{(1)} - x^{(2)})^2) \\\\ -\\exp(-0.5 (x^{(2)} - x^{(1)})^2) & 1 \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "If we have a noisy GP, we would have to add $\\sigma^2 \\boldsymbol{I}_2$ to $\\boldsymbol{K}$ with resulting inverse:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{K}_y^{-1} = \\frac{1}{(1+\\sigma^2)^2-\\exp(-(x^{(1)} - x^{(2)})^2)} \\begin{pmatrix} 1+\\sigma^2 & -\\exp(-0.5 (x^{(1)} - x^{(2)})^2) \\\\ -\\exp(-0.5 (x^{(2)} - x^{(1)})^2) & 1+\\sigma^2 \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "Assuming a zero mean GP, we can derive $\\frac{\\partial \\boldsymbol{K}_y}{\\partial \\theta}$ with $\\theta = \\sigma^2$, which gives us the identity matrix. We can thus maximize the marginal likelihood (slide on [*Gaussian Process Training*](https://slds-lmu.github.io/i2ml/chapters/19_gaussian_processes/19-05-training/)), by finding $\\sigma^2$ that yields:\n",
    "$$\\text{tr}\\left( \\boldsymbol{K}_y^{-1} \\boldsymbol{y} \\boldsymbol{y}^\\top \\boldsymbol{K}_y^{-1} - \\boldsymbol{K}_y^{-1} \\right) = 0.$$\n",
    "\n",
    "This can be solved analytically (though quite tedious). We will use a root-finding function for this. For the posterior predictive distribution we can make use of the results from the previous exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de03c41f",
   "metadata": {},
   "source": [
    "label: setup\n",
    "# Code Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b711c976",
   "metadata": {},
   "source": [
    "label: kernel_setup\n",
    "# Kernel Function Definition\n",
    "\n",
    "We define the squared exponential kernel functions with length scale $l = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55358fa",
   "metadata": {},
   "source": [
    "label: data_gen\n",
    "## Data Generation\n",
    "\n",
    "We generate synthetic data according to the GP generating process. First, we sample input points $x$, then construct the kernel matrix $K$ and add noise to get $K_y$, and finally sample observations $y$ from the multivariate normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd1dce9",
   "metadata": {},
   "source": [
    "label: hyperparameter_optimization_explanation\n",
    "# Hyperparameter Optimization\n",
    "\n",
    "We implement the root-finding function to optimize the noise parameter $\\sigma^2$ by setting the derivative of the marginal likelihood to zero. This corresponds to finding the root of the trace expression derived earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af57434",
   "metadata": {},
   "source": [
    "label: optim_visualization\n",
    "## Visualization of Optimization\n",
    "\n",
    "We plot the marginal likelihood derivative as a function of $\\sigma^2$ to visualize the optimization problem and show where the optimal value is found (where the derivative equals zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fea36d",
   "metadata": {},
   "source": [
    "label: posterior_predictive\n",
    "# Posterior Predictive Distribution\n",
    "\n",
    "We implement a function to draw samples from the posterior predictive distribution. This uses the GP posterior mean and variance formulas to generate predictions at new input points $x^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9329125f",
   "metadata": {},
   "source": [
    "label: sampling_visualization\n",
    "## Sampling and Visualization\n",
    "\n",
    "Finally, we draw samples from the posterior predictive distribution at a test point $x^* = 0$ using the optimized noise parameter, and visualize the distribution with a histogram."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
