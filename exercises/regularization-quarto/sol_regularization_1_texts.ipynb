{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0591ea0b",
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "source": [
        "label: colab_R_link\n",
        "https://colab.research.google.com/github/slds-lmu/lecture_sl/blob/main/exercises/regularization-quarto/inserted/sol_regularization_quarto_1_R.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb6a1d3c",
      "metadata": {},
      "source": [
        "label: colab_python_link\n",
        "https://colab.research.google.com/github/slds-lmu/lecture_sl/blob/main/exercises/regularization-quarto/inserted/sol_regularization_1_py.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f78cfe51",
      "metadata": {
        "id": "f78cfe51",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "label: exercise_2_a\n",
        "# Exercise 2 (a)\n",
        "Simulate a data set with $n = 100$ observations based on the relationship $y = \\sin(x_1) + \\varepsilon$ with noise term $\\varepsilon$ following some distribution. Simulate $p=100$ additional covariates $x_2,\\ldots,x_{101}$ that are not related to $y$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af574691",
      "metadata": {
        "id": "af574691"
      },
      "source": [
        "label: install_packages\n",
        "## Packages\n",
        "If you want to reproduce the results, you may need to run the cell below"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "839fb2d6",
      "metadata": {
        "id": "839fb2d6"
      },
      "source": [
        "label: generating_data\n",
        "## Generating the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63d36b7a",
      "metadata": {
        "id": "63d36b7a"
      },
      "source": [
        "label: vis_relationship\n",
        "### Visualizing the relationships\n",
        "Let's see the relationships of $Y$ with $x_1$ and an unrelated covariate $x_2$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd12cd68",
      "metadata": {
        "id": "fd12cd68"
      },
      "source": [
        "label: exercise_2_b\n",
        "# Exercise 2 (b)\n",
        "On this data set, use different models (and software packages) of your choice to demonstrate\n",
        "\n",
        "* overfitting and underfitting;\n",
        "* L1, L2 and elastic net regularization;\n",
        "* the underdetermined problem;\n",
        "* the bias-variance trade-off;\n",
        "* early stopping using a simple neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26cb0408",
      "metadata": {
        "id": "26cb0408"
      },
      "source": [
        "label: under_over_fitting_intro\n",
        "## Underfitting and Overfitting\n",
        "Let's try to approximate the (sinusoidal) relationship between $y$ and $x_1$ with:\n",
        "\n",
        "1) a simple linear model\n",
        "2) a polynomial model of degree 50.\n",
        "\n",
        "Also, in order for us to see the effects of under/over-fitting not only visually but also numerically, lets split the data into training and test sets and calculate the training and test errors for each model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac8d82f3",
      "metadata": {
        "id": "ac8d82f3"
      },
      "source": [
        "label: imports_and_splitting\n",
        "### Imports and Splitting the Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94e4d843",
      "metadata": {
        "id": "94e4d843"
      },
      "source": [
        "label: underfitting_demo\n",
        "### Underfitting demonstration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c0a9c4c",
      "metadata": {
        "id": "1c0a9c4c"
      },
      "source": [
        "label: lin_reg_fit_and_eval\n",
        "#### Fitting Linear Regression and Evaluating the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed02a2d8",
      "metadata": {
        "id": "ed02a2d8"
      },
      "source": [
        "label: lin_reg_interpretation\n",
        "#### Linear Regression Train/Test Error Interpretation\n",
        "\n",
        "The error on the unseen data is significantly higher then the error on the training set which may indicate that the model is overfitting, but we must first ask ourselves whether or not the results on the training set are actually good. If the training error is significantly higher than the Bayes risk (i.e. the minimum possible error), then we may be underfitting the data.\n",
        "\n",
        "Visual inspection will help us here as well:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c464d745",
      "metadata": {
        "id": "c464d745"
      },
      "source": [
        "label: lin_reg_visual_inspection\n",
        "#### Linear Regression Visual Inspection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "253c395a",
      "metadata": {
        "id": "253c395a"
      },
      "source": [
        "label: lin_reg_plot_interpretation\n",
        "#### Linear Regression Plot Interpretation\n",
        "By plotting the predictions against the actual data and the $\\sin x$ function (noiseless DGP) we can see that the model lacks complexity $\\Rightarrow$ suffers from underfitting problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8c6f8a8",
      "metadata": {
        "id": "a8c6f8a8"
      },
      "source": [
        "label: overfitting_demo\n",
        "### Overfitting demonstration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29122b1c",
      "metadata": {
        "id": "29122b1c"
      },
      "source": [
        "label: poly_reg_fit_and_eval\n",
        "#### Fitting Polynomial Regression and Evaluating the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dfdc02f",
      "metadata": {
        "id": "4dfdc02f"
      },
      "source": [
        "label: poly_reg_interpretation\n",
        "#### Polynomial Regression Train/Test Error Interpretation\n",
        "\n",
        "Test error is undescribably higher than the training error, this is a clear sign of overfitting. The model is too complex for the data and it has learned the noise in the training set.\n",
        "\n",
        "Let's see this visually:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf085a70",
      "metadata": {
        "id": "cf085a70"
      },
      "source": [
        "label: poly_reg_visual_inspection\n",
        "#### Polynomial Regression Visual Inspection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba4e78c3",
      "metadata": {
        "id": "ba4e78c3"
      },
      "source": [
        "label: poly_reg_plot_interpretation\n",
        "#### Polynomial Regression Plot Interpretation\n",
        "By looking at the predictions for the points around -1.3 we can nicely see how much the model bends to pass through the training data points, and learns the noise instead of the actual pattern.\n",
        "\n",
        "The reader may find it interesting to decrease the degree of the polynomial to make the demonstration less extreme, and for degree = 7 you may even notice that the train/test errors alone indicate less overfitting than in the case of simple linear regression.\n",
        "\n",
        "Also, we encourage the reader to try to find the optimal degree of the polynomial."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aacb950",
      "metadata": {},
      "source": [
        "label: poly_reg_plot_interpretation_py\n",
        "\n",
        "*Additional note:*\n",
        "We can observe that the model outputs ~0 for $x \\in [1,1]$. We encourage the reader to try understanding why this happens.\n",
        "\n",
        "Hints:\n",
        "\n",
        "1. Look into model coefficients using `.coef_` attribute\n",
        "2. Think about the plot of $x$ raised to power of (for example) 42."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4f817d0",
      "metadata": {
        "id": "d4f817d0"
      },
      "source": [
        "label: regularization_demo\n",
        "## L1, L2 and Elastic Net Regularization\n",
        "\n",
        "Now let's demonstrate different types of regularization techniques and observe how they affect the coefficient paths (for each hyperparameter lambda the resulting model's coefficients)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc5f8d3a",
      "metadata": {},
      "source": [
        "label: regularization_helper_function_R\n",
        "**Helper Function for Plotting Regularization Paths in R**\n",
        "\n",
        "The R implementation uses the `glmnet` package which provides the `glmnet()` function for fitting regularized models and `plot.glmnet()` for visualizing the coefficient paths."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d365eb12",
      "metadata": {
        "id": "d365eb12"
      },
      "source": [
        "label: regularization_helper_function_py\n",
        "**Helper Function for Plotting Regularization Paths**\n",
        "\n",
        "Here `sklearn`'s `enet_path` function is used which under the hood fits a `ElasticNet` model. This makes the code much shorter since by passing 1, 0 or something in between as the `l1_ratio` parameter we can get the Lasso, Ridge or Elastic Net regularization paths respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43fe9221",
      "metadata": {
        "id": "43fe9221"
      },
      "source": [
        "label: l1_lasso_regularization\n",
        "## L1 (Lasso) Regularization\n",
        "\n",
        "For small lambda values (weak penalization) we can see that the uninformative covariates do have non-zero coefficients, but by increasing the lambda they eventually go to **exactly** zero. However, when we penalize the coefficients too strongly, the coefficient of $x_1$ also goes to zero, which is bad, since $x_1$ is actually informative."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "275efa3e",
      "metadata": {
        "id": "275efa3e"
      },
      "source": [
        "label: l2_ridge_regularization\n",
        "## L2 (Ridge) Regularization\n",
        "\n",
        "A few differences from the Lasso plot above:\n",
        "\n",
        "- The lambda values are much bigger here (smallest being around $10^4$ instead of $10^{-3}$)\n",
        "- As a result we also see that the coefficients are much smaller (0.0001 instead of 0.65)\n",
        "- Also, most importantly the coefficients **do not go to zero**, but rather to very small values. The Lasso performs feature selection, while the Ridge does not."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2761ee77",
      "metadata": {
        "id": "2761ee77"
      },
      "source": [
        "label: elastic_net_regularization\n",
        "## Elastic Net Regularization (with l1_ratio = 0.3)\n",
        "\n",
        "This approach is somewhere in-between the two above - lambda values are bigger than Lasso, but not as big as Ridge."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f08e7f99",
      "metadata": {
        "id": "f08e7f99"
      },
      "source": [
        "label: regularization_comparison_note\n",
        "### Comparing Regularization Methods\n",
        "\n",
        "The reader may find it interesting to compare the plots for the same lambda values across different regularization techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2863bfd2",
      "metadata": {
        "id": "2863bfd2"
      },
      "source": [
        "label: underdetermined_problem\n",
        "## The Underdetermined Problem\n",
        "\n",
        "As a reminder, we can obtain an analytic solution for a linear regression model with Mean Squared Error (MSE) loss by solving the normal equations\n",
        "$$\n",
        "\\beta = (X^TX)^{-1}X^TY\n",
        "$$\n",
        "\n",
        "The **underdetermined problem** arises when we have more covariates than observations, because in that case then the design matrix ($X$) is not full rank $\\Rightarrow$ the inverse of the $X^T X$ does not exist $\\Rightarrow$ we cannot solve the normal equations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7465c69e",
      "metadata": {
        "id": "7465c69e"
      },
      "source": [
        "label: calc_xt_x_determinant\n",
        "Although we see that the determinant is zero, let's try to compute the inverse (although in practice we would obtain the analytic solution (for the normal equation) by solving the System of Linear Equations $X^TX\\beta = X^TY$, and avoiding the inverse altogether)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99a5e622",
      "metadata": {
        "id": "99a5e622"
      },
      "source": [
        "label: inverse_existence_explanation_py\n",
        "The code above does produce an output (although it is mathematically impossible), since from the machine's perspective the matrix is not singular, because of the floating point approximations. The reader may dug deeper into the topic by following [this](https://stackoverflow.com/questions/28712734/numpy-possible-for-zero-determinant-matrix-to-be-inverted) Stack Overflow post.\n",
        "\n",
        "We'll demonstrate the underdetermined problem with a manually created example.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31f9983e",
      "metadata": {},
      "source": [
        "label: inverse_existence_explanation_R\n",
        "\n",
        "The code throws the error \"Error in solve.default(t(X_head) %*% X_head): system is computationally singular: reciprocal condition number = 2.59398e-20\" because the matrix is singular, meaning it does not have an inverse.\n",
        "\n",
        "The reader may find it interesting that the equivalent code in Python does produce an output (although it is mathematically impossible), since from the machine's perspective the matrix is not singular, because of the floating point approximations. The reader may dug deeper into the topic by following [this](https://stackoverflow.com/questions/28712734/numpy-possible-for-zero-determinant-matrix-to-be-inverted) Stack Overflow post."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51398bb9",
      "metadata": {
        "id": "51398bb9"
      },
      "source": [
        "label: bias_variance_tradeoff\n",
        "## The Bias-Variance Trade-off\n",
        "\n",
        "Let's demonstrate the bias-variance trade-off by comparing polynomial models of different degrees."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa2570f2",
      "metadata": {
        "id": "fa2570f2"
      },
      "source": [
        "label: bias_variance_interpretation\n",
        "### Interpreting the Bias-Variance Trade-off\n",
        "\n",
        "We can see that the green line (degree=1) is too simple and will have a underfitting (high bias) problem. On the other hand, the purple line (degree=15) is too complex and will have a overfitting (high variance) problem. The yellow line (degree=3) serves as a good compromise between the two.\n",
        "\n",
        "We invite the reader to experiment with different degrees of the polynomial, maybe additionally looking at train/test errors and tring to find the optimal degree."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bc99f10",
      "metadata": {
        "id": "0bc99f10"
      },
      "source": [
        "label: early_stopping_neural_network\n",
        "## Early Stopping using a Simple Neural Network\n",
        "\n",
        "First of all let's see how the model performs without early stopping."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ef996e9",
      "metadata": {
        "id": "3ef996e9"
      },
      "source": [
        "label: no_early_stopping_interpretation\n",
        "### Interpreting Results Without Early Stopping\n",
        "\n",
        "We can see that starting from epoch 15 model did not make any significant improvements. That's not the case here, but usually the model also starts to overfit to the training data, and the validation loss rises when we let it run for too many epochs.\n",
        "\n",
        "In order to avoid overfitting and use our computational resources more efficiently, we can use early stopping. We will track the validation loss and if for a given `patience` period (e. g. 5 epochs) the validation loss (`monitor`) does not improve by a given threshold (`min_delta`) (e. g. 0.01), we stop the training.\n",
        "\n",
        "*Side note:*\n",
        "\n",
        "In our case the validation loss did not start rising because the model had already achieved perfect fit (loss = 0) to the training data, so the weights were not updating anymore, and the validation loss did not change. Usually, for more complex DGP's achieving perfect fit is not possible."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "551644bf",
      "metadata": {
        "id": "551644bf"
      },
      "source": [
        "label: early_stopping_interpretation\n",
        "### Interpreting Early Stopping Results\n",
        "\n",
        "We see that the neural network ran only for 16 epochs, although we specified 100 epochs. This is because we used early stopping and the optimizer stopped after 6 epochs without significant improvement in the validation loss.\n",
        "\n",
        "We encourage the reader to split the data and look into train/test errors for the same neural network with and without early stopping. What do you observe regarding overfitting?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
