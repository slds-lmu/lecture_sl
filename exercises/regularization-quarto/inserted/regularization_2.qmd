---
title: "Regularization - Exercise 2"
subtitle: "[Supervised Learning](https://slds-lmu.github.io/i2ml/)"
notebook-view:  false
---

::: {.content-hidden when-format="pdf"}
::: {.hidden}
{{< include ../_quarto/latex-math.qmd >}}
:::
:::

::: {.content-visible when-profile="solution"}
::: callout-tip
You can run the notebooks in Google Colab here: [R](https://colab.research.google.com/github/slds-lmu/lecture_sl/blob/main/exercises/regularization-quarto/inserted/sol_regularization_2_R.ipynb), [Python](https://colab.research.google.com/github/slds-lmu/lecture_sl/blob/main/exercises/regularization-quarto/inserted/sol_regularization_2_py.ipynb).
:::
:::

# Exercise 1 (a - vi): Soft Thresholding Operator
$$\theta^*_j =\begin{cases}
    \frac{\rho_j + \lambda}{z_j} & \text{ for } \rho_j < -\lambda\\
    0 & \text{ for }  -\lambda \leq \rho_j \leq \lambda\\
   \frac{\rho_j - \lambda}{z_j} & \text{ for } \rho_j > \lambda
\end{cases}$$

Plot $\theta^*_j$ as a function of $\rho_j$ for $\rho_j \in [-5, 5], \lambda = 1, z_j = 1.$ This function is called the soft thresholding operator.

::: {.content-visible when-profile="solution"}

::: {.panel-tabset}
### R
{{< embed sol_regularization_2_R.ipynb#soft_thresholding_plot_data echo=true >}}
### Python
{{< embed sol_regularization_2_py.ipynb#soft_thresholding_plot_data echo=true >}}
:::

Plotting

::: {.panel-tabset}
### R
{{< embed sol_regularization_2_R.ipynb#soft_thresholding_plot echo=true >}}
### Python
{{< embed sol_regularization_2_py.ipynb#soft_thresholding_plot echo=true >}}
:::

## Interpretation of the Soft Thresholding Operator
As we can see from the plot the coefficient $\theta^*_j$ is equal to zero when $\rho_j$ is between $-\lambda$ and $\lambda$, and therefore performs feature selection.

::: 

# Exercise 1 (d): Lasso vs. Projected Lasso Comparison

You are given the code below to compare the quality of the projected Lasso regression vs. the regular Lasso regression.

Complete the missing code of the algorithms and interpret the result.

::: {.panel-tabset}
### R
{{< embed sol_regularization_2_R.ipynb#lasso_vs_projected_exercise_code_part_1 echo=true >}}
### Python
{{< embed sol_regularization_2_py.ipynb#lasso_vs_projected_exercise_code_part_1 echo=true >}}
:::

No need to make changes in the code below. 

Small note: the y label of the plot should shows the Root Mean Squared Error (RMSE) of the parameter estimation $\sqrt{\dfrac{1}{p}\sum_{j=1}^{p}(\hat{\theta}_j-\theta_{j,\mathrm{true}})^2}$

::: {.panel-tabset}
### R
{{< embed sol_regularization_2_R.ipynb#lasso_vs_projected_exercise_code_part_2 echo=true >}}
### Python
{{< embed sol_regularization_2_py.ipynb#lasso_vs_projected_exercise_code_part_2 echo=true >}}
:::


::: {.content-visible when-profile="solution"}


## Solution

::: {.panel-tabset}
### R
{{< embed sol_regularization_2_R.ipynb#installing_packages echo=true >}}
### Python
{{< embed sol_regularization_2_py.ipynb#installing_packages echo=true >}}
:::


### Projected Orthogonal Lasso
Please refer to subexercises above for more details

Line 10 - Eigenvalue Decomposition: $\mathbf{X}^T \mathbf{X} = \mathbf{V} \mathbf{D} \mathbf{V}^T$

Line 13 - Transformation Matrix: $\mathbf{A} = \mathbf{V} \mathbf{D}^{-0.5}$

Line 14 - Transformed Design Matrix: $\tilde{\mathbf{X}} = \mathbf{X} \mathbf{A}$

Line 18 - OLS Solution in Projected Space : $\hat{\boldsymbol{\theta}}_{\text{OLS}} = \tilde{\mathbf{X}}^T \mathbf{y}$

Line 19-20 - Soft Thresholding in Projected Space: 
$$\hat{\boldsymbol{\theta}}_{\text{proj},j}^* = \begin{cases}
    \hat{\boldsymbol{\theta}}_{\text{OLS},j} - \lambda & \text{if } \hat{\boldsymbol{\theta}}_{\text{OLS},j} > \lambda \\
    0 & \text{if } |\hat{\boldsymbol{\theta}}_{\text{OLS},j}| \leq \lambda \\
    \hat{\boldsymbol{\theta}}_{\text{OLS},j} + \lambda & \text{if } \hat{\boldsymbol{\theta}}_{\text{OLS},j} < -\lambda
\end{cases}$$

Line 22 - Back-transformation to Original Space: $\boldsymbol{\theta}^* = \mathbf{A} \hat{\boldsymbol{\theta}}_{\text{proj}}^*$

::: {.panel-tabset}
### R
{{< embed sol_regularization_2_R.ipynb#lasso_vs_projected_solution_code_proj_orth echo=true >}}
### Python
{{< embed sol_regularization_2_py.ipynb#lasso_vs_projected_solution_code_proj_orth echo=true >}}
:::

### Coordinate Descent Lasso 
Please refer to subexercises above for more details


Line 15 - Partial Residual: $r^{(j)} = \mathbf{y} - \mathbf{X}_{-j} \boldsymbol{\theta}_{-j}$

Line 16 - Residual Correlation: $\rho_j = \mathbf{X}_j^T r^{(j)} = \sum\limits_{i=1}^n X_{ij} \left( y_i - \sum\limits_{k \neq j} X_{ik} \theta_k \right)$

Line 17 - Feature Norm Squared: $z_j = \|\mathbf{X}_j\|_2^2 = \sum\limits_{i=1}^n X_{ij}^2$

From line 19 - Soft Thresholding Update:
$$\theta_j^{(t+1)} = \begin{cases}
    \frac{\rho_j + \lambda}{z_j} & \text{if } \rho_j < -\lambda \\
    0 & \text{if } |\rho_j| \leq \lambda \\
    \frac{\rho_j - \lambda}{z_j} & \text{if } \rho_j > \lambda
\end{cases}$$

Please note that we avoid additional sum loops by vectorization in the implementation

::: {.panel-tabset}
### R
{{< embed sol_regularization_2_R.ipynb#lasso_vs_projected_solution_code_lasso echo=true >}}
### Python
{{< embed sol_regularization_2_py.ipynb#lasso_vs_projected_solution_code_lasso echo=true >}}
:::

### Comparison of Lasso and Projected Lasso
The code below was provided in the exercise for comparing the 2 approaches in true parameter identification

::: {.panel-tabset}
### R
{{< embed sol_regularization_2_R.ipynb#lasso_vs_projected_solution_code_comparison echo=true >}}
### Python
{{< embed sol_regularization_2_py.ipynb#lasso_vs_projected_solution_code_comparison echo=true >}}
:::

### Interpretation of Results

In this simulation, the true parameter vector is sparse in its original coordinate system. Hence, as expected, the regular Lasso regression outperforms the projected approach on average when identifying the true parameters in this scenario.

### Full code

::: {.panel-tabset}
### R
{{< embed sol_regularization_2_R.ipynb#full_code echo=true >}}
### Python
{{< embed sol_regularization_2_py.ipynb#full_code echo=true >}}
:::

:::
