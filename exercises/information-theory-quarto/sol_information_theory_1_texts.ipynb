{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0382bb9c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "label: colab_R_link\n",
    "https://colab.research.google.com/github/slds-lmu/lecture_sl/blob/main/exercises/information-theory-quarto/inserted/sol_information_theory_1_R.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2830727",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "label: colab_python_link\n",
    "https://colab.research.google.com/github/slds-lmu/lecture_sl/blob/main/exercises/information-theory-quarto/inserted/sol_information_theory_1_py.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd12cd68",
   "metadata": {},
   "source": [
    "label: exercise_1_b\n",
    "# Exercise 1.b\n",
    "Sample points according to the true distribution and visualize the KLD for different parameter settings of\n",
    "the Gaussian distribution (including the optimal one if available)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa59b6a9",
   "metadata": {},
   "source": [
    "label: heading_sampling_binomial\n",
    "## Sampling from a Binomial Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02defe4",
   "metadata": {},
   "source": [
    "label: heading_various_gaussian_distributions\n",
    "## Various Gaussian distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e24f17",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "label: plotting\n",
    "### Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b162ec3",
   "metadata": {},
   "source": [
    "label: kl_divergence\n",
    "## KL divergence values\n",
    "For these distributions, we get the following KL divergence values (up to an additive constant):\n",
    "\n",
    "$$\n",
    "D_{KL}(f||q) = c_3 + 0.5 \\log \\sigma^2 + \\frac{1}{2\\sigma^2} (\\text{Var}_f(X) + (np-\\mu)^2))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb050ce",
   "metadata": {},
   "source": [
    "label: conclusion_1_b\n",
    "## Conclusion (not sure if this is worth including)\n",
    "KL divergence values let us the how badly we have specified of the distribution. Optimally specified Gaussian produced the lowest value. Changing the mean and/or variance resulted in an increased KL divergence values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f5a8da",
   "metadata": {},
   "source": [
    "label: exercise_1_c\n",
    "# Exercise 1.c\n",
    "Since we are now required to calculate the exact KLD values, we would also have to calculate $\\mathbb{E}_f(\\log f(X))$, which is somewhat more difficult. If you search the internet for a solution ($\\rightarrow$ \"entropy of a binomial distribution\"), you will find an approximate solution using the de-Moivre-Laplace theorem. Alternatively, we could make use of the central limit theorem, but then we would just approximate $f$ with a normal distribution with $\\mu = np$ and $\\sigma^2 = np(1-p)$, which would give us a constant KLD of zero (the very same happens if you use the first approximation using the de-Moivre-Laplace-theorem). We here instead will approximate the expectation using a large sample from the true underlying distribution:\n",
    "$$\n",
    "D_{KL}(f||q) \\approx \\frac{1}{B} \\sum_{b=1}^B [\\log f(X) - \\log q(X|\\mu = np, \\sigma^2 = np(1-p))]\n",
    "$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
