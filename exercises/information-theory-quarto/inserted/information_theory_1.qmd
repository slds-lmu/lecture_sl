---
title: "Exercise 1 -- Information Theory I"
subtitle: "[Supervised Learning](https://slds-lmu.github.io/i2ml/)"
notebook-view:  false
  # - notebook: sol_information_theory_1_R.ipynb
  #   title: "Exercise sheet for R"
  #   url: "https://github.com/slds-lmu/lecture_sl/blob/main/exercises/information-theory-quarto/sol_information_theory_1_R.ipynb"
---

::: {.content-hidden when-format="pdf"}
::: {.hidden}
{{< include ../_quarto/latex-math.qmd >}}
:::
:::

::: callout-tip
You can run the notebooks in Google Colab here: [R](https://colab.research.google.com/github/slds-lmu/lecture_sl/blob/main/exercises/information-theory-quarto/inserted/sol_information_theory_1_R.ipynb), [Python](https://colab.research.google.com/github/slds-lmu/lecture_sl/blob/main/exercises/information-theory-quarto/inserted/sol_information_theory_1_py.ipynb).
:::

# Exercise 1.b
Sample points according to the true distribution and visualize the KLD for different parameter settings of
the Gaussian distribution (including the optimal one if available)

::: {.content-visible when-profile="solution"}

## Sampling from a Binomial Distribution

::: {.panel-tabset}
### R
{{< embed sol_information_theory_1_R.ipynb#sampling_binomial echo=true >}}
### Python
{{< embed sol_information_theory_1_py.ipynb#sampling_binomial echo=true >}}
:::

## Various Gaussian distributions

::: {.panel-tabset}
### R
{{< embed sol_information_theory_1_R.ipynb#defining_densities echo=true >}}
### Python
{{< embed sol_information_theory_1_py.ipynb#defining_densities echo=true >}}
:::

### Plotting

::: {.panel-tabset}
### R
{{< embed sol_information_theory_1_R.ipynb#plotting echo=true >}}
### Python
{{< embed sol_information_theory_1_py.ipynb#plotting echo=true >}}
:::

## KL divergence values
For these distributions, we get the following KL divergence values (up to an additive constant):

$$
D_{KL}(f||q) = c_3 + 0.5 \log \sigma^2 + \frac{1}{2\sigma^2} (\text{Var}_f(X) + (np-\mu)^2))
$$

::: {.panel-tabset}
### R
{{< embed sol_information_theory_1_R.ipynb#kld_value echo=true >}}
### Python
{{< embed sol_information_theory_1_py.ipynb#kld_value echo=true >}}
:::

## Conclusion
KL divergence values let us see how badly we have specified the distribution. Optimally specified Gaussian produced the lowest value. Changing the mean and/or variance resulted in an increased KL divergence values

# Exercise 1.c
Since we are now required to calculate the exact KLD values, we would also have to calculate $\mathbb{E}_f(\log f(X))$, which is somewhat more difficult. If you search the internet for a solution ($\rightarrow$ "entropy of a binomial distribution"), you will find an approximate solution using the de-Moivre-Laplace theorem. Alternatively, we could make use of the central limit theorem, but then we would just approximate $f$ with a normal distribution with $\mu = np$ and $\sigma^2 = np(1-p)$, which would give us a constant KLD of zero (the very same happens if you use the first approximation using the de-Moivre-Laplace-theorem). We here instead will approximate the expectation using a large sample from the true underlying distribution:
$$
D_{KL}(f||q) \approx \frac{1}{B} \sum_{b=1}^B [\log f(X) - \log q(X|\mu = np, \sigma^2 = np(1-p))]
$$

::: {.panel-tabset}
### R
{{< embed sol_information_theory_1_R.ipynb#kld_value_varied_p_n echo=true >}}
### Python
{{< embed sol_information_theory_1_py.ipynb#kld_value_varied_p_n echo=true >}}
:::

:::