---
title: "Exercise 2 -- Information Theory II"
subtitle: "[Supervised Learning](https://slds-lmu.github.io/i2ml/)"
notebook-view:  false
  # - notebook: sol_information_theory_2_R.ipynb
  #   title: "Exercise sheet for R"
  #   url: "https://github.com/slds-lmu/lecture_sl/blob/main/exercises/information-theory-quarto/sol_information_theory_2_R.ipynb"
---

::: {.content-hidden when-format="pdf"}
::: {.hidden}
{{< include ../_quarto/latex-math.qmd >}}
:::
:::

::: callout-tip
You can run the notebooks in Google Colab here: [R](https://colab.research.google.com/github/slds-lmu/lecture_sl/blob/main/exercises/information-theory-quarto/inserted/sol_information_theory_2_R.ipynb), [Python](https://colab.research.google.com/github/slds-lmu/lecture_sl/blob/main/exercises/information-theory-quarto/inserted/sol_information_theory_2_py.ipynb).
:::

# Exercise 3: Smoothed Cross-Entropy Loss (b)
Implement the smoothed cross-entropy. We provide the signature of the function here as a reference:

::: {.panel-tabset}
### R
{{< embed sol_information_theory_2_R.ipynb#exercise_3_signature echo=true >}}
### Python
{{< embed sol_information_theory_2_py.ipynb#exercise_3_signature echo=true >}}
:::

::: {.content-visible when-profile="solution"}

# Solution

## Motivation
Before starting the solutions, let's try to develop a feeling for the smoothed cross-entropy loss by plotting it for different values of the smoothing parameter.

::: {.panel-tabset}
### R
{{< embed sol_information_theory_2_R.ipynb#motivation echo=true >}}
### Python
{{< embed sol_information_theory_2_py.ipynb#motivation echo=true >}}
:::

### Plotting

::: {.panel-tabset}
### R
{{< embed sol_information_theory_2_R.ipynb#motivation_plotting echo=true >}}
### Python
{{< embed sol_information_theory_2_py.ipynb#motivation_plotting echo=true >}}
:::

### Interpretation
We can see that for the standard CE (black line) the loss goes to zero  when the predicted probability for the true class goes to one (over-confident prediction). On the other hand, the smoothed CE (colored lines) takes on comparable values whenever the predicted probabilities are small (e.g before 0.2), but once the predicted probability approaches one, the curves actually go up, and by that penalizing over-confident predictions. The larger the smoothing parameter, the more pronounced this effect is.

## Main solution

### Confident Model
Let's build a "confident model". The model will have very high predicted probabilities for one of the labels. Additionally, we will add one sample with a not confident prediction (i.e. the predicted probabilities are close to uniform) and see how the smoothed cross-entropy behaves in this case.

::: {.panel-tabset}
### R
{{< embed sol_information_theory_2_R.ipynb#full_solution echo=true >}}
### Python
{{< embed sol_information_theory_2_py.ipynb#full_solution echo=true >}}
:::

### Interpretation
We can see that by adding smoothing we have changed the labels matrix (1's have decrease to 0.86 and zeros ent up to 0.06) which resulted in average loss increased from around 0.29 to 0.61 (~2.13x increase).

Let's observe the changes in the loss for each sample:
| Sample ID | Prediction distribution  | Prediction true label | Standard CE | Smoothed CE | Change factor |
| :-------: | -------------------------------------- | :------: | ---------: | ----------: | ------------: |
|     1     | (0.850, 0.100, 0.050)                  |   0.85  |      0.163 |       0.494 |         3.040 |
|     2     | (0.050, 0.900, 0.050)                  |   0.90  |      0.105 |       0.491 |         4.658 |
|     3     | (0.020, 0.950, 0.030)                  |   0.95  |      0.051 |       0.539 |        10.509 |
|     4     | (0.130, 0.020, 0.850)                  |   0.85  |      0.163 |       0.538 |         3.308 |
|     5     | (0.860, 0.040, 0.100)                  |   0.86  |      0.151 |       0.499 |         3.307 |
|     6     | (0.340, 0.330, 0.330)                  |   0.34  |      1.079 |       1.083 |         1.004 |

We can observe that the loss for the not confident prediction (sample 6) has remained practically unchanged, but for the confident predictions (samples 1-5) the loss has increased significantly. The increase is especially noticeable for the 3rd sample, for which the prediction was very confident (0.95) and the loss has increased by a factor of 10.5.

:::