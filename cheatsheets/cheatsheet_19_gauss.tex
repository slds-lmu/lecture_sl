\documentclass{beamer}
\newcommand \beameritemnestingprefix{}


\usepackage[orientation=landscape,size=a0,scale=1.4,debug]{beamerposter}
\mode<presentation>{\usetheme{mlr}}


\usepackage[utf8]{inputenc} % UTF-8
\usepackage[english]{babel} % Language
\usepackage{hyperref} % Hyperlinks
\usepackage{ragged2e} % Text position
\usepackage[export]{adjustbox} % Image position
\usepackage[most]{tcolorbox}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}


\input{../latex-math/basic-math.tex}
\input{../latex-math/basic-ml.tex}
\input{../latex-math/ml-trees.tex}
\input{../latex-math/ml-nn.tex}
\input{../latex-math/ml-gp}


\title{Supervised Learning :\,: CHEAT SHEET} % Package title in header, \, adds thin space between ::
\newcommand{\packagedescription}{ % Package description in header
%	The \textbf{I2ML}: Introduction to Machine Learning course offers an introductory and applied overview of "supervised" Machine Learning. It is organized as a digital lecture.
}

\newlength{\columnheight} % Adjust depending on header height
\setlength{\columnheight}{84cm} 

\newtcolorbox{codebox}{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	hbox}

\newtcolorbox{codeboxmultiline}[1][]{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	#1}

\begin{document}
\begin{frame}[fragile]{}
\begin{columns}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{

					\begin{myblock}{Bayesian Linear Model}
							
								Bayesian Linear Model:
							$$\yi = \fxi + \epsi = \thetav^T \xi + \epsi, \quad \text{for } i \in \{1, \ldots, n\}$$
								
								where $\epsi \sim \mathcal{N}(0, \sigma^2).$
							
								Parameter vector $\thetav$ is stochastic and follows a distribution.\\
							
								
								Gaussian variant:
								\begin{itemize}[$\bullet$]
									\setlength{\itemindent}{+.3in}
									\item Prior distribution: $\thetav \sim \mathcal{N}(\zero, \tau^2 \id_p)$ 
									\item Posterior distribution:	$
									\thetav ~|~ \Xmat, \yv \sim \mathcal{N}(\sigma^{-2}\bm{K}^{-1}\Xmat^\top\yv, \bm{K}^{-1})
									$ with $\bm{K}:= \sigma^{-2}\Xmat^\top\Xmat + \frac{1}{\tau^2} \id_p$
									\item Predictive distribution of $y_* = 	\thetav^\top \xv_*$ for a new observations $\xv_*$: 
									$$
									y_* ~|~ \Xmat, \yv, \xv_* \sim \mathcal{N}(\sigma^{-2}\yv^\top \Xmat \Amat^{-1}\xv_*, \xv_*^\top\Amat^{-1}\xv_*)
									$$
								\end{itemize}
\end{myblock}

                \begin{myblock}{Gaussian Processes}
                  
                  
								\fbox{
									\parbox{\dimexpr\textwidth-2\fboxsep-2\fboxrule}{
										\begin{table}
											\begin{tabular}{cc}
												\textbf{Weight-Space View} & \textbf{Function-Space View} \vspace{4mm}\\ 
												Parameterize functions & Work on functions directly\\
												\footnotesize Example: $\fxt = \thetav^\top \xv$ & \vspace{3mm}\\
												Define distributions on $\thetav$ & Define distributions on $f$ \vspace{4mm}\\
												Inference in parameter space $\Theta$ & Inference in function space $\Hspace$
											\end{tabular}
										\end{table}  
									}
								}\\
							

								\textbf{Gaussian Processes:} A function $\fx$ is generated by a GP $\gp$ if for \textbf{any finite} set of inputs $\left\{\xv^{(1)}, \dots, \xv^{(n)}\right\}$, the associated vector of function values $\bm{f} = \left(f(\xv^{(1)}), \dots, f(\xv^{(n)})\right)$ has a Gaussian distribution
								%
								$$
								\bm{f} = \left[f\left(\xi[1]\right),\dots, f\left(\xi[n]\right)\right] \sim \mathcal{N}\left(\bm{m}, \bm{K}\right),
								$$
								%
								with 
								%
								\begin{eqnarray*}
									\textbf{m} &:=& \left(m\left(\xi\right)\right)_{i}, \quad
									\textbf{K} := \left(k\left(\xi, \xv^{(j)}\right)\right)_{i,j}, 
								\end{eqnarray*}
								%
								where $m(\xv)$ is the mean function and $k(\xv, \xv^\prime)$ is the covariance function. \\
							
                Types of \textbf{covariance functions}:
								\begin{itemize}[$\bullet$]
									\setlength{\itemindent}{+.3in}
									%			
									\item $k(.,.)$ is stationary if it is as a function of $\bm{d} = \bm{x} - \bm{x}^\prime$, $ \leadsto k(\bm{d})$
									\item $k(.,.)$ is isotropic if it is a function of $r = \|\bm{x} - \bm{x}^\prime\|$,  $ \leadsto k(r)$
									\item $k(., .)$ is a dot product covariance function if $k$ is a function of $\bm{x}^T \bm{x}^\prime$
								\end{itemize}

							\end{myblock}
				}
			\end{minipage}
		\end{beamercolorbox}
	\end{column}
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{column}{.31\textwidth}
\begin{beamercolorbox}[center]{postercolumn}
\begin{minipage}{.98\textwidth}
\parbox[t][\columnheight]{\textwidth}{

\begin{myblock}{}  
							
								Commonly used covariance functions:
								
								\begin{center}
										\fbox{  
										\begin{tabular}{|c|c|}
											\hline
											Name & $k(\bm{x}, \bm{x}^\prime)$\\
											\hline
											constant & $\sigma_0^2$ \\ [1em]
											linear & $\sigma_0^2 + \bm{x}^T\bm{x}^\prime$ \\ [1em]
											polynomial & $(\sigma_0^2 + \bm{x}^T\bm{x}^\prime)^p$ \\ [1em]
											squared exponential & $\exp(- \frac{\|\bm{x} - \bm{x}^\prime\|^2}{2\ls^2})$ \\ [1em]
											Mat√©rn & \begin{footnotesize} $\frac{1}{2^\nu \Gamma(\nu)}\biggl(\frac{\sqrt{2 \nu}}{\ls}\|\bm{x} - \bm{x}^\prime\|\biggr)^{\nu} K_\nu\biggl(\frac{\sqrt{2 \nu}}{\ls}\|\bm{x} - \bm{x}^\prime\|\biggr)$\end{footnotesize}  \\ [1em]
											exponential & $\exp\left(- \frac{\|\bm{x} - \bm{x}^\prime\|}{\ls}\right)$ \\ [1em]
											\hline
									\end{tabular} }\\
								\end{center}
									

\end{myblock}
							\begin{myblock}{Gaussian Processes Prediction}

                \begin{codebox}
								\textbf{Posterior Process}
                \end{codebox}
								
								Assuming a zero-mean GP prior $\mathcal{GP}\left(\bm{0}, k(\xv, \xv^\prime)\right).$ 
							%	
								For $ f_* = f\left(\xv_*\right)$ on single unobserved test point $\xv_*$ 
							%	
								\begin{eqnarray*}
									f_* ~|~ \xv_*, \Xmat, \bm{f} \sim \mathcal{N}(\bm{k}_{*}^{T}\Kmat^{-1}\bm{f}, \bm{k}_{**} - \bm{k}_*^T \Kmat ^{-1}\bm{k}_*),
								\end{eqnarray*}
							%
								where, $\Kmat = \left(k\left(\xi, \xv^{(j)}\right)\right)_{i,j}$, $\bm{k}_* = \left[k\left(\xv_*, \xi[1]\right), ..., k\left(\xv_*, \xi[n]\right)\right]$ and $ \bm{k}_{**}\ = k(\xv_*, \xv_*)$. \\
								
								%
								For multiple unobserved test points
								$
								\bm{f}_* = \left[f\left(\xi[1]_*\right), ..., f\left(\xi[m]_*\right)\right]:
								$
								\begin{eqnarray*}
									\bm{f}_* ~|~ \Xmat_*, \Xmat, \bm{f} \sim \mathcal{N}(\Kmat_{*}^{T}\Kmat^{-1}\bm{f}, \Kmat_{**} - \Kmat_*^T \Kmat ^{-1}\Kmat_*).
								\end{eqnarray*} 
								with $\Kmat_* = \left(k\left(\xi, \xv_*^{(j)}\right)\right)_{i,j}$, $\Kmat_{**} = \left(k\left(\xi[i]_*, \xi[j]_*\right)\right)_{i,j}$.\\
								
								Predictive mean when assuming a non-zero mean GP prior $\gp$ with mean $m(\xv):$ 
								$$
								m(\Xmat_*) + \Kmat_*\Kmat^{-1}\left(\bm{y} - m(\Xmat)\right)
								$$
								Predictive variance remains unchanged. \\
								
							%
							%
							\end{myblock}

}
\end{minipage}
\end{beamercolorbox}
\end{column}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{column}{.31\textwidth}
\begin{beamercolorbox}[center]{postercolumn}
\begin{minipage}{.98\textwidth}
\parbox[t][\columnheight]{\textwidth}{

 \begin{myblock}{}
\begin{codebox}
								\textbf{Noisy Posterior Process}
                \end{codebox}
							%	
								Assuming a zero-mean GP prior $\mathcal{GP}\left(\bm{0}, k(\xv, \xv^\prime)\right):$ 
							%	
								\begin{eqnarray*}
									\bm{f}_* ~|~ \Xmat_*, \Xmat, \bm{y} \sim \mathcal{N}(\bm{m}_{\text{post}}, \bm{K}_\text{post}).
								\end{eqnarray*}
								with nugget $\sigma^2 $ and
							%	 
								\begin{eqnarray*}
									\bm{m}_{\text{post}} &=& \Kmat_{*}^{T} \left(\Kmat+ \sigma^2 \cdot \id\right)^{-1}\bm{y} \\
									\bm{K}_\text{post} &=& \Kmat_{**} - \Kmat_*^T \left(\Kmat  + \sigma^2 \cdot \id\right)^{-1}	\Kmat_*,	
								\end{eqnarray*} 
							%	
								
								Predictive mean when assuming a non-zero mean GP prior $\gp$ with mean $m(\xv):$ 
								$$
								m(\Xmat_*) + \Kmat_*(\Kmat +\sigma^2 \id)^{-1}\left(\bm{y} - m(\Xmat)\right)
								$$
								Predictive variance remains unchanged.
							%
							%
							\end{myblock}

  \begin{myblock}{Train a Gaussian Processes}
  
 We can learn the numerical hyperparameters of a selected covariance function directly during GP training.

 Let us assume 
$$
	y = \fx + \eps, ~ \eps \sim \mathcal{N}\left(0, \sigma^2\right),
$$
where $\fx \sim \mathcal{GP}\left(\bm{0}, k\left(\xv, \xv^\prime | \thetav \right)\right)$. 



Observing $\bm{y} \sim \mathcal{N}\left(\bm{0}, \bm{K} + \sigma^2 \id\right)$, the marginal log-likelihood (or evidence) is
\begin{eqnarray*}
\log p(\bm{y} ~|~ \bm{X}, \thetav) &=& \log \left[\left(2 \pi\right)^{-n / 2} |\bm{K}_y|^{-1 / 2} \exp\left(- \frac{1}{2} \bm{y}^\top \bm{K}_y^{-1} \bm{y}\right) \right]\\
&=& -\frac{1}{2}\bm{y}^T\bm{K}_y^{-1} \bm{y} - \frac{1}{2} \log \left| \bm{K}_y \right| - \frac{n}{2} \log 2\pi. 
\end{eqnarray*}
with $\bm{K}_y:=\bm{K} + \sigma^2 \id$ and $\thetav$ denoting the hyperparameters (the parameters of the covariance function). 
\\


The three terms of the marginal likelihood have interpretable roles, considering that 
the model becomes less flexible as the length-scale increases:
\begin{itemize}[$\bullet$]
\setlength{\itemindent}{+.3in}
\item the data fit $-\frac{1}{2}\bm{y}^T\bm{K}_y^{-1} \bm{y}$, which tends to decrease if the length scale increases
\item the complexity penalty $- \frac{1}{2} \log \left| \bm{K}_y \right|$, which depends on the covariance function only and which increases with the length-scale, because the model gets less complex with growing length-scale
\item a normalization constant $- \frac{n}{2} \log 2\pi$
\end{itemize}
\end{myblock}
  }
  
  \end{minipage}
  \end{beamercolorbox}
  \end{column}
  
  
  
\end{columns}
\end{frame}
\end{document}